{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#dsxconnect","title":"DSX\u2011Connect","text":"<p>A modular, integration\u2011agnostic framework for orchestrating file scanning and remediation at scale.</p> <ul> <li>Reusable scanning core (queues + workers)</li> <li>Pluggable DSX\u2011Connectors for each repository/service</li> <li>Handles on-demand and on-access scanning and remediation</li> <li>Supports docker compose and helm/k8s deployment models</li> <li>Event\u2011driven, fault\u2011tolerant, and cloud\u2011portable</li> </ul> <p></p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Overview</li> <li>Connectors</li> <li>Deployment Models</li> <li>Filters Reference</li> </ul>"},{"location":"#architecture","title":"Architecture","text":""},{"location":"overview-quick-start/","title":"Quick Start (Docker Compose)","text":"<p>Spin up DSX-Connect, DSXA, and a filesystem connector on a single machine using Docker Compose. \\ The steps below assume you have access to and unpacked a release bundle (<code>dsx-connect-&lt;version&gt;.tar.gz</code>) \\ that ships with the Compose files referenced here, and that Docker Desktop (or Docker Engine with the Compose plugin) is available locally.</p>"},{"location":"overview-quick-start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop (or Docker Engine with the Compose plugin) </li> <li>A bundle of the DSX-Connect docker compose files (<code>dsx-connect-&lt;version&gt;.tar.gz</code>)</li> </ul>"},{"location":"overview-quick-start/#1-launch-dsxa-and-dsx-connect-core","title":"1. Launch DSXA and DSX-Connect core","text":"<ol> <li>Unpack the release bundle and <code>cd</code> into the extracted directory.</li> <li>(First run only) Create the shared bridge network used by every compose file:    <pre><code>docker network create dsx-connect-network\n</code></pre></li> <li> <p>Configure and start the Deep Instinct scanning appliance (DSXA) services. You can edit the compose file directly or use environment variables.</p> </li> <li> <p>Option A (export variables):      <pre><code>export APPLIANCE_URL=\"https://&lt;di&gt;.customers.deepinstinctweb.com\"\nexport TOKEN=\"&lt;DSXA token&gt;\"\nexport SCANNER_ID=\"&lt;scanner id&gt;\"\nexport IMAGE=\"dsxconnect/dpa-rocky9:4.1.1.2020\"   # optional override\nexport FLAVOR=\"rest,config\"                       # optional override\nexport NO_SSL=\"true\"                              # optional override\n\ndocker compose -f docker-compose-dsxa.yaml up -d\n</code></pre></p> </li> <li> <p>Option B (env file):      <pre><code>cat &gt; dsxa.env &lt;&lt;'EOF'\nAPPLIANCE_URL=https://&lt;di&gt;.customers.deepinstinctweb.com\nTOKEN=&lt;DSXA token&gt;\nSCANNER_ID=&lt;scanner id&gt;\nIMAGE=dsxconnect/dpa-rocky9:4.1.1.2020   # optional override\nFLAVOR=rest,config                       # optional override\nNO_SSL=true                              # optional override\nEOF\n\ndocker compose --env-file dsxa.env -f docker-compose-dsxa.yaml up -d\n</code></pre></p> </li> </ol> <p>The compose file binds DSXA to the shared <code>dsx-connect-network</code> and exposes port <code>5000</code> on the host. Adjust the environment values above as needed; no YAML edits are required.</p> <ol> <li> <p>Start DSX-Connect core (API, workers, Redis, UI). </p> <p>The supplied compose file should come preconfigured to the dsx-connect image to use, but this can be overriden with the DSXCONNECT_IMAGE environment setting either within the compose file or as an environment setting (or file).</p> <p>Run the compose as is:    <pre><code>docker compose -f docker-compose-dsx-connect-all-services.yaml up -d\n</code></pre>    or, overriding the image repository and tag:    <pre><code>export DSXCONNECT_IMAGE=\"dsxconnect/dsx-connect:&lt;version x.y.z&gt;\"\ndocker compose -f docker-compose-dsx-connect-all-services.yaml up -d\n</code></pre></p> </li> <li> <p>Confirm the stack is healthy:    <pre><code>docker compose -f docker-compose-dsx-connect-all-services.yaml ps\n</code></pre>    You should see containers for the API (<code>dsx-connect-api</code>), workers, Redis, and supporting services. The UI becomes available at <code>http://localhost:8586/</code>.</p> </li> </ol>"},{"location":"overview-quick-start/#2-add-a-filesystem-connector","title":"2. Add a filesystem connector","text":"<ol> <li>Pick a host directory for testing:    <pre><code>mkdir -p ~/Documents/dsx-connect-test\n</code></pre>    Optionally drop a few sample files into <code>~/Documents/dsx-connect-test</code> so you have something to scan immediately.</li> <li>Edit <code>filesystem-connector-&lt;version&gt;/docker/docker-compose-filesystem-connector.yaml</code> and update the two placeholders near the top:    <pre><code>x-common-paths:\n  SCAN_FOLDER_PATH: &amp;scan-folder \"/Users/&lt;you&gt;/Documents/dsx-connect-test\"\n  QUARANTINE_FOLDER_PATH: &amp;quarantine-folder \"/Users/&lt;you&gt;/Documents/dsx-connect-test/dsxconnect-quarantine\"\n</code></pre>    These values are bind-mounted into the container so the connector can read and quarantine files from your host.</li> <li> <p>Set to Item Action to quarantine (move) malicious files:      <pre><code>DSXCONNECTOR_ITEM_ACTION: \"move\" # defines what action, if any, for a connector to take on malicious files (nothing, delete, tag, move, move_tag)\n</code></pre></p> </li> <li> <p>Bring up the connector on the shared network:    <pre><code>docker compose -f filesystem-connector-&lt;version&gt;/docker-compose-filesystem-connector.yaml up -d\n</code></pre>    Within a few seconds the connector registers with DSX-Connect and starts monitoring the folder (composition sets <code>DSXCONNECTOR_MONITOR=true</code> and enables polling by default).</p> </li> </ol>"},{"location":"overview-quick-start/#3-explore-the-ui-and-run-a-scan","title":"3. Explore the UI and run a scan","text":"<ol> <li>Browse to <code>http://localhost:8586</code>.</li> <li> <p>On the Connectors tab you should see a card for <code>filesystem-connector</code> showing its status and metadata.</p> <ul> <li>The gear icon reveals the effective runtime configuration (resolved asset path, monitoring settings, etc.).</li> <li>Preview lists a handful of files straight from the connector. This is a great way to confirm the connector is pointed at the right folder.</li> <li>Sample Scan triggers scans for the first five files in the monitored folder.</li> </ul> </li> <li> <p>Try either flow:</p> <ul> <li>Click Full Scan to enumerate every file under <code>~/Documents/dsx-connect-test</code> and send each one for inspection. Scan results appear in the Scan Results pane of the UI.</li> <li>Drop a new file into the folder. The connector\u2019s monitor fires a webhook event and the file shows up in Scan Results automatically. If you configured an item action (e.g., <code>move</code>), confirm that quarantined files land in <code>~/Documents/dsx-connect-test/dsxconnect-quarantine</code> once remediation runs.</li> </ul> </li> </ol> <p>Done!  You can click on scan results to see more details, or click Full Scan again to re-run the scan.</p>"},{"location":"overview-quick-start/#4-tear-down","title":"4. Tear down","text":"<pre><code>docker compose -f docker-compose-dsx-connect-all-services.yaml down\ndocker compose -f docker-compose-dsxa.yaml down\ndocker compose -f filesystem-connector-&lt;version&gt;/docker-compose-filesystem-connector.yaml down\n</code></pre>"},{"location":"overview/","title":"DSX\u2011Connect Overview","text":"<p>DSX\u2011Connect is a modular integration framework that orchestrates safe, scalable file scanning and remediation while remaining completely agnostic to specific repository integrations. The core \u201cscan brain\u201d handles queuing, parallelism, and remediation; DSX\u2011Connectors adapt repositories (Filesystem, S3, GCS, SharePoint/OneDrive, GitHub, etc.) to a simple, consistent API.</p>"},{"location":"overview/#key-components","title":"Key Components","text":"<ul> <li>DSX\u2011Connect (Core):</li> <li>Receives scan requests, fetches file content from connectors, invokes scanning, and applies remediation.</li> <li>Scales from one file to billions using queues and horizontally scalable workers.</li> <li> <p>Integration\u2011agnostic: depends only on a stable connector API.</p> </li> <li> <p>DSX\u2011Connectors (Connectors):</p> </li> <li>Connectors implement a consistent API and focus on the specifics of a repository.</li> <li>Easily extendable: add new connectors to support new repositories.</li> <li>Stateless and can be deployed independently.</li> </ul>"},{"location":"overview/#architecture-advantages","title":"Architecture Advantages","text":"<ul> <li>Clear delineation: DSX-Connect core orchestrates scanning, verdict handling, logging, etc...; connectors handle repository specific semantics (list/read/actions).</li> <li>Event\u2011driven: Queues decouple enumeration, scanning, and remediation for high throughput and resilience.</li> <li>Fault tolerant &amp; HA: Retries with exponential backoff, dead letter queues (DLQ), graceful degradation.</li> <li>Scalable: Core Worker pools handle parallelism and scaling; connectors operate independently.</li> <li>Flexible: Add new repositories by writing a connector; core components stay the same.</li> <li>Consistent &amp; pluggable: All connectors share the same API contract; updating one doesn\u2019t disrupt others.</li> <li>Observability: Health endpoints, queue metrics, DLQ management, structured logs.</li> </ul>"},{"location":"overview/#example-workflow","title":"Example Workflow","text":"<p>The following diagram illustrates a simplified workflow of DSX-Connect, deployed with Scan Request and Verdict workers, and a Filesystem Connector.  First, a quick overview of connector APIs relevavnt to this example:</p> <ul> <li><code>full_scan</code>: enumerate items and enqueue scan requests (streaming enumeration recommended)</li> <li><code>read_file</code>: retrieve file content (binary stream)</li> <li><code>item_action</code>: perform remediation (delete/move/tag)</li> </ul> <p>Filesystem Connector deployed with access to a folder: ~/Documents, and quarantine folder set to ~/Documents/quarantine.</p> <ol> <li>DSX-Connect triggers a <code>full_scan</code> on the connector then...</li> <li>the connector lists all files in ~/Documents and for each file...</li> <li>sends Scan Requests to DSX-Connect which queues each request.  </li> <li>DSX\u2011Connect Scan Request Worker dequeues a Scan Request and... </li> <li>Requests the file from the connector (<code>read_file</code>), and...</li> <li>scan the file with DSXA and places the DSXA verdict in the Verdict Queue.</li> <li>Verdict Worker dequeues a verdict and then, on a malicious verdict,</li> <li>calls on the connector's <code>item_action</code> to take action the connector (delete/move/tag the file). In this case, the Filesystem Connector moves the file to ~/Documents/quarantine.</li> </ol> <p>Benefits of decoupling: resiliency (queue persistence), scale (parallel workers), and isolation (enumeration doesn\u2019t block scanning).</p>"},{"location":"overview/#filesystem-example-image","title":"Filesystem Example (Image)","text":""},{"location":"overview/#architecture-overview","title":"Architecture Overview","text":"<p>Mermaid diagram:  </p> <pre><code>flowchart LR\n%% Define Repositories (left column)\n    subgraph Repositories[\"File Repositories\"]\n        AWSRepo[\"AWS S3\"]\n        AzureRepo[\"Azure Blob Storage\"]\n        GCPRepo[\"Google Cloud Storage\"]\n        FSRepo[\"Filesystem\"]\n    end\n\n%% Define Connectors (middle column)\n    subgraph Connectors[\"DSX-Connectors\"]\n        AWSConn[\"AWS Connector\"]\n        AzureConn[\"Azure Connector\"]\n        GCPConn[\"GCP Connector\"]\n        FSConn[\"Filesystem Connector\"]\n    end\n\n%% Define Core System (right column)\n    subgraph DSX[\"DSX-Connect Core\"]\n      direction TB\n      API[\"API (FastAPI)\"]\n      SRQ[\"Scan Request Queue\"]\n      SRW[\"Scan Request Worker\"]\n      VQ[\"Verdict Queue\"]\n      VW[\"Verdict Worker\"]\n      RQ[\"Scan Result Queue\"]\n      RW[\"Scan Result Worker\"]\n      RSYS[\"rsyslog / stdout\"]\n      STATS[\"Stats\"]\n      RDB[\"Results DB\"]\n\n      %% Force a vertical flow\n      API --&gt; SRQ\n      SRQ --&gt; SRW\n      SRW --&gt; VQ\n      VQ --&gt; VW\n      VW --&gt; RQ\n      RQ --&gt; RW\n      RW --&gt; RSYS\n      RW --&gt; STATS\n      RW --&gt; RDB\n    end\n\n%% Relationships (bidirectional)\n    AWSRepo &lt;--&gt; AWSConn\n    AzureRepo &lt;--&gt; AzureConn\n    GCPRepo &lt;--&gt; GCPConn\n    FSRepo &lt;--&gt; FSConn\n\n    AWSConn &lt;--&gt; API\n    AzureConn &lt;--&gt; API\n    GCPConn &lt;--&gt; API\n    FSConn &lt;--&gt; API\n\n%% Styling for clarity\n    classDef repo fill:#e3f2fd,stroke:#1565c0,color:#0d47a1,stroke-width:1px;\n    classDef conn fill:#e3f2fd,stroke:#1565c0,color:#0d47a1,stroke-width:1px;\n    classDef core fill:#fff3e0,stroke:#ef6,color:#9c6a00,stroke-width:1px;\n    classDef queue fill:#ffe0b2,stroke:#ef6,color:#9c6a00,stroke-width:1px;\n    classDef worker fill:#e8f5e9,stroke:#2e7d32,color:#1b5e20,stroke-width:1px;\n    class AWSConn,AzureConn,GCPConn,FSConn conn;\n    class SRQ,VQ,RQ queue;\n    class SRW,VW,RW worker;\n</code></pre> <p>Below is a high\u2011level Mermaid diagram that complements the static overview:</p>"},{"location":"overview/#sequence-full-scan-mermaid","title":"Sequence: Full Scan (Mermaid)","text":"<p>This is a more detailed sequence adapted from the Confluence page.</p> <pre><code>sequenceDiagram\n    participant UI as Web UI/API Client\n    participant DSX as DSX-Connect Core\n    participant Queue as Redis Queue\n    participant Worker as Celery Worker\n    participant Conn as Connector\n    participant Repo as Repository\n    participant DSXA as DSXA Scanner\n\n    Note over UI, DSXA: Full Scan Initiation Flow\n\n    UI-&gt;&gt;+DSX: POST /connectors/full_scan/{uuid}\n    DSX-&gt;&gt;+Conn: POST /{connector}/full_scan\n    Note over DSX, Conn: Background task initiated\n\n    Conn-&gt;&gt;+Repo: List/enumerate items\n    Repo--&gt;&gt;-Conn: Item keys/paths\n\n    loop For each item\n        Conn-&gt;&gt;Conn: Build ScanRequest(location, metainfo)\n        Conn-&gt;&gt;+DSX: POST /scan/request\n        DSX-&gt;&gt;+Queue: Enqueue scan_request_task\n        Queue--&gt;&gt;-DSX: Task queued\n        DSX--&gt;&gt;-Conn: {status: success, message: queued}\n    end\n\n    Conn--&gt;&gt;-DSX: Full scan invoked (enqueued)\n    DSX--&gt;&gt;-UI: Full scan initiated\n\n    Note over UI, DSXA: Asynchronous Processing of Queued Scans\n\n    loop For each queued scan request\n        Worker-&gt;&gt;+Queue: Pop scan_request_task\n        Queue--&gt;&gt;-Worker: ScanRequest data\n        Worker-&gt;&gt;+Conn: POST /{connector}/read_file\n        Conn-&gt;&gt;+Repo: Download/stream content\n        Repo--&gt;&gt;-Conn: Blob stream\n        Conn--&gt;&gt;-Worker: StreamingResponse (content)\n        Worker-&gt;&gt;+DSXA: POST /scan/binary/v2\n        DSXA--&gt;&gt;-Worker: Verdict (benign/malicious)\n        alt Malicious\n            Worker-&gt;&gt;+Conn: PUT /{connector}/item_action\n            Conn-&gt;&gt;+Repo: Apply action (move/tag/delete)\n            Repo--&gt;&gt;-Conn: Action applied\n            Conn--&gt;&gt;-Worker: ItemActionStatusResponse\n        end\n        Worker-&gt;&gt;Worker: Store result / notify\n    end</code></pre>"},{"location":"overview/#deployment-models","title":"Deployment Models","text":"<ul> <li>Docker Compose: Ideal for evaluations and minimal production deployments.</li> <li>Kubernetes + Helm: Scalable, resilient, and portable across AKS/EKS/GKE/OKE. Integrates with HA Redis, object storage, and logging.  Ideal for     production environments connected to multiple repositories of the same or different types, high availability a necessity.</li> </ul> <p>Both models use the same container images and configuration patterns, easing promotion from test to prod.</p>"},{"location":"overview/#fault-tolerance-high-availability","title":"Fault Tolerance &amp; High Availability","text":"<ul> <li>Retries &amp; DLQ: Intelligent retries per failure type (connector/DSXA/timeouts/rate limits). DLQ preserves failed tasks for reprocessing.</li> <li>Graceful degradation: Continue accepting/queuing when downstreams are degraded; resume automatically.</li> <li>HA patterns: Stateless workers; shared queues; multiple API replicas; connectors operate independently.</li> </ul>"},{"location":"connectors/","title":"Connectors","text":"<p>Designs and guides for DSX\u2011Connectors.</p> <ul> <li>M365 Email (Design)</li> <li>Filesystem (Overview)</li> <li>Google Cloud Storage</li> </ul> <p>Additional connector deployment guides are linked under Deployment.</p>"},{"location":"connectors/concepts/","title":"Connector Concepts","text":"<p>Once you have the helm/docker quickstart under your belt, every connector exposes the same core configuration knobs. These shared concepts keep dsx-connect behavior consistent regardless of whether you are scanning an S3 bucket or a SharePoint site.</p>"},{"location":"connectors/concepts/#asset","title":"Asset","text":"<p><code>DSXCONNECTOR_ASSET</code> defines the root location that the connector owns. Full scans start here, and \u201con-access\u201d feeds (webhooks, monitors) scope themselves to the same root. The exact meaning depends on the backend:</p> Connector family Typical value Notes AWS S3 <code>bucket-name</code> Optionally include a prefix (<code>bucket-name/prefix</code>) if you want a sub-tree only. Azure Blob Storage <code>container-name</code> Combined with <code>DSXCONNECTOR_FILTER</code> for virtual folder scoping. Google Cloud Storage <code>bucket-name</code> Behaves like S3\u2014think of it as the top of the directory tree. Filesystem Absolute path (<code>/mnt/share</code>, <code>/app/scan_folder</code>) Defaults to the mounted <code>scanVolume</code>. SharePoint / OneDrive / M365 Mail Site/document-library, drive, or mailbox root See the connector-specific doc for precise URI requirements. <p>Always set the asset to a stable, exact root\u2014no wildcards. If you need multiple roots, deploy multiple connectors.</p>"},{"location":"connectors/concepts/#filter","title":"Filter","text":"<p><code>DSXCONNECTOR_FILTER</code> narrows the asset. For storage connectors this usually means subdirectories or prefixes (<code>logs/2025/*</code>). SharePoint/OneDrive filters can target libraries or change types. The semantics are connector-specific, but the intent is identical: scope work under the asset without changing the root. See Reference \u2192 Assets &amp; Filters for examples.</p>"},{"location":"connectors/concepts/#item-action-policy","title":"Item action policy","text":"<p><code>DSXCONNECTOR_ITEM_ACTION</code> tells the connector what to do when dsx-connect marks an object malicious:</p> Value Behavior <code>nothing</code> Report only. Leave the object untouched. <code>delete</code> Remove the object. <code>tag</code> Apply provider-specific metadata/tagging (e.g., S3 object tags). <code>move</code> Relocate the object (usually to quarantine). <code>move_tag</code> Move + tag in a single workflow. <p>When you pick <code>move</code> or <code>move_tag</code>, also set <code>DSXCONNECTOR_ITEM_ACTION_MOVE_METAINFO</code>. Interpreting that field is connector-specific (S3 key prefix, filesystem directory, SharePoint folder, etc.) but it always describes the quarantine destination.</p>"},{"location":"connectors/concepts/#putting-it-together","title":"Putting it together","text":"<p>A good deployment checklist:</p> <ol> <li>Decide on the asset root for each connector instance.</li> <li>Add filters only when you genuinely need a sub-scope; otherwise keep it empty.</li> <li>Pick an item action that matches your response policy and ensure the quarantine path/tag exists.</li> </ol> <p>For the precise shape of each field (SharePoint site URL vs. filesystem path, etc.), jump to the connector-specific page under Connectors \u2192 Connector Deployments.</p>"},{"location":"connectors/filesystem/","title":"Filesystem Connector (Overview)","text":"<p>The Filesystem connector performs a full scan over a directory and can monitor for new/modified files. It implements the standard DSX\u2011Connector API (<code>full_scan</code>, <code>read_file</code>, <code>item_action</code>, <code>webhook_event</code>, <code>repo_check</code>) and remains stateless \u2014 scanning and decisions happen in DSX\u2011Connect.</p>"},{"location":"connectors/filesystem/#full-scan-and-actions","title":"Full Scan and Actions","text":"<ul> <li><code>full_scan</code>: enumerates files and posts scan requests to DSX\u2011Connect asynchronously.</li> <li><code>read_file</code>: streams file bytes to DSX\u2011Connect when a worker requests content.</li> <li><code>item_action</code>: deletes/moves/tags based on policy when a file is malicious.</li> </ul>"},{"location":"connectors/filesystem/#example-diagram","title":"Example Diagram","text":"<p>Filters: Use the centralized rsync\u2011like filter rules in Reference \u2192 Filters.</p>"},{"location":"connectors/m365-email/","title":"M365 Email Attachments Connector (Design)","text":"<p>This document describes a new connector, <code>m365-mail-connector</code>, that scans Microsoft 365 (Exchange Online) email attachments via Microsoft Graph and integrates with dsx-connect using the Enrollment + per\u2011connector HMAC model.</p>"},{"location":"connectors/m365-email/#goals","title":"Goals","text":"<ul> <li>Near real\u2011time scanning of email attachments at scale.</li> <li>Safe bootstrap: Enrollment token \u2192 provision HMAC; all subsequent traffic signed (both directions).</li> <li>Webhook-first with delta-query backup for reliability.</li> <li>Optional remediation: move message, delete attachment, or tag on malicious verdicts.</li> </ul>"},{"location":"connectors/m365-email/#highlevel-flow","title":"High\u2011Level Flow","text":"<ul> <li>Startup</li> <li>Register with dsx-connect (Enrollment header); receive HMAC (<code>kid</code>, <code>secret</code>); store in memory.</li> <li>Validate Graph credentials by obtaining a token (client credentials flow).</li> <li>Create/renew Graph subscriptions for target mailboxes (e.g., <code>/users/{id}/messages</code>), with <code>includeResourceData=true</code> when desired.</li> <li>Webhook events (Graph \u2192 connector <code>/webhook_event</code>)</li> <li>Validate notification (validationToken for GET; POST signature/clientState for deliveries).</li> <li>For each change: fetch message metadata; if <code>hasAttachments=true</code>, enumerate attachments.</li> <li>For <code>fileAttachment</code>: stream <code>/attachments/{attId}/$value</code> to dsx-connect scan request.</li> <li>For <code>itemAttachment</code>: optional; usually low value; flag via config.</li> <li>For <code>referenceAttachment</code>: optional; requires <code>Files.Read.All</code> to download from OneDrive/SharePoint; or skip and rely on storage connectors.</li> <li>Remediation (optional)</li> <li>Move message (<code>/messages/{id}/move</code>) to a quarantine folder.</li> <li>Delete malicious attachment.</li> <li>Tag subject/body or add category.</li> <li>Full Scan / Backfill</li> <li>Use delta queries (<code>/users/{id}/mailFolders('inbox')/messages/delta</code>) to enumerate new/changed messages and scan attachments.</li> <li>Persist delta tokens per mailbox in dsx-connect\u2019s Connector State KV (<code>m365/delta:&lt;upn&gt;</code>) to continue incrementally.</li> <li>Treat \u201cfull scan\u201d as a bounded delta pass across every configured asset (mailbox or mailbox/folder). Each pass walks the Graph delta chain, enqueues attachments, then stores the returned <code>@odata.deltaLink</code> as the new cursor.</li> <li>A background delta runner executes every <code>delta_run_interval_seconds</code> (default 600s) to catch drift and initial loads.</li> <li>Manual command: <code>POST /dsx-connect/api/v1/connectors/full_scan/{uuid}</code> triggers a one-shot delta run via <code>@connector.full_scan</code>, reusing the same logic and reporting how many attachments were enqueued (honors <code>?limit=N</code> for test runs).</li> </ul>"},{"location":"connectors/m365-email/#permissions","title":"Permissions","text":"<ul> <li>Application permissions (recommended):</li> <li>Minimum read: <code>Mail.Read</code> (or <code>Mail.ReadWrite</code> if actions are enabled).</li> <li>For <code>referenceAttachment</code> download: <code>Files.Read.All</code>.</li> <li>For resource data in notifications: cert + <code>includeResourceData=true</code> subscriptions.</li> </ul>"},{"location":"connectors/m365-email/#configuration-examples","title":"Configuration (examples)","text":"<ul> <li>Tenant / App creds: <code>M365_TENANT_ID</code>, <code>M365_CLIENT_ID</code>, <code>M365_CLIENT_SECRET</code> (or client cert).</li> <li>Scope of mailboxes: explicit UPN list via <code>M365_MAILBOX_UPNS</code> (comma-separated), security group, or all users.</li> <li>Subscription management: renewal interval, includeResourceData.</li> <li>Processing policy: max attachment size, allowed types, referenceAttachments policy.</li> <li>Item actions policy: <code>move</code>, <code>delete</code>, <code>tag</code>, <code>nothing</code>.</li> </ul>"},{"location":"connectors/m365-email/#assets-filters-mailbox-scope","title":"Assets &amp; Filters (Mailbox Scope)","text":"<ul> <li>Asset (<code>DSXCONNECTOR_ASSET</code>) sets the precise mailbox or mailbox/folder root \u2014 no wildcards.</li> <li>Mailbox-level: <code>user@contoso.com</code></li> <li>Mailbox folder: <code>user@contoso.com/Inbox</code>, <code>user@contoso.com/Finance</code></li> <li>Use assets to shard large estates by deploying multiple connector instances (per mailbox or per high-value folder).</li> <li>Filter (<code>DSXCONNECTOR_FILTER</code>) applies rsync-style include/exclude rules to attachment filenames relative to the asset.</li> <li>Examples: <code>/.zip</code>, <code>**/*.docx</code>, <code>-tmp/</code></li> <li>Pros: expressive for suffix- or path-based selection without changing the asset root.</li> <li>Cons: complex excludes can force broader provider listings, so prefer assets for coarse partitions.</li> <li>Sharding patterns</li> <li>Asset-based: instance A \u2192 <code>user1@contoso.com/Inbox</code>, B \u2192 <code>user1@contoso.com/Finance</code>, C \u2192 <code>user2@contoso.com/Inbox</code>.</li> <li>Filter-based: single mailbox asset, multiple instances with include-only filters such as <code>filter=Inbox/</code> vs <code>filter=Finance/</code>.</li> <li>Combine with DSX scheduler for predictable concurrency and per-mailbox throttling.</li> </ul>"},{"location":"connectors/m365-email/#security","title":"Security","text":"<ul> <li>Enrollment + HMAC per connector for all dsx\u2011connect \u2194 connector private routes.</li> <li>Webhook validation &amp; signature checks for Graph notifications.</li> <li>OAuth tokens stored in memory only (no on-disk secrets).</li> </ul>"},{"location":"connectors/m365-email/#reliability-scaling","title":"Reliability &amp; Scaling","text":"<ul> <li>Subscription renew loop with jittered backoff.</li> <li>Delta queries to catch missed events; idempotent dedupe on (messageId, attachmentId) in Redis.</li> <li>Respect Graph 429 / Retry-After.</li> <li>Stream downloads to avoid memory pressure; size limits enforced by config.</li> </ul>"},{"location":"connectors/m365-email/#files-scaffold","title":"Files (scaffold)","text":"<ul> <li><code>connectors/m365_mail/config.py</code>: pydantic config for Graph + connector settings.</li> <li><code>connectors/m365_mail/start.py</code>: Typer CLI to launch uvicorn.</li> <li><code>connectors/m365_mail/m365_mail_connector.py</code>: DSXConnector wiring; webhook, repo_check stubs.</li> <li><code>connectors/m365_mail/graph_client.py</code>: placeholder Graph helpers (auth, fetch attachments) \u2014 to be implemented.</li> <li><code>connectors/m365_mail/subscriptions.py</code>: placeholder subscription manager \u2014 to be implemented.</li> <li><code>connectors/m365_mail/version.py</code>.</li> <li><code>connectors/m365_mail/deploy/helm</code>: chart (ingress only for <code>/webhook_event</code>, HMAC on private routes).</li> <li><code>connectors/m365_mail/deploy/docker</code>: compose + deployment guide.</li> </ul>"},{"location":"connectors/m365-email/#initial-constraints-defaults","title":"Initial Constraints / Defaults","text":"<ul> <li>Start with <code>referenceAttachment: skip</code> (rely on storage connectors) \u2014 can add later.</li> <li>Actions disabled by default; enable via config.</li> <li>Mailbox scope configurable (start with explicit UPN list for safety).</li> </ul>"},{"location":"connectors/m365-email/#typical-actions-industry-patterns","title":"Typical Actions (Industry Patterns)","text":"<p>Many mature email security products (e.g., Proofpoint, Microsoft Defender) commonly apply these actions post\u2011verdict:</p> <ul> <li>Strip + Banner: Delete the attachment and deliver the message with a banner/placeholder inserted into the body (e.g., \u201cAttachment removed for security review\u201d). Often includes a link or note on how to retrieve from quarantine.</li> <li>Subject Tagging: Prepend a tag such as \u201c[Suspicious]\u201d or \u201c[External]\u201d to increase caution and support mail flow rules.</li> <li>Quarantine/Move: Move the entire message to a quarantine folder/mailbox for later review or release.</li> <li>URL Rewriting/Defang: Common in link protection; out of scope initially for attachments.</li> <li>Detonation + Delayed Delivery: Hold message until sandbox verdict returns; or \u201cdynamic delivery\u201d: deliver body immediately and re\u2011attach if clean.</li> <li>Metadata/Headers: Add X\u2011headers or categories for SOC/SIEM workflows (e.g., \u201cX\u2011Attachment\u2011Scan: malicious\u201d).</li> </ul> <p>For v1 of this connector, we propose: - Default action: Delete attachment + prepend an HTML banner (configurable). - Optional actions (toggle per policy): Move to Quarantine folder, Subject tagging, Add X\u2011headers. - Preserve audit data (message/attachment metadata) in Redis or by moving to an audit mailbox/folder for investigations.</p>"},{"location":"connectors/m365-email/#banner-template-example","title":"Banner Template (example)","text":"<p>Configurable banner (HTML) prepended to the message body upon malicious detection. Example:</p> <pre><code>&lt;div style=\"border:1px solid #f00;background:#fee;padding:12px;margin-bottom:12px;\"&gt;\n  &lt;strong&gt;Security Notice:&lt;/strong&gt; An attachment was removed from this email during scanning.\n  If you believe this is an error, please contact your security team.\n&lt;/div&gt;\n</code></pre> <p>The connector will PATCH the message body to prepend this HTML after deleting the attachment.</p>"},{"location":"connectors/m365-email/#webhooks-vs-gateways","title":"Webhooks vs Gateways","text":"<p>Two common models exist:</p> <ul> <li>Pre\u2011delivery (inline SEG/gateway): MX points to a gateway which scans before delivery (Pros: prevention; Cons: infrastructure, mail flow changes).</li> <li>Post\u2011delivery (API\u2011only): Microsoft Graph change notifications (webhooks) + delta queries (Pros: cloud\u2011native, simple to deploy; Cons: post\u2011delivery model, relies on timely API callbacks).</li> </ul> <p>This connector follows the post\u2011delivery approach: - Webhook\u2011first for near real\u2011time eventing. - Delta queries for backfill and resiliency (missed notifications, downtime windows).</p>"},{"location":"connectors/m365-email/#policy-defaults-proposed","title":"Policy Defaults (Proposed)","text":"<ul> <li><code>handle_reference_attachments=false</code></li> <li><code>enable_actions=false</code> (when enabled, default = delete attachment + banner; quarantine and tagging optional)</li> <li><code>max_attachment_bytes=50MB</code></li> <li><code>mailbox_upns</code> explicit list required initially (safer). \u201cAll users\u201d can be added later.</li> </ul>"},{"location":"connectors/m365-email/#graph-api-mapping-actions","title":"Graph API Mapping (Actions)","text":"<ul> <li>Delete attachment: <code>DELETE /users/{user}/messages/{id}/attachments/{attId}</code></li> <li>Insert banner: <code>PATCH /users/{user}/messages/{id}</code> with updated HTML body (prepend banner + original body)</li> <li>Move to Quarantine: <code>POST /users/{user}/messages/{id}/move</code> to folder name/path</li> <li>Subject tagging: <code>PATCH /users/{user}/messages/{id}</code> (subject prepend)</li> <li>Categories/Headers: <code>PATCH /users/{user}/messages/{id}</code> (categories); custom headers limited via Graph \u2014 prefer categories/tags</li> </ul>"},{"location":"connectors/m365-email/#mermaid-webhooks-delta-actions","title":"Mermaid: Webhooks + Delta + Actions","text":"<pre><code>sequenceDiagram\n    participant Graph as Microsoft Graph\n    participant Conn as m365\u2011mail\u2011connector\n    participant KV as dsx\u2011connect KV (state)\n    participant DSX as DSX\u2011Connect Core\n    participant Q as Queues (Redis)\n    participant W as Workers\n    participant DSXA as DSXA Scanner\n\n    Note over Graph, Conn: Webhook notifications (POST)\n    Graph-&gt;&gt;+Conn: Change notification(s)\n    Conn-&gt;&gt;Conn: Validate clientState / signatures\n    Conn-&gt;&gt;Graph: GET message metadata (+attachments)\n    alt has fileAttachment(s)\n        loop For each fileAttachment\n            Conn-&gt;&gt;DSX: POST /scan/request (location=m365://user/messages/{mid}/attachments/{aid})\n            DSX-&gt;&gt;Q: Enqueue scan_request_task\n        end\n    else No attachments\n        Note over Conn: Ignore or delta will catch later\n    end\n\n    Note over DSX, DSXA: Asynchronous processing\n    W-&gt;&gt;Q: Dequeue scan_request_task\n    W-&gt;&gt;+Conn: POST /{connector}/read_file (location)\n    Conn-&gt;&gt;Graph: GET /attachments/{id}/$value (stream)\n    Conn--&gt;&gt;-W: StreamingResponse (content)\n    W-&gt;&gt;+DSXA: POST /scan/binary/v2 (content)\n    DSXA--&gt;&gt;-W: Verdict (benign/malicious)\n    alt Malicious &amp; actions enabled\n        W-&gt;&gt;+Conn: PUT /{connector}/item_action\n        Conn-&gt;&gt;Graph: DELETE attachment\n        Conn-&gt;&gt;Graph: PATCH message body (prepend banner)\n        opt subject tag\n            Conn-&gt;&gt;Graph: PATCH subject (prefix)\n        end\n        opt move to Quarantine\n            Conn-&gt;&gt;Graph: GET/POST ensure dest folder\n            Conn-&gt;&gt;Graph: POST /messages/{id}/move\n        end\n        Conn--&gt;&gt;-W: ItemActionStatusResponse\n    end\n\n    Note over Conn, KV: Delta queries for drift correction\n    Conn-&gt;&gt;KV: GET delta:&lt;upn&gt; (ns=m365)\n    alt token exists\n        Conn-&gt;&gt;Graph: GET /messages/delta (token)\n    else no token\n        Conn-&gt;&gt;Graph: GET /messages/delta (initial window)\n    end\n    Graph--&gt;&gt;Conn: Changed messages (page(s)) + nextDeltaLink\n    loop Changed messages\n        Conn-&gt;&gt;Graph: GET message, list attachments\n        Conn-&gt;&gt;DSX: POST /scan/request (for each fileAttachment)\n    end\n    Conn-&gt;&gt;KV: PUT delta:&lt;upn&gt; (update token)</code></pre>"},{"location":"connectors/m365-email/#webhook-validation-clientstate","title":"Webhook Validation + clientState","text":"<ul> <li>GET validation: Graph sends a verification request with <code>validationToken</code> query param. The connector echoes this token as plain text.</li> <li>ClientState (optional): configure a shared <code>clientState</code> and the connector will verify it on webhook deliveries.</li> </ul>"},{"location":"connectors/m365-email/#kv-state-delta-tokens","title":"KV State (Delta Tokens)","text":"<ul> <li>Delta tokens are stored via dsx\u2011connect\u2019s HMAC\u2011protected KV endpoints.</li> <li>See Operations \u2192 Connector State KV for usage.</li> </ul>"},{"location":"deployment/authentication/","title":"Authentication","text":"<p>Mandatory authentication keeps every connector and dsx-connect call tied to a registered identity. Once enabled, only connectors that successfully enroll can invoke dsx-connect APIs (including high-impact actions such as <code>item_action</code>, <code>read_file</code>, or <code>delete</code>). Likewise, dsx-connect can no longer invoke a connector\u2019s private endpoints unless the request is DSX-HMAC signed, so ad-hoc <code>curl</code>, Swagger UI, or Postman calls against connector routes simply fail.</p> <p>Use this page as part of the Kubernetes deployment flow (the same mechanisms exist in Docker Compose, but production-grade TLS/auth combos are expected to run via Helm).</p>"},{"location":"deployment/authentication/#how-it-works","title":"How it works","text":"<ol> <li>Bootstrap (Enrollment Token). You provide dsx-connect with one or more enrollment tokens. A connector can register only if it presents a valid token.</li> <li>Per-connector HMAC. During registration, dsx-connect issues a unique <code>hmac_key_id</code>/<code>hmac_secret</code>, stores it in Redis, and returns it to the connector (never persisted on disk).</li> <li>Mutual DSX-HMAC. After bootstrap, every connector \u2194 dsx-connect request must include the <code>Authorization: DSX-HMAC ...</code> header. This guarantees that dangerous ops such as <code>item_action</code> run only against connectors that dsx-connect provisioned, and likewise connectors accept commands only from the authenticated dsx-connect API.</li> </ol> <pre><code>sequenceDiagram\n    participant Connector\n    participant API as dsx-connect API\n    Connector-&gt;&gt;API: POST /connectors/register\\nHeader: X-Enrollment-Token=TOKEN\n    API--&gt;&gt;Connector: 201 + hmac_key_id &amp; hmac_secret\n    Note over Connector,API: Connector keeps HMAC credentials in memory only\n    Connector-&gt;&gt;API: DSX-HMAC signed requests (scan/request, enqueue_done, ...)\n    API-&gt;&gt;Connector: DSX-HMAC signed requests (full_scan, read_file, ...)</code></pre>"},{"location":"deployment/authentication/#server-dsx-connect-chart-settings","title":"Server (dsx-connect chart) settings","text":"<p>Enable auth via Helm values:</p> <pre><code>dsx-connect-api:\n  auth:\n    enabled: true\n    enrollment:\n      secretName: dsx-connect-api-auth-enrollment\n      key: token\n</code></pre> <ul> <li><code>auth.enabled</code>: flips the global DSX-HMAC requirement on connector-only APIs.</li> <li><code>auth.enrollment.secretName</code> / <code>auth.enrollment.key</code>: reference the Secret that holds your enrollment token (<code>kubectl apply -f examples/secrets/auth-enrollment-secret.yaml</code>).</li> <li>Use <code>auth.enrollment.extraTokens</code> (CSV) if you need to overlap tokens during rotation.</li> </ul> <p>When auth is enabled, dsx-connect still accepts the enrollment token on protected endpoints as a break-glass path. Remove that Secret access in production if you don\u2019t need the escape hatch.</p>"},{"location":"deployment/authentication/#connector-chart-settings","title":"Connector chart settings","text":"<p>Every connector chart exposes the same block:</p> <pre><code>auth_dsxconnect:\n  enabled: true                # Enforce DSX-HMAC verification on inbound (dsx-connect \u2192 connector) calls\n  enrollmentSecretName: aws-s3-connector-env\n  enrollmentKey: DSXCONNECT_ENROLLMENT_TOKEN\n</code></pre> <ul> <li>Set <code>auth_dsxconnect.enabled=true</code> so the connector rejects unsigned calls (FastAPI docs/endpoints are hidden as soon as this flag is true).</li> <li>Provide the same Secret/key that contains <code>DSXCONNECT_ENROLLMENT_TOKEN</code>. Typically you create this Secret from your <code>.env</code> file (<code>kubectl create secret generic ... --from-env-file</code>).</li> <li>Outbound requests automatically include DSX-HMAC once registration succeeds\u2014you don\u2019t configure the key/secret directly.</li> </ul>"},{"location":"deployment/authentication/#operational-guidance","title":"Operational guidance","text":"<ul> <li>Why it matters: Without DSX-HMAC, an attacker who knows your connector IPs could call <code>/item_action</code>, <code>/read_file</code>, or <code>/full_scan</code> directly. With auth enabled, the connector answers only to signed dsx-connect requests. Meanwhile dsx-connect rejects any connector request that doesn\u2019t include a valid key/secret pair it minted.</li> <li>Registration failures (401): usually mean the connector Secret doesn\u2019t match the dsx-connect enrollment Secret. Redeploy the connector with the correct enrollment Secret, or rotate the API token to match the connector.</li> <li>Unsigned inbound calls: If you enable <code>auth_dsxconnect.enabled=true</code>, the connector logs \u201cmissing DSX-HMAC header\u201d errors when dsx-connect isn\u2019t configured yet\u2014verify that dsx-connect\u2019s <code>auth.enabled</code> is true and registration succeeded.</li> <li>Rotation: Populate <code>auth.enrollment.extraTokens</code> (server) or temporarily expand the Secret to include both old/new tokens. Roll connectors so they pick up the new token, confirm re-registration, then remove the old token.</li> <li>Local testing: Keep auth disabled in Compose quickstarts. Flip both blocks on in a dev cluster when you need end-to-end coverage; connectors will show <code>Connector authentication: enrollment token provided; DSX-HMAC verification enabled.</code> on boot.</li> </ul> <p>For deeper reference\u2014including curl samples and template internals\u2014see <code>dsx_connect/deploy/helm/DSX-AUTHENTICATION.md</code>.</p>"},{"location":"deployment/k8s-guides/","title":"Kubernetes Guides (per connector)","text":"<p>Connector Helm guides live under each connector repo path:</p> <ul> <li>connectors/aws_s3/deploy/helm/DEPLOYMENT_GUIDE_K8S.md</li> <li>connectors/azure_blob_storage/deploy/helm/DEPLOYMENT_GUIDE_K8S.md</li> <li>connectors/filesystem/deploy/helm/DEPLOYMENT_GUIDE_K8S.md</li> <li>connectors/google_cloud_storage/deploy/helm/DEPLOYMENT_GUIDE_K8S.md</li> <li>connectors/sharepoint/deploy/helm/DEPLOYMENT_GUIDE_K8S.md</li> </ul>"},{"location":"deployment/k8s-guides/#filters-central-reference","title":"Filters (Central Reference)","text":"<ul> <li>To avoid repeating the filter rules in each guide, see Reference \u2192 Filters for the rsync\u2011style include/exclude semantics used across all connectors.</li> </ul>"},{"location":"deployment/log-collectors/","title":"Log Collectors and Fan-Out","text":"<p>This page collects the log-collector appendix shipped with the Helm charts. It explains how dsx-connect workers emit scan-result events over syslog and describes the reference collector deployments you can use to forward those events to your SIEMs.</p> <ul> <li>Emission: workers log JSON over syslog (TCP/TLS recommended). Key env vars:</li> <li><code>DSXCONNECT_SYSLOG__TRANSPORT=tcp|tls|udp</code></li> <li><code>DSXCONNECT_SYSLOG__SYSLOG_SERVER_URL=&lt;collector service&gt;</code></li> <li><code>DSXCONNECT_SYSLOG__SYSLOG_SERVER_PORT=&lt;port&gt;</code></li> </ul>"},{"location":"deployment/log-collectors/#option-a-rsyslog-helm-subchart","title":"Option A \u2014 rsyslog (Helm subchart)","text":"<p>The repo includes <code>deploy/helm/charts/rsyslog</code>, a wrapper around the upstream image that listens on TCP 514 and optionally forwards to Splunk HEC or another SIEM.</p> <p>Install:</p> <pre><code>helm upgrade --install rsyslog \\\n  dsx_connect/deploy/helm/charts/rsyslog\n</code></pre> <p>Key values:</p> <pre><code>service:\n  type: ClusterIP\n  tcpPort: 514\nconfig:\n  writeToStdout: true\n  forward:\n    enabled: true\n    target: siem.example.com\n    port: 6514\n    tls: true\n</code></pre> <p>Point dsx-connect at the collector (worker <code>.env</code>):</p> <pre><code>DSXCONNECT_SYSLOG__TRANSPORT=tcp\nDSXCONNECT_SYSLOG__SYSLOG_SERVER_URL=rsyslog\nDSXCONNECT_SYSLOG__SYSLOG_SERVER_PORT=514\n</code></pre>"},{"location":"deployment/log-collectors/#option-b-syslog-ng-reference-config","title":"Option B \u2014 syslog-ng (reference config)","text":"<p>Example <code>syslog-ng.conf</code>:</p> <pre><code>@version: 3.38\noptions { chain-hostnames(no); flush-lines(1); keep-hostname(yes); };\nsource s_net {\n  syslog(ip(\"0.0.0.0\") transport(\"tcp\") port(514));\n  syslog(ip(\"0.0.0.0\") transport(\"udp\") port(514));\n};\ndestination d_stdout { file(\"/dev/stdout\"); };\nlog { source(s_net); destination(d_stdout); };\n</code></pre> <p>Add additional destinations (Splunk HEC, etc.) as needed.</p>"},{"location":"deployment/log-collectors/#option-c-fluent-bit-reference-config","title":"Option C \u2014 Fluent Bit (reference config)","text":"<p>Fluent Bit can replace rsyslog/syslog-ng and fan-out to multiple cloud services:</p> <pre><code>[INPUT]\n    Name   syslog\n    Mode   tcp\n    Listen 0.0.0.0\n    Port   514\n    Parser json\n\n[OUTPUT]\n    Name   splunk\n    Match  *\n    Host   splunk.example.com\n    Port   8088\n    TLS    On\n    Splunk_Token YOUR_TOKEN\n    Splunk_Send_Raw On\n\n[OUTPUT]\n    Name   cloudwatch_logs\n    Match  *\n    region us-west-2\n    log_group_name  dsx-connect\n    log_stream_name dsx-${HOSTNAME}\n</code></pre>"},{"location":"deployment/log-collectors/#notes","title":"Notes","text":"<ul> <li>Prefer TCP/TLS for transport; Kubernetes <code>kubectl port-forward</code> works only with TCP.</li> <li>Collectors can log to stdout for <code>kubectl logs</code> inspection and/or to persistent volumes.</li> <li>Fluent Bit, Vector, and syslog-ng all offer rich destination support; pick the collector that matches your organization\u2019s logging stack.</li> </ul>"},{"location":"deployment/docker/aws-s3/","title":"AWS S3 Connector \u2014 Docker Compose","text":"<p>This guide shows how to deploy the AWS S3 connector with Docker Compose for quick testing/POV.</p>"},{"location":"deployment/docker/aws-s3/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed locally (or a container VM)</li> <li>AWS credentials (as env vars or a secret) with permissions to list/read (and optionally write/move/delete) objects</li> <li>A Docker network shared with dsx\u2011connect (example: <code>dsx-connect-network</code>)</li> </ul>"},{"location":"deployment/docker/aws-s3/#compose-file","title":"Compose File","text":"<p>Use the example at <code>connectors/aws_s3/deploy/docker/docker-compose-aws-s3-connector.yaml</code> as a starting point.</p>"},{"location":"deployment/docker/aws-s3/#core-connector-env-common-across-connectors","title":"Core connector env (common across connectors)","text":"Variable Description <code>DSXCONNECTOR_DSX_CONNECT_URL</code> dsx\u2011connect base URL (use <code>http://dsx-connect-api:8586</code> on the shared Docker network). <code>DSXCONNECTOR_CONNECTOR_URL</code> Callback URL dsx-connect uses to reach the connector (defaults to the service name inside the Docker network). <code>DSXCONNECTOR_ASSET</code> Target bucket or <code>bucket/prefix</code> to scope listings. <code>DSXCONNECTOR_FILTER</code> Optional rsync\u2011style include/exclude rules relative to the asset. <code>DSXCONNECTOR_ITEM_ACTION</code> What to do on malicious verdicts (<code>nothing</code>, <code>delete</code>, <code>move</code>, <code>move_tag</code>). Set to <code>move</code>/<code>move_tag</code> to trigger connector-side quarantine. <code>DSXCONNECTOR_ITEM_ACTION_MOVE_METAINFO</code> Destination bucket/prefix to receive moved objects when <code>item_action</code> is <code>move</code> or <code>move_tag</code>."},{"location":"deployment/docker/aws-s3/#aws-specific-settings","title":"AWS-specific settings","text":"Variable Description <code>AWS_ACCESS_KEY_ID</code> Access key with List/Get (and optional Put/Delete) permissions for the target bucket. <code>AWS_SECRET_ACCESS_KEY</code> Secret for the access key above. <p>Example: <pre><code>docker compose -f connectors/aws_s3/deploy/docker/docker-compose-aws-s3-connector.yaml up -d\n</code></pre></p>"},{"location":"deployment/docker/aws-s3/#assets-and-filters","title":"Assets and Filters","text":"<ul> <li><code>DSXCONNECTOR_ASSET</code> should be set to your bucket (e.g., <code>my-bucket</code>) or <code>bucket/prefix</code> to scope listings.</li> <li>If a prefix is provided, listings start at that sub\u2011root and filters are evaluated relative to it.</li> <li>See Reference \u2192 Assets &amp; Filters to understand sharding and partitioning patterns.</li> </ul>"},{"location":"deployment/docker/aws-s3/#notes","title":"Notes","text":"<ul> <li>Use <code>DSXCONNECTOR_ASSET</code> to configure the target bucket or <code>bucket/prefix</code>.</li> </ul>"},{"location":"deployment/docker/aws-s3/#tls-options","title":"TLS Options","text":"<ul> <li><code>DSXCONNECTOR_USE_TLS</code>: Serve the connector over HTTPS (mount cert/key and set this to <code>true</code>).</li> <li><code>DSXCONNECTOR_TLS_CERTFILE</code> / <code>DSXCONNECTOR_TLS_KEYFILE</code>: Paths to the mounted certificate and private key when TLS is enabled.</li> <li><code>DSXCONNECTOR_VERIFY_TLS</code>: Keep <code>true</code> (default) to verify dsx-connect\u2019s certificate; set to <code>false</code> only for local dev.</li> <li><code>DSXCONNECTOR_CA_BUNDLE</code>: Optional CA bundle path when verifying dsx-connect with a private CA.</li> </ul>"},{"location":"deployment/docker/aws-s3/#webhook-exposure","title":"Webhook Exposure","text":"<p>If you forward events into the connector\u2019s HTTP endpoints (e.g., using tunnels or an external load balancer), expose the host port mapped to <code>8600</code> (default in compose) and point your upstream system at that URL. <code>DSXCONNECTOR_CONNECTOR_URL</code> should remain the Docker-network URL (e.g., <code>http://aws-s3-connector:8600</code>) so dsx-connect can reach the service internally.</p>"},{"location":"deployment/docker/aws-s3/#provider-notes-aws-s3","title":"Provider Notes (AWS S3)","text":"<ul> <li>Region/Endpoint: ensure the connector can reach the correct S3 endpoint for your bucket\u2019s region.</li> <li>IAM Policies: least\u2011privilege for List/Get; add Put/Delete if actions are enabled.</li> <li>SSE\u2011KMS: if objects are KMS\u2011encrypted, confirm key permissions for decryption.</li> <li>Path\u2011style vs Virtual host: modern S3 endpoints default to virtual host; avoid path\u2011style unless required by your setup.</li> </ul>"},{"location":"deployment/docker/azure-blob/","title":"Azure Blob Storage Connector \u2014 Docker Compose","text":"<p>This guide shows how to deploy the Azure Blob connector with Docker Compose for quick testing/POV.</p>"},{"location":"deployment/docker/azure-blob/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed locally (or a container VM)</li> <li>Azure Storage credentials with permissions to list/read (and optionally write/move/delete) blobs:</li> <li>Connection string (recommended for POV) or SAS/Managed Identity as applicable</li> <li>A Docker network shared with dsx\u2011connect (example: <code>dsx-connect-network</code>)</li> </ul>"},{"location":"deployment/docker/azure-blob/#compose-file","title":"Compose File","text":"<p>Use <code>connectors/azure_blob_storage/deploy/docker/docker-compose-azure-blob-storage-connector.yaml</code> as a starting point.</p>"},{"location":"deployment/docker/azure-blob/#core-connector-env-common-across-connectors","title":"Core connector env (common across connectors)","text":"Variable Description <code>DSXCONNECTOR_DSX_CONNECT_URL</code> dsx\u2011connect base URL (use <code>http://dsx-connect-api:8586</code> on the shared Docker network). <code>DSXCONNECTOR_CONNECTOR_URL</code> Callback URL dsx-connect uses to reach the connector (defaults to the service name inside the Docker network). <code>DSXCONNECTOR_ASSET</code> Container or <code>container/prefix</code> to scope listings. <code>DSXCONNECTOR_FILTER</code> Optional rsync\u2011style include/exclude rules relative to the asset. <code>DSXCONNECTOR_ITEM_ACTION</code> What to do on malicious verdicts (<code>nothing</code>, <code>delete</code>, <code>move</code>, <code>move_tag</code>). Use <code>move</code>/<code>move_tag</code> to relocate blobs after verdict. <code>DSXCONNECTOR_ITEM_ACTION_MOVE_METAINFO</code> Destination container/prefix for moved blobs when using <code>move</code>/<code>move_tag</code>."},{"location":"deployment/docker/azure-blob/#azure-specific-settings","title":"Azure-specific settings","text":"Variable Description <code>AZURE_STORAGE_CONNECTION_STRING</code> Connection string for the storage account (store via secrets). <p>Example: <pre><code>docker compose -f connectors/azure_blob_storage/deploy/docker/docker-compose-azure-blob-storage-connector.yaml up -d\n</code></pre></p>"},{"location":"deployment/docker/azure-blob/#assets-and-filters","title":"Assets and Filters","text":"<ul> <li><code>DSXCONNECTOR_ASSET</code> should be set to your container (e.g., <code>my-container</code>) or <code>container/prefix</code> to scope listings.</li> <li>If a prefix is provided, listings start at that sub\u2011root and filters are evaluated relative to it.</li> <li>See Reference \u2192 Assets &amp; Filters for sharding/partition guidance.</li> </ul>"},{"location":"deployment/docker/azure-blob/#notes","title":"Notes","text":"<ul> <li>Provide <code>AZURE_STORAGE_CONNECTION_STRING</code> (or other supported auth env) via secrets for security.</li> </ul>"},{"location":"deployment/docker/azure-blob/#tls-options","title":"TLS Options","text":"<ul> <li><code>DSXCONNECTOR_USE_TLS</code>: Serve the connector over HTTPS (mount cert/key and enable as needed).</li> <li><code>DSXCONNECTOR_TLS_CERTFILE</code> / <code>DSXCONNECTOR_TLS_KEYFILE</code>: Paths to the mounted certificate and key when TLS is enabled.</li> <li><code>DSXCONNECTOR_VERIFY_TLS</code>: Keep <code>true</code> (default) to verify dsx-connect\u2019s certificate; set to <code>false</code> only for local dev.</li> <li><code>DSXCONNECTOR_CA_BUNDLE</code>: Optional CA bundle path when verifying dsx-connect with a private CA.</li> </ul>"},{"location":"deployment/docker/azure-blob/#webhook-exposure","title":"Webhook Exposure","text":"<p>If you expose connector endpoints (e.g., for HTTP callbacks) outside Docker, tunnel or publish the host port mapped to <code>8610</code> (compose default). Keep <code>DSXCONNECTOR_CONNECTOR_URL</code> pointing to the Docker-network address (e.g., <code>http://azure-blob-storage-connector:8610</code>) so dsx-connect can reach the service internally.</p>"},{"location":"deployment/docker/azure-blob/#provider-notes-azure-blob","title":"Provider Notes (Azure Blob)","text":"<ul> <li>Auth: connection string works well for POV; SAS or managed identity might be used in production.</li> <li>HNS (ADLS Gen2): hierarchical namespace affects path semantics; test your prefixes under HNS.</li> <li>Listing costs: large containers can incur list costs; sharding by asset improves performance.</li> <li>SAS Expiry: ensure long enough validity for ongoing scans.</li> </ul>"},{"location":"deployment/docker/dsx-connect/","title":"dsx-connect Core \u2014 Docker Compose","text":"<p>This guide walks through running the full dsx-connect platform (API + workers + Redis + optional log collector + optional DSXA scanner) using the compose files under <code>dsx_connect/deploy/docker/</code>.</p>"},{"location":"deployment/docker/dsx-connect/#files-in-this-package","title":"Files in This Package","text":"<p>Path: <code>dsx_connect/deploy/docker</code></p> <ul> <li><code>docker-compose-dsx-connect-all-services.yaml</code> \u2014 API, Redis, Celery workers, optional rsyslog profile, SSE dependencies.</li> <li><code>docker-compose-dsxa.yaml</code> \u2014 optional DSXA scanner for local malware verdicts.</li> <li><code>certs/</code> \u2014 dev TLS certificates + generator script (<code>generate-dev-cert.sh</code>).</li> <li><code>README.md</code> \u2014 legacy deployment notes (this page mirrors and expands on it).</li> </ul>"},{"location":"deployment/docker/dsx-connect/#architecture-components","title":"Architecture &amp; Components","text":"<p>For the full diagrams see Overview. At a glance:</p> <ul> <li>dsx_connect_api: FastAPI app on port 8586 (REST + SSE + dashboard).</li> <li>Redis: queue broker + cache + pub/sub.</li> <li>Celery workers: scan_request, verdict_action, results, notification.</li> <li>Log collector: rsyslog profile (optional, defaults to stdout only).</li> <li>DSXA scanner: malware analysis engine (optional compose file).</li> </ul> <p></p> <p>All worker communication flows through Redis queues. Scan requests enter <code>scan_request_queue</code>, DSXA handles scanning, verdicts/actions are enqueued, results are persisted/published, and SSE keeps the UI in sync.</p>"},{"location":"deployment/docker/dsx-connect/#core-vs-connector-env","title":"Core vs Connector Env","text":"Core Env (common) Description <code>DSXCONNECT_REDIS_URL</code> / <code>DSXCONNECT_TASKQUEUE__*</code> Queue broker + backend URLs (defaults: <code>redis://redis:6379/0</code>). <code>DSXCONNECT_RESULTS_DB</code> / <code>DSXCONNECT_RESULTS_DB__RETAIN</code> Results DB backend + retention. Use Redis for demos or set to in-memory for ephemeral use. <code>DSXCONNECT_USE_TLS</code>, <code>DSXCONNECT_TLS_CERTFILE</code>, <code>DSXCONNECT_TLS_KEYFILE</code> Enable HTTPS on the API (see TLS section). <code>DSXCONNECT_SCANNER__SCAN_BINARY_URL</code> Endpoint for DSXA (<code>http://dsxa_scanner:5000/scan/binary/v2</code> when using the companion compose file)."},{"location":"deployment/docker/dsx-connect/#tls-options","title":"TLS Options","text":"<ul> <li>Dev certs: run <code>certs/generate-dev-cert.sh</code> \u2192 mount <code>/app/certs</code> \u2192 set <code>DSXCONNECT_USE_TLS=true</code>.</li> <li>Production certs (volume mount):   <pre><code>services:\n  dsx_connect_api:\n    volumes:\n      - ./certs:/app/certs:ro\n    environment:\n      DSXCONNECT_USE_TLS: \"true\"\n      DSXCONNECT_TLS_CERTFILE: \"/app/certs/server.crt\"\n      DSXCONNECT_TLS_KEYFILE: \"/app/certs/server.key\"\n</code></pre></li> <li>Baking certs into the image: copy certs in a custom Dockerfile and <code>chown/chmod</code> appropriately.</li> <li>Connectors: set <code>DSXCONNECTOR_VERIFY_TLS=true</code> and optionally <code>DSXCONNECTOR_CA_BUNDLE</code> to the CA path. Only skip verification for quick local demos.</li> </ul>"},{"location":"deployment/docker/dsx-connect/#component-details","title":"Component Details","text":""},{"location":"deployment/docker/dsx-connect/#dsx_connect_api-fastapi","title":"dsx_connect_api (FastAPI)","text":"<ul> <li>REST API, SSE stream, UI dashboard, job management.</li> <li>Health check: <code>GET /dsx-connect/api/v1/healthz</code>.</li> <li>Dependencies: Redis (queues + SSE pub/sub).</li> </ul>"},{"location":"deployment/docker/dsx-connect/#redis","title":"Redis","text":"<ul> <li>Task broker + cache.</li> <li>Keyspace notifications enabled for SSE broadcasts.</li> <li>Lives on the compose network as <code>redis</code>.</li> </ul>"},{"location":"deployment/docker/dsx-connect/#celery-workers","title":"Celery Workers","text":"Worker Queue Default concurrency Responsibilities <code>dsx_connect_scan_request_worker</code> <code>scan_request_queue</code> 2 Fetch files from connectors, submit to DSXA, enqueue verdicts. IO-bound; scale this first. <code>dsx_connect_verdict_action_worker</code> <code>verdict_action_queue</code> 1 Execute post-scan actions (delete/move/tag). Calls back into connectors. <code>dsx_connect_results_worker</code> <code>scan_result_queue</code> 1 Persist results, update stats, forward to syslog. <code>dsx_connect_notification_worker</code> <code>scan_result_notification_queue</code> 1 Publish events via Redis pub/sub, SSE, optional webhooks. <p>Worker commands follow <code>celery -A dsx_connect.celery_app.celery_app worker --loglevel=warning -Q &lt;queue&gt; --concurrency=&lt;n&gt;</code>. Increase <code>--concurrency</code> or scale the service to parallelize.</p>"},{"location":"deployment/docker/dsx-connect/#log-collector-rsyslog","title":"Log Collector (rsyslog)","text":"<ul> <li>Enable the <code>rsyslog</code> profile to collect events; it writes to stdout for easy <code>docker logs</code>.</li> <li>dsx_connect_results_worker sends JSON events to <code>syslog:514</code> by default. Override <code>DSXCONNECT_SYSLOG__SYSLOG_SERVER_URL</code>/<code>PORT</code> to point at an external collector or leave unset to disable.</li> </ul>"},{"location":"deployment/docker/dsx-connect/#dsxa-scanner-optional","title":"DSXA Scanner (Optional)","text":"<ul> <li>Bring up via <code>docker compose -f docker-compose-dsxa.yaml up -d</code>.</li> <li>The scan request worker hits DSXA at <code>http://dsxa_scanner:5000/scan/binary/v2</code>.</li> <li>Can swap with a remote DSXA URL without changing compose; just override <code>DSXCONNECT_SCANNER__SCAN_BINARY_URL</code>.</li> </ul>"},{"location":"deployment/docker/dsx-connect/#deployment-via-docker-compose","title":"Deployment via Docker Compose","text":""},{"location":"deployment/docker/dsx-connect/#env-file-and-pinned-image-tags","title":"Env file and pinned image tags","text":"<ul> <li>Copy the sample env and pin tags: <pre><code>cp docker-compose.env.sample .env\n# edit .env to set DSXCONNECT_IMAGE=dsxconnect/dsx-connect:&lt;release-tag&gt;\n# and any DSXA/connector tags you want to pin\n</code></pre></li> <li> <p>Compose will read <code>.env</code> automatically; <code>docker-compose-dsx-connect-all-services.yaml</code> uses <code>DSXCONNECT_IMAGE</code> for the core image.</p> </li> <li> <p>Create shared network (once) <pre><code>docker network create dsx-connect-network --driver bridge\n</code></pre></p> </li> <li>Start DSXA scanner (optional)     If you want to use a DSXA scanner deployed withing the same Docker, use this.  If you have an existing DSXA scanner, set <code>DSXCONNECT_SCANNER__SCAN_BINARY_URL</code> to point at it.     <pre><code>docker compose -f docker-compose-dsxa.yaml up -d\n</code></pre></li> <li>Start dsx-connect stack <pre><code>cd dsx_connect/deploy/docker\ndocker compose -f docker-compose-dsx-connect-all-services.yaml up -d\n</code></pre>    Expected output (example):    <pre><code>[+] Running 8/8\n\u2714 Network dsx-connect-network                Created\n\u2714 Container dsx-connect-redis-1              Started\n\u2714 Container dsx-connect-rsyslog-1            Started\n\u2714 Container dsx-connect-dsx_connect_api-1    Started\n\u2714 Container dsx-connect-scan_request_worker-1 Started\n\u2714 Container dsx-connect-verdict_action_worker-1 Started\n\u2714 Container dsx-connect-results_worker-1     Started\n\u2714 Container dsx-connect-notification_worker-1 Started\n</code></pre></li> <li>Verify </li> <li>API: http://localhost:8586  </li> <li><code>docker compose -f docker-compose-dsx-connect-all-services.yaml ps</code> to confirm healthy containers.  </li> <li>Logs: <code>docker compose -f ... logs -f dsx_connect_api</code></li> <li>Stop <pre><code>docker compose -f docker-compose-dsx-connect-all-services.yaml down\n</code></pre></li> </ul>"},{"location":"deployment/docker/dsx-connect/#authentication","title":"Authentication","text":"<ul> <li>Docker Compose deployments intentionally run with connector auth disabled (no enrollment tokens, connectors unauthenticated). This keeps local demos simple.</li> <li>For production-grade deployments with enrollment + DSX-HMAC enforced, use the Helm charts (<code>dsx_connect/deploy/helm</code>) where secrets and toggles are managed securely.</li> </ul>"},{"location":"deployment/docker/dsx-connect/#common-troubleshooting","title":"Common Troubleshooting","text":"Symptom Fix Port 8586 already in use Edit compose file to remap API port or stop the conflicting service. Workers stuck waiting for Redis Ensure Redis health check passes; look at <code>docker logs dsx-connect-redis-1</code>. SSE clients disconnecting API has a 30s graceful shutdown window; ensure you stop the stack with <code>docker compose down</code> to let SSE flush. Syslog not receiving events Start the stack with the <code>rsyslog</code> profile, or point <code>DSXCONNECT_SYSLOG__SYSLOG_SERVER_URL</code> at your collector. Need persistent Redis Mount a volume to <code>/data</code> in the Redis service. Large backlogs Scale <code>dsx_connect_scan_request_worker</code> (and connectors/DSXA) to increase throughput."},{"location":"deployment/docker/filesystem/","title":"Filesystem Connector \u2014 Docker Compose","text":"<p>This guide shows how to deploy the Filesystem connector with Docker Compose for quick testing/POV. The connector itself always reads from <code>/app/scan_folder</code> and writes quarantine actions to <code>/app/quarantine</code> inside the container, so your job is simply to mount whichever filesystem you want to scan (local folder, NAS, cloud share, etc.) to those paths.</p>"},{"location":"deployment/docker/filesystem/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed locally (or a container VM)</li> <li>A host folder to scan (and optionally a quarantine folder), mounted into the container</li> <li>A Docker network shared with dsx\u2011connect (example: <code>dsx-connect-network</code>)</li> </ul>"},{"location":"deployment/docker/filesystem/#compose-file","title":"Compose File","text":"<p>Use <code>connectors/filesystem/deploy/docker/docker-compose-filesystem-connector.yaml</code> as a starting point.</p>"},{"location":"deployment/docker/filesystem/#core-connector-env-common-across-connectors","title":"Core connector env (common across connectors)","text":"Variable Description <code>DSXCONNECTOR_DSX_CONNECT_URL</code> dsx-connect base URL (e.g., <code>http://dsx-connect-api:8586</code> on the shared Docker network). <code>DSXCONNECTOR_CONNECTOR_URL</code> Callback URL dsx-connect uses to reach the connector (defaults to the service name inside the Docker network). <code>DSXCONNECTOR_ASSET</code> Always <code>/app/scan_folder</code> inside the container; bind your host/NAS path to this mount. <code>DSXCONNECTOR_FILTER</code> Optional rsync-style rules evaluated relative to <code>/app/scan_folder</code>. <code>DSXCONNECTOR_ITEM_ACTION</code> What to do on malicious verdicts (<code>nothing</code>, <code>delete</code>, <code>move</code>, <code>move_tag</code>). Use <code>move</code>/<code>move_tag</code> to relocate files into the quarantine mount. <code>DSXCONNECTOR_ITEM_ACTION_MOVE_METAINFO</code> Target path interpreted by the connector when moving files (defaults to <code>/app/quarantine</code>)."},{"location":"deployment/docker/filesystem/#filesystem-specific-settings","title":"Filesystem-specific settings","text":"Field / Env Description <code>SCAN_FOLDER_PATH</code> Host or NAS path you want to scan (Compose anchor bound into <code>/app/scan_folder</code>). <code>QUARANTINE_FOLDER_PATH</code> Host or NAS path for quarantined files (bound into <code>/app/quarantine</code>). <code>DSXCONNECTOR_ASSET_DISPLAY_NAME</code> Overrides what the UI shows for the asset (set to the same host scan path for clarity). <code>DSXCONNECTOR_MONITOR</code> <code>true</code> to enable inotify-based monitoring of <code>/app/scan_folder</code>. <code>DSXCONNECTOR_MONITOR_FORCE_POLLING</code> <code>true</code> to poll instead of relying on inotify (useful for remote filesystems that don\u2019t emit events). <p>Example: <pre><code>docker compose -f connectors/filesystem/deploy/docker/docker-compose-filesystem-connector.yaml up -d\n</code></pre></p>"},{"location":"deployment/docker/filesystem/#configure-host-volumes-pre-flight","title":"Configure Host Volumes (pre-flight)","text":"<p>The Compose file defines two anchors you must customize before running:</p> <pre><code>x-common-paths: &amp;common-paths\n  SCAN_FOLDER_PATH: &amp;scan-folder \"/path/to/local/folder\"\n  QUARANTINE_FOLDER_PATH: &amp;quarantine-folder \"/path/to/local/folder/dsxconnect-quarantine\"\n</code></pre> <ul> <li><code>SCAN_FOLDER_PATH</code> should point to the host directory you want to scan.</li> <li><code>QUARANTINE_FOLDER_PATH</code> should point to the host directory where quarantined files can be moved.</li> </ul> <p>Those anchors are referenced in the <code>volumes</code> section:</p> <pre><code>    volumes:\n      - type: bind\n        source: *scan-folder        # host path to scan\n        target: /app/scan_folder    # in-container path (matches DSXCONNECTOR_ASSET)\n      - type: bind\n        source: *quarantine-folder  # host path for quarantine moves\n        target: /app/quarantine\n</code></pre> <p>Set the host paths once, then run <code>docker compose up</code>. No changes to <code>DSXCONNECTOR_ASSET</code> are required\u2014the connector always operates on <code>/app/scan_folder</code>, which now maps to your chosen host directory. To keep the dsx-connect UI readable, also set <code>DSXCONNECTOR_ASSET_DISPLAY_NAME</code> (already wired in the Compose file) to the same host path so the card shows <code>/Users/.../SAMPLES/PDF</code> instead of <code>/app/scan_folder</code>.</p>"},{"location":"deployment/docker/filesystem/#local-vs-remote-mounts","title":"Local vs Remote Mounts","text":"<ul> <li>Local bind mounts (default compose file): use <code>SCAN_FOLDER_PATH</code> / <code>QUARANTINE_FOLDER_PATH</code> anchors so Docker binds host directories into <code>/app/scan_folder</code> and <code>/app/quarantine</code>.</li> <li>Remote/NAS mounts: swap the scan volume with an NFS or SMB volume before binding to <code>/app/scan_folder</code>. The quarantine path can remain a local bind if desired, or also point to a NAS export.</li> </ul> <p>Example NFS compose snippet (<code>docker-compose-filesystem-connector-nfs.yaml</code>):</p> <pre><code>volumes:\n  nfs_mount:\n    driver: local\n    driver_opts:\n      type: \"nfs\"\n      o: \"addr=192.168.86.44,vers=3,nolock,tcp,resvport\"\n      device: \":/mnt/fileshare/scanshare\"\n\nservices:\n  filesystem_connector:\n    volumes:\n      - nfs_mount:/app/scan_folder\n      - type: bind\n        source: *quarantine-folder\n        target: /app/quarantine\n</code></pre> <p>Update the <code>addr</code>, <code>device</code>, and NFS options to match your NAS. This mounts the remote export inside the container so <code>DSXCONNECTOR_ASSET=/app/scan_folder</code> still works unchanged.</p> <p>Example SMB/CIFS snippet (requires <code>cifs-utils</code> on the Docker host):</p> <pre><code>volumes:\n  smb_mount:\n    driver: local\n    driver_opts:\n      type: cifs\n      o: \"username=svcaccount,password=changeme,vers=3.0,uid=1000,gid=1000\"\n      device: \"//fileserver01/share/scans\"\n\nservices:\n  filesystem_connector:\n    volumes:\n      - smb_mount:/app/scan_folder\n      - type: bind\n        source: *quarantine-folder\n        target: /app/quarantine\n</code></pre> <p>Adjust the credentials, share path, and <code>uid/gid</code> to match your environment. CIFS behaves like any other bind once mounted inside the container.</p> <p>Example AFS (OpenAFS) snippet (requires the host to have <code>openafs-client</code> and a mounted <code>/afs</code> tree):</p> <pre><code>services:\n  filesystem_connector:\n    volumes:\n      - type: bind\n        source: /afs/yourcell.com/projects/scans\n        target: /app/scan_folder\n      - type: bind\n        source: *quarantine-folder\n        target: /app/quarantine\n</code></pre> <p>Make sure the Docker host\u2019s AFS cache manager has tokens for the target path (<code>kinit</code> + <code>aklog</code>), and adjust the <code>/afs/...</code> path to match your cell. Once bound, the connector treats <code>/app/scan_folder</code> like any other volume.</p>"},{"location":"deployment/docker/filesystem/#assets-and-filters","title":"Assets and Filters","text":"<ul> <li><code>DSXCONNECTOR_ASSET</code> points to the path inside the container (default <code>/app/scan_folder</code>). Mount the host directory with <code>volumes:</code> so the container sees the real files, e.g., <code>./samples:/app/scan_folder</code>.</li> <li>For quarantine actions, mount a second host path (e.g., <code>./quarantine:/app/quarantine</code>) and set <code>DSXCONNECTOR_ITEM_ACTION_MOVE_METAINFO</code> if needed.</li> <li>Filters are always relative to the container path defined in <code>DSXCONNECTOR_ASSET</code>; they do not reference host paths directly.</li> <li>See Reference \u2192 Assets &amp; Filters for guidance on sharding and scoping.</li> </ul>"},{"location":"deployment/docker/filesystem/#webhook-exposure","title":"Webhook Exposure","text":"<p>Expose or tunnel the host port mapped to <code>8620</code> when dsx-connect (or other internal services) must reach private connector routes. Keep <code>DSXCONNECTOR_CONNECTOR_URL</code> set to the Docker-network hostname (e.g., <code>http://filesystem-connector:8620</code>) so dsx-connect resolves the service internally, and forward the host port through your preferred tunnel only for inbound events that originate outside Docker.</p>"},{"location":"deployment/docker/filesystem/#tls-options","title":"TLS Options","text":"<ul> <li><code>DSXCONNECTOR_USE_TLS</code>: Serve the connector over HTTPS (mount cert/key and enable as needed).</li> <li><code>DSXCONNECTOR_TLS_CERTFILE</code> / <code>DSXCONNECTOR_TLS_KEYFILE</code>: Paths to the mounted certificate and private key when TLS is enabled.</li> <li><code>DSXCONNECTOR_VERIFY_TLS</code>: Keep <code>true</code> (default) to verify dsx-connect\u2019s certificate; set to <code>false</code> only for local dev.</li> <li><code>DSXCONNECTOR_CA_BUNDLE</code>: Optional CA bundle path when verifying dsx-connect with a private CA.</li> </ul>"},{"location":"deployment/docker/filesystem/#notes","title":"Notes","text":"<ul> <li>Consider enabling monitor (<code>DSXCONNECTOR_MONITOR=true</code>) for real-time file change detection.</li> <li>If you can mount the storage into <code>/app/scan_folder</code>, the connector can scan it\u2014local disks, NAS shares, and remote filesystems all work once bound.</li> </ul>"},{"location":"deployment/docker/google-cloud-storage/","title":"Google Cloud Storage Connector \u2014 Docker Compose","text":"<p>This guide shows how to deploy the Google Cloud Storage connector with Docker Compose for quick testing/POV. Google Cloud Storage connectors support both full scan (on-demand) and continuous monitoring (on-access) scanning, as well as the ability to perform remediation actions on files (delete/move/tag/move and tag).</p> <p>For monitoring, users have two options:</p> <ul> <li>Use Google Cloud Pub/Sub notifications to trigger the connector (recommended)</li> <li>Use Cloud Run/Cloud Functions on Google Cloud to monitor the bucket and trigger the connector via the /webhook/event API.</li> </ul> <p>Pub/Sub notifications are the recommended approach, as it requires less maintenance and the extra step of monitoring middleware to send the connector events.  </p>"},{"location":"deployment/docker/google-cloud-storage/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed locally (or a container VM)</li> <li>A GCP service account JSON secret (mounted into the container) with permissions to list/read (and optionally write/move/delete) objects \u2014 see Reference \u2192 Google Cloud Credentials for setup details.</li> <li>A Docker network shared with dsx\u2011connect (example: <code>dsx-connect-network</code>)</li> </ul>"},{"location":"deployment/docker/google-cloud-storage/#compose-file","title":"Compose File","text":"<p>Use the following file <code>docker-compose-google-cloud-storage-connector.yaml</code>.</p>"},{"location":"deployment/docker/google-cloud-storage/#core-connector-env-common-across-connectors","title":"Core connector env (common across connectors)","text":"Variable Description <code>DSXCONNECTOR_DSX_CONNECT_URL</code> dsx-connect base URL (use <code>http://dsx-connect-api:8586</code> on the shared Docker network). <code>DSXCONNECTOR_CONNECTOR_URL</code> Callback URL dsx-connect uses to reach the connector (defaults to the service name inside the Docker network). <code>DSXCONNECTOR_ASSET</code> Root bucket and/or <code>bucket/prefix</code> to scan and monitor. <code>DSXCONNECTOR_FILTER</code> Optional rsync\u2011style include/exclude rules relative to the asset. <code>DSXCONNECTOR_ITEM_ACTION</code> What to do on malicious verdicts (<code>nothing</code>, <code>delete</code>, <code>move</code>, <code>move_tag</code>). Use <code>move</code>/<code>move_tag</code> to relocate objects after verdict. <code>DSXCONNECTOR_ITEM_ACTION_MOVE_METAINFO</code> Destination bucket/prefix for moved objects when using <code>move</code>/<code>move_tag</code>. <code>DSXCONNECTOR_MONITOR</code> Enable change monitoring (<code>true</code>/<code>false</code>). Requires Pub/Sub notifications.  (note, if using Webhook style notification, this flag can remain false)"},{"location":"deployment/docker/google-cloud-storage/#google-cloud-specific-settings","title":"Google Cloud-specific settings","text":"Variable Description <code>GOOGLE_APPLICATION_CREDENTIALS</code> Path to the mounted service account JSON (e.g., <code>/app/creds/service-account.json</code>). <code>GOOGLE_CLOUD_PROJECT</code> Optional\u2014used when the service account JSON omits <code>project_id</code>. <code>GCS_PUBSUB_PROJECT_ID</code> (if <code>DSXCONNECTOR_MONITOR: true</code>) GCP project that owns the Pub/Sub subscription receiving bucket events. <code>GCS_PUBSUB_SUBSCRIPTION</code> (if <code>DSXCONNECTOR_MONITOR: true</code>) Subscription name or full resource path (e.g., <code>gcs-object-events</code> or <code>projects/&lt;proj&gt;/subscriptions/&lt;sub&gt;</code>). <code>GCS_PUBSUB_ENDPOINT</code> Override Pub/Sub endpoint (useful for local emulators). Leave blank for production. <p>The Project ID should match the Service Account JSON file value (which was applied as a secret above).  Subscription name is the name of the Pub/Sub subscription that receives bucket events, which was create as part of the service account setup.</p>"},{"location":"deployment/docker/google-cloud-storage/#minimal-deployment","title":"Minimal Deployment","text":""},{"location":"deployment/docker/google-cloud-storage/#mount-your-serviceaccount-json","title":"Mount your service\u2011account JSON.","text":"<p>Create a google service account and download the JSON: Google Cloud Credentials.  Place the JSON next to the compose file (or use an absolute path), or anywhere that the deployed docker container can access on the host system. The default mount path is <code>/app/creds/gcp-sa.json</code> to a file named <code>gcp-sa.json</code> in the same directory. The easiet deployment with no changes needed is to name the JSON file <code>gcp-sa.json</code> and place it in the same directory as the compose file.: <pre><code>      # Mount a Google service account JSON and point GOOGLE_APPLICATION_CREDENTIALS to it\n      - type: bind\n        source: ./gcp-sa.json\n        target: /app/creds/gcp-sa.json\n        read_only: true\n        bind:\n          selinux: z\n</code></pre> If you wish to use a different filename, change the <code>source</code> path to the desired filename.</p>"},{"location":"deployment/docker/google-cloud-storage/#deploy-the-connector","title":"Deploy the connector","text":"<p>Support full-scan bucket, no monitoring, no remediation:</p> <ul> <li>DSXCONNECTOR_ASSET: \"bucket name\"</li> <li>DSXCONNECTOR_ITEM_ACTION: \"nothing\" # &lt;-- the default</li> </ul> <p>Deploy: <pre><code>docker compose -f docker-compose-google-cloud-storage-connector.yaml up -d\n</code></pre></p>"},{"location":"deployment/docker/google-cloud-storage/#test-scan","title":"Test Scan","text":"<p>If you just have the connector running (no dsx-connect), you still navigate to the connector's API page: http://localhost:8630/docs</p> <p>This confirms that the connector is running and listening on port 8630.  From the OpenAPI docs you can invoke a full scan by POSTing to the connector's full_scan endpoint,  and you should see the following response:</p> <p><pre><code>{\n  \"status\": \"success\",\n  \"message\": \"Full scan initiated\",\n  \"description\": \"The scan is running in the background. job_id=06930255-a48b-443a-840f-414a2659855e\",\n  \"id\": null,\n  \"preview\": null\n}\n</code></pre> If you have dsx-connect running, navigate to the DSX-Connect UI, note the Google Cloud Storage 'card' and click the <code>Full Scan</code> button, to invoke a scan.</p>"},{"location":"deployment/docker/google-cloud-storage/#assets-and-filters","title":"Assets and Filters","text":"<ul> <li><code>DSXCONNECTOR_ASSET</code> should be set to your bucket (e.g., <code>my-bucket</code>) or <code>bucket/prefix</code> to scope listings.</li> <li>If a prefix is provided, listings start at that sub\u2011root and filters are evaluated relative to it.</li> <li>See Reference \u2192 Assets &amp; Filters for sharding/partition guidance.</li> </ul>"},{"location":"deployment/docker/google-cloud-storage/#monitoring","title":"Monitoring","text":"<ul> <li>Use <code>DSXCONNECTOR_ASSET</code> to set the bucket (and optional prefix) to monitor.</li> <li>If Pub/Sub monitoring is enabled, the connector listens to Pub/Sub in a background thread. Create a bucket notification that publishes to the subscription; for example:   <pre><code>gsutil notification create -t gcs-object-events -f json gs://my-bucket\n</code></pre></li> </ul> <p>The service account must have <code>roles/storage.objectViewer</code> on the bucket and <code>roles/pubsub.subscriber</code> on the subscription.   The connector listens for <code>OBJECT_FINALIZE</code> (new/updated object) and <code>OBJECT_METADATA_UPDATE</code> events; these are fixed defaults.   See Reference \u2192 Google Cloud Credentials for the full setup commands.</p>"},{"location":"deployment/docker/google-cloud-storage/#tls-options","title":"TLS Options","text":"<ul> <li><code>DSXCONNECTOR_USE_TLS</code>: Serve the connector over HTTPS (mount cert/key as needed).</li> <li><code>DSXCONNECTOR_TLS_CERTFILE</code> / <code>DSXCONNECTOR_TLS_KEYFILE</code>: Paths to the mounted certificate and key when TLS is enabled.</li> <li><code>DSXCONNECTOR_VERIFY_TLS</code>: Keep <code>true</code> (default) to verify dsx-connect\u2019s certificate; set to <code>false</code> only for local dev.</li> <li><code>DSXCONNECTOR_CA_BUNDLE</code>: Optional CA bundle path when verifying dsx-connect with a private CA.</li> </ul>"},{"location":"deployment/docker/google-cloud-storage/#webhooks-when-and-how-to-expose","title":"Webhooks: When and How to Expose","text":"<p>You\u2019d reach for the /webhook/event path instead of native Pub/Sub in a few scenarios:</p> <ul> <li>Pub/Sub isn\u2019t an option (restricted project, org policy, private cloud, or you\u2019re already forwarding events through something else like Cloud Storage \u2192   Eventarc \u2192 Cloud Run).</li> <li>You already have middleware that enriches or filters events and can simply POST to the connector\u2014switching to Pub/Sub would add new moving pieces.</li> <li>You want to keep control of retries/backoff or fan out to multiple systems before notifying dsx-connect.</li> <li>The connector runs where Pub/Sub access is awkward (air\u2011gapped network segment, proxies, workload identity gaps), but you can still reach dsx-connect   over HTTP/S.</li> <li>You plan to feed events from several sources beyond Cloud Storage (e.g., a centralized event hub), so hitting the webhook maintains a single integration   pattern.</li> <li>You need custom authentication/validation in front of the connector; a small gateway/service can enforce that and call the webhook.</li> </ul> <p>Pub/Sub remains the simplest path when it\u2019s available, but the webhook keeps things flexible if you\u2019ve already standardized on HTTP callbacks or have compliance/runtime constraints around Pub/Sub.</p> <p>For external callbacks into the connector, expose or tunnel the host port mapped to <code>8630</code> (compose default). Upstream systems should hit that public address. Internally, set <code>DSXCONNECTOR_CONNECTOR_URL</code> to the Docker-service URL (e.g., <code>http://google-cloud-storage-connector:8630</code>) so dsx-connect can reach the container.</p>"},{"location":"deployment/docker/intro/","title":"Docker Compose Best Practices","text":"<p>Think of the Compose YAML as a template and <code>.env</code> files as the fill. Keep the YAML stable; swap <code>.env</code> files per environment (dev/stage/prod) to pin image tags and inject secrets without editing YAML.</p>"},{"location":"deployment/docker/intro/#core-ideas","title":"Core ideas","text":"<ul> <li>Use <code>.env</code> files to pin images and supply secrets. Avoid editing <code>docker-compose-*.yaml</code>.</li> <li>Maintain one env file per environment: <code>.dev.env</code>, <code>.stage.env</code>, <code>.prod.env</code>.</li> <li>Pin all images in the env file (core, DSXA, connectors) so you know exactly what you\u2019re running.</li> <li>Reuse the same env files when you move to Kubernetes (convert to a Secret).</li> </ul>"},{"location":"deployment/docker/intro/#sample-env","title":"Sample <code>.env</code>","text":"<p><pre><code># Core + DSXA images (pin releases)\nDSXCONNECT_IMAGE=dsxconnect/dsx-connect:1.2.3\nDSXA_IMAGE=dsxconnect/dpa-rocky9:4.1.1.2020\n\n# Connector image example\nONEDRIVE_IMAGE=dsxconnect/onedrive-connector:0.1.7\n\n# Core auth (optional)\n#DSXCONNECT_ENROLLMENT_TOKEN=abc123\n\n# DSXA settings (if you run DSXA locally)\n#APPLIANCE_URL=https://&lt;di&gt;.customers.deepinstinctweb.com\n#TOKEN=&lt;DSXA token&gt;\n#SCANNER_ID=&lt;scanner id&gt;\n\n# OneDrive connector settings (example)\n#ONEDRIVE_TENANT_ID=...\n#ONEDRIVE_CLIENT_ID=...\n#ONEDRIVE_CLIENT_SECRET=...\n#ONEDRIVE_USER_ID=...\n</code></pre> Compose reads <code>.env</code> automatically; use <code>--env-file</code> to point to a different one (e.g., <code>.stage.env</code>).</p>"},{"location":"deployment/docker/intro/#how-to-run-with-env-files","title":"How to run with env files","text":"<ol> <li>Copy the sample: <code>cp docker-compose.env.sample .dev.env</code> and edit values.</li> <li>Run core (example): <pre><code>docker network create dsx-connect-network || true\ndocker compose --env-file .dev.env -f dsx_connect/deploy/docker/docker-compose-dsx-connect-all-services.yaml up -d\n</code></pre></li> <li>Run DSXA if needed: <pre><code>docker compose --env-file .dev.env -f dsx_connect/deploy/docker/docker-compose-dsxa.yaml up -d\n</code></pre></li> <li>Run a connector (example OneDrive): <pre><code>docker compose --env-file .dev.env -f connectors/onedrive/deploy/docker/docker-compose-onedrive-connector.yaml up -d\n</code></pre></li> </ol> <p>Swap <code>.dev.env</code> with <code>.stage.env</code> or <code>.prod.env</code> as needed; the YAML stays the same.</p>"},{"location":"deployment/docker/intro/#reuse-for-kubernetes","title":"Reuse for Kubernetes","text":"<p>Create a Secret from the same env file and reference it in Helm values: <pre><code>kubectl create secret generic dsxconnect-env --from-env-file=.prod.env -n your-namespace\n# In values.yaml (core + connectors):\n# envSecretRefs:\n#   - dsxconnect-env\n</code></pre> This keeps configuration consistent across Compose and K8s.</p>"},{"location":"deployment/docker/intro/#tips","title":"Tips","text":"<ul> <li>Keep secrets out of YAML; store them in env files or your secret manager.</li> <li>Pin tags in env files; avoid <code>:latest</code> for anything shared.</li> <li>Use separate env files per environment; commit only samples (<code>*.env.sample</code>), not real secrets.</li> <li>For TLS, mount your CA bundle and set <code>DSXCONNECTOR_VERIFY_TLS=true</code> and <code>DSXCONNECTOR_CA_BUNDLE=...</code>.</li> </ul>"},{"location":"deployment/docker/m365-mail/","title":"M365 Email Connector \u2014 Docker Compose","text":"<p>This guide explains how to run the <code>m365-mail-connector</code> (Outlook / Exchange Online) with Docker Compose for demos and local development.</p>"},{"location":"deployment/docker/m365-mail/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed and a network shared with the dsx-connect stack (for example, <code>dsx-connect-network</code>).</li> <li>dsx-connect API running (Docker Compose or K8S); note the base URL exposed to the connector.</li> <li>Microsoft Graph application (client credentials flow) with the required permissions (see Reference \u2192 Azure Credentials for detailed steps):</li> <li><code>Mail.Read</code> (or <code>Mail.ReadWrite</code> if remediation actions are enabled)</li> <li><code>Files.Read.All</code> if you plan to download <code>referenceAttachment</code>s (optional for v1)</li> <li>Service principal credentials: supply via <code>M365_TENANT_ID</code>, <code>M365_CLIENT_ID</code>, <code>M365_CLIENT_SECRET</code>.</li> <li>List of mailbox UPNs or mailbox folders to monitor via <code>M365_MAILBOX_UPNS</code> (comma-separated).</li> <li>Public HTTPS endpoint for Graph change notifications (e.g., port-forward via ngrok/Cloudflare Tunnel). Only <code>/{connector}/webhook/event</code> must be reachable from Microsoft Graph.</li> </ul>"},{"location":"deployment/docker/m365-mail/#compose-file","title":"Compose File","text":"<p>Start from <code>connectors/m365_mail/deploy/docker/docker-compose-m365-mail-connector.yaml</code>. It references the published connector image and binds port <code>8650</code> (container + host) so you can forward webhooks easily.</p>"},{"location":"deployment/docker/m365-mail/#core-connector-env-common-across-connectors","title":"Core connector env (common across connectors)","text":"Variable Description <code>DSXCONNECTOR_DSX_CONNECT_URL</code> Base URL for dsx-connect (e.g., <code>http://dsx-connect-api:8586</code> on the shared Docker network). <code>DSXCONNECTOR_CONNECTOR_URL</code> Internal callback URL dsx-connect uses to reach this connector (defaults to the service name inside the Docker network, e.g., <code>http://m365-mail-connector:8650</code>). <code>DSXCONNECTOR_ASSET</code> Alias for <code>M365_MAILBOX_UPNS</code>; comma-separated mailbox or mailbox/folder entries (e.g., <code>user@contoso.com/Inbox</code>). <code>DSXCONNECTOR_FILTER</code> Optional rsync-style filters for attachment names under the asset (see Reference \u2192 Filters). <code>DSXCONNECTOR_ITEM_ACTION</code> What dsx-connect should do on malicious verdicts (<code>nothing</code>, <code>delete</code>, <code>move</code>, <code>move_tag</code>). Set to <code>move</code>/<code>move_tag</code> when you want the connector to remediate mail. <code>DSXCONNECTOR_ITEM_ACTION_MOVE_METAINFO</code> Optional string that accompanies move/tag actions (defaults to <code>dsxconnect-quarantine</code>; keep unless you have connector-specific logic)."},{"location":"deployment/docker/m365-mail/#m365-specific-settings","title":"M365-specific settings","text":"Variable Description <code>M365_MAILBOX_UPNS</code> Comma-separated list of mailbox UPNs (e.g., <code>user@contoso.com,groupscan@contoso.com</code>). <code>M365_TENANT_ID</code>, <code>M365_CLIENT_ID</code>, <code>M365_CLIENT_SECRET</code> Microsoft Graph app registration credentials. <code>M365_CLIENT_STATE</code> Optional shared secret for webhook <code>clientState</code> validation. <code>DSXCONNECTOR_WEBHOOK_URL</code> Optional public HTTPS base URL for Graph webhooks (falls back to <code>DSXCONNECTOR_CONNECTOR_URL</code>). Use this with ngrok or another tunnel so dsx-connect can stay on the internal URL while Graph reaches the connector. <code>DSXCONNECTOR_DELTA_RUN_INTERVAL_SECONDS</code> Background delta backfill cadence (defaults to 600). <code>DSXCONNECTOR_TRIGGER_DELTA_ON_NOTIFICATION</code> When <code>true</code>, run a delta pass immediately after each webhook (default <code>false</code>). <p>Example:</p> <pre><code>docker compose \\\n  -f connectors/m365_mail/deploy/docker/docker-compose-m365-mail-connector.yaml \\\n  up -d\n</code></pre>"},{"location":"deployment/docker/m365-mail/#assets-filters-and-sharding","title":"Assets, Filters, and Sharding","text":"<ul> <li><code>DSXCONNECTOR_ASSET</code> maps directly to the mailbox scope:</li> <li>Entire mailbox: <code>user@contoso.com</code></li> <li>Specific folder: <code>user@contoso.com/Finance</code></li> <li>Use multiple connector instances when sharding large estates (per mailbox or per folder). Each instance should receive a distinct asset and mailbox list.</li> <li>Apply <code>DSXCONNECTOR_FILTER</code> when you want to include/exclude attachment names (e.g., <code>**/*.zip</code>, <code>-tmp/</code>). Filters are evaluated relative to the asset\u2019s mailbox/folder path.</li> </ul> <p>See Reference \u2192 Assets &amp; Filters for sharding patterns.</p>"},{"location":"deployment/docker/m365-mail/#webhook-exposure","title":"Webhook Exposure","text":"<p>Microsoft Graph must reach <code>https://&lt;public-host&gt;/m365-mail-connector/webhook/event</code>.</p> <ol> <li>Expose the container\u2019s port <code>8650</code> via an HTTPS tunnel or reverse proxy (ngrok, Cloudflare Tunnel, etc.). The tunnel terminates on the Docker host and forwards traffic to <code>localhost:8650</code>.</li> <li>Register Microsoft Graph subscriptions using that public HTTPS URL.</li> <li>Leave <code>DSXCONNECTOR_CONNECTOR_URL</code> pointing at the Docker-network hostname (e.g., <code>http://m365-mail-connector:8650</code>) so dsx-connect can reach the connector internally.</li> </ol>"},{"location":"deployment/docker/m365-mail/#compose-vs-kubernetes","title":"Compose vs. Kubernetes","text":"<ul> <li>Docker Compose</li> <li><code>DSXCONNECT_AUTH__ENABLED</code> remains <code>false</code>; dsx-connect does not require enrollment tokens or DSX-HMAC inbound signatures.</li> <li>The connector stores Graph credentials only in memory; no Kubernetes Secret orchestration is needed.</li> <li>Delta tokens are persisted through dsx-connect\u2019s KV API, but the calls are unsigned in local dev.</li> <li>Kubernetes</li> <li>Enable dsx-connect enrollment + HMAC so every connector POST/GET is signed.</li> <li>Mount enrollment tokens through Secrets (<code>DSXCONNECT_ENROLLMENT_TOKEN</code>) and set <code>auth_dsxconnect.enabled=true</code> (plus <code>auth_dsxconnect.enrollmentSecretName</code>) in both charts.</li> <li>Ingress/NetworkPolicy defaults expose only <code>/webhook_event</code> publicly and restrict other paths to dsx-connect.</li> </ul> <p>Use Compose for local validation and switch to the Helm charts (<code>connectors/m365_mail/deploy/helm/</code>) for production-grade deployments with enrollment and DSX-HMAC enforced.</p>"},{"location":"deployment/docker/m365-mail/#azure-credentials-reference","title":"Azure Credentials Reference","text":"<p>See Reference \u2192 Azure Credentials for a portal walkthrough, CLI automation, and Graph API fallback commands for registering the application, capturing tenant/client IDs, generating client secrets, and granting Microsoft Graph application permissions.</p>"},{"location":"deployment/docker/m365-mail/#exposing-the-webhook-locally-ngrok-example","title":"Exposing the Webhook Locally (ngrok example)","text":"<p>Microsoft Graph delivers notifications only to publicly reachable HTTPS endpoints. For local testing:</p> <ol> <li>Install ngrok and run <code>ngrok http 8650</code>. ngrok prints both HTTP and HTTPS URLs (e.g., <code>https://&lt;random&gt;.ngrok-free.app</code>).</li> <li>Keep <code>DSXCONNECTOR_CONNECTOR_URL=http://127.0.0.1:8650</code> so dsx-connect calls the connector over localhost.</li> <li>Set <code>DSXCONNECTOR_WEBHOOK_URL=https://&lt;random&gt;.ngrok-free.app</code> so the connector registers the ngrok address with Graph.</li> <li>Restart the connector. Subscription reconciliation will now succeed, and Graph notifications will arrive at the tunneled endpoint.</li> </ol> <p>Any secure tunnel (Cloudflare Tunnel, Azure Relay, etc.) works similarly: expose port 8650, note the HTTPS URL, and place it in <code>DSXCONNECTOR_WEBHOOK_URL</code>.</p>"},{"location":"deployment/docker/m365-mail/#faster-scanning-after-notifications","title":"Faster Scanning After Notifications","text":"<ul> <li>The connector relies on Graph delta queries for durability. By default it waits <code>DSXCONNECTOR_DELTA_RUN_INTERVAL_SECONDS</code> (600s) between runs. During that interval you may see multiple webhook events, but attachments are processed when the next delta pass runs.</li> <li>To reduce latency, either lower the interval (e.g., <code>DSXCONNECTOR_DELTA_RUN_INTERVAL_SECONDS=30</code>) or set <code>DSXCONNECTOR_TRIGGER_DELTA_ON_NOTIFICATION=true</code> so the connector runs a delta pass immediately after each webhook.</li> <li>Even with the trigger enabled, the periodic delta loop stays active to recover from missed notifications.</li> </ul>"},{"location":"deployment/docker/m365-mail/#operational-notes","title":"Operational Notes","text":"<ul> <li>The background delta runner and <code>@connector.full_scan</code> reuse the same Graph delta code path. Trigger a manual pass with <code>POST /dsx-connect/api/v1/connectors/full_scan/{uuid}</code> (optional <code>?limit=N</code>).</li> <li>Delta tokens live under the <code>m365/delta:&lt;upn&gt;</code> namespace in dsx-connect\u2019s KV store. For Compose, the connector automatically initializes them when the API is reachable.</li> <li>Webhooks deliver near real-time attachment notifications; delta backfill handles drift and initial load. Keep the docker container running so subscription renewals (30-minute reconciliation loop) continue.</li> <li>Remediation actions kick in as soon as <code>DSXCONNECTOR_ITEM_ACTION</code> is set to <code>delete</code>, <code>move</code>, or <code>move_tag</code>; no extra toggle is required (the legacy <code>DSXCONNECTOR_ENABLE_ACTIONS</code> variable is ignored unless explicitly set to <code>false</code> for compatibility).</li> </ul>"},{"location":"deployment/docker/m365-mail/#tls-options","title":"TLS Options","text":"<ul> <li><code>DSXCONNECTOR_USE_TLS</code>: Serve the connector over HTTPS (mount cert/key and enable as needed).</li> <li><code>DSXCONNECTOR_TLS_CERTFILE</code> / <code>DSXCONNECTOR_TLS_KEYFILE</code>: Paths to the mounted certificate and private key when TLS is enabled.</li> <li><code>DSXCONNECTOR_VERIFY_TLS</code>: Keep <code>true</code> (default) to verify dsx-connect\u2019s certificate; set to <code>false</code> only for local dev.</li> <li><code>DSXCONNECTOR_CA_BUNDLE</code>: Optional CA bundle path when verifying dsx-connect with a private CA.</li> </ul>"},{"location":"deployment/docker/sharepoint/","title":"SharePoint Connector \u2014 Docker Compose","text":"<p>This guide shows how to deploy the SharePoint connector with Docker Compose for quick testing/POV.</p>"},{"location":"deployment/docker/sharepoint/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker installed locally (or a container VM)</li> <li>SharePoint app registration credentials (tenant ID, client ID, client secret). See Reference \u2192 Azure Credentials for a step-by-step walkthrough.</li> <li>A Docker network shared with dsx\u2011connect (example: <code>dsx-connect-network</code>)</li> </ul>"},{"location":"deployment/docker/sharepoint/#compose-file","title":"Compose File","text":"<p>Use <code>connectors/sharepoint/deploy/docker/docker-compose-sharepoint-connector.yaml</code> as a starting point.</p>"},{"location":"deployment/docker/sharepoint/#core-connector-env-common-across-connectors","title":"Core connector env (common across connectors)","text":"Variable Description <code>DSXCONNECTOR_DSX_CONNECT_URL</code> dsx-connect base URL (use <code>http://dsx-connect-api:8586</code> on the shared Docker network). <code>DSXCONNECTOR_CONNECTOR_URL</code> Callback URL dsx-connect uses to reach the connector (defaults to the service name inside the Docker network). <code>DSXCONNECTOR_ASSET</code> SharePoint scope, e.g., full site URL or doc library/folder path. <code>DSXCONNECTOR_FILTER</code> Optional rsync\u2011style include/exclude rules relative to the asset. <code>DSXCONNECTOR_ITEM_ACTION</code> What to do on malicious verdicts (<code>nothing</code>, <code>delete</code>, <code>move</code>, <code>move_tag</code>). Set to <code>move</code>/<code>move_tag</code> to relocate files after verdict. <code>DSXCONNECTOR_ITEM_ACTION_MOVE_METAINFO</code> Destination (site/doc lib/folder path or label) for moved items when using <code>move</code>/<code>move_tag</code>."},{"location":"deployment/docker/sharepoint/#sharepoint-specific-settings","title":"SharePoint-specific settings","text":"<p>Define these values in your Compose environment (the sample file expects plain <code>SP_*</code> variables in the shell/<code>.env</code> file; the compose template expands them to the connector-ready <code>DSXCONNECTOR_SP_*</code> envs).</p> Variable Description <code>SP_TENANT_ID</code> Azure AD tenant ID for the SharePoint app registration. <code>SP_CLIENT_ID</code> Client ID for the SharePoint app registration. <code>SP_CLIENT_SECRET</code> Client secret for the SharePoint app registration (store securely). <code>SP_VERIFY_TLS</code> Optional override (<code>true</code>/<code>false</code>) for Graph TLS verification (defaults to <code>true</code>). <code>SP_CA_BUNDLE</code> Optional CA bundle path for Graph TLS verification. <code>SP_WEBHOOK_ENABLED</code> Set to <code>true</code> to enable Microsoft Graph change notifications (optional). <code>SP_WEBHOOK_URL</code> Public HTTPS URL Graph calls for change notifications (required when webhooks enabled). <code>SP_WEBHOOK_CLIENT_STATE</code> Optional shared secret Graph includes in webhook payloads. <code>SP_WEBHOOK_CHANGE_TYPES</code> Optional override of Graph change types (default <code>updated</code>). <p>Example <code>.env</code> fragment for Compose:</p> <pre><code># SharePoint credentials\nSP_TENANT_ID=d1509054-f881-493e-9d8e-e69932e4e865\nSP_CLIENT_ID=2d546ee8-0592-4aa7-9d3e-1f03e398634c\nSP_CLIENT_SECRET=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\n# Optional overrides\nSP_VERIFY_TLS=true\nSP_CA_BUNDLE=\n\n# Change notifications (optional)\nSP_WEBHOOK_ENABLED=false\nSP_WEBHOOK_URL=\nSP_WEBHOOK_CLIENT_STATE=\n</code></pre> <p>Launch with: <pre><code>docker compose --env-file .env \\\n  -f connectors/sharepoint/deploy/docker/docker-compose-sharepoint-connector.yaml up -d\n</code></pre></p>"},{"location":"deployment/docker/sharepoint/#assets-and-filters","title":"Assets and Filters","text":"<ul> <li><code>DSXCONNECTOR_ASSET</code> should be set to your SharePoint scope (site/doc lib/folder). Navigate to the exact folder in SharePoint Online, grab the full URL (e.g., <code>https://contoso.sharepoint.com/sites/Site/Shared%20Documents/dsx-connect/scantest</code>), and paste it here.</li> <li>Filters are evaluated relative to that scope (children).</li> <li>See Reference \u2192 Assets &amp; Filters for sharding/partition guidance.</li> </ul>"},{"location":"deployment/docker/sharepoint/#notes","title":"Notes","text":"<ul> <li>Use <code>DSXCONNECTOR_ASSET</code> to configure the SharePoint URL scope (site/doc lib/folder).</li> </ul>"},{"location":"deployment/docker/sharepoint/#tls-options","title":"TLS Options","text":"<ul> <li><code>DSXCONNECTOR_USE_TLS</code>: Serve the connector over HTTPS (mount cert/key and enable as needed).</li> <li><code>DSXCONNECTOR_TLS_CERTFILE</code> / <code>DSXCONNECTOR_TLS_KEYFILE</code>: Paths to the mounted certificate and key when TLS is enabled.</li> <li><code>DSXCONNECTOR_VERIFY_TLS</code>: Keep <code>true</code> (default) to verify dsx-connect\u2019s certificate; set to <code>false</code> only for local dev.</li> <li><code>DSXCONNECTOR_CA_BUNDLE</code>: Optional CA bundle path when verifying dsx-connect with a private CA.</li> </ul>"},{"location":"deployment/docker/sharepoint/#webhook-exposure","title":"Webhook Exposure","text":"<p>If you expose SharePoint webhook callbacks or other HTTP endpoints outside Docker, tunnel or publish the host port mapped to <code>8640</code> (compose default when ports are enabled). Keep <code>DSXCONNECTOR_CONNECTOR_URL</code> pointing to the Docker-network URL (e.g., <code>http://sharepoint-connector:8640</code>) so dsx-connect can reach the container internally.</p>"},{"location":"deployment/kubernetes/aws-s3/","title":"AWS S3 Connector \u2014 Helm Deployment","text":"<p>Deploy the <code>aws-s3-connector-chart</code> (under <code>connectors/aws_s3/deploy/helm</code>) using the steps below, whether you work directly from the repo or from the OCI registry.</p>"},{"location":"deployment/kubernetes/aws-s3/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.19+ cluster with <code>kubectl</code> access.</li> <li>Helm 3.2+.</li> <li>Access to <code>oci://registry-1.docker.io/dsxconnect/aws-s3-connector-chart</code>.</li> </ul>"},{"location":"deployment/kubernetes/aws-s3/#preflight-tasks","title":"Preflight Tasks","text":"<p>Create the AWS credentials Secret before installing:</p> <pre><code>kubectl create secret generic aws-credentials \\\n  --from-literal=AWS_ACCESS_KEY_ID=&lt;key&gt; \\\n  --from-literal=AWS_SECRET_ACCESS_KEY=&lt;secret&gt;\n</code></pre> <p>(<code>connectors/aws_s3/deploy/helm/aws-secret.yaml</code> contains a template if you prefer to edit/apply a manifest.)</p>"},{"location":"deployment/kubernetes/aws-s3/#configuration","title":"Configuration","text":""},{"location":"deployment/kubernetes/aws-s3/#required-settings","title":"Required settings","text":"<ul> <li><code>env.DSXCONNECTOR_ASSET</code>: target bucket or <code>bucket/prefix</code>.</li> <li><code>env.DSXCONNECTOR_FILTER</code>: optional rsync-style include/exclude set (see Filter reference).</li> <li><code>env.DSXCONNECTOR_DISPLAY_NAME</code>: friendly label in the dsx-connect UI.</li> <li><code>env.DSXCONNECTOR_ITEM_ACTION</code> plus <code>env.DSXCONNECTOR_ITEM_ACTION_MOVE_METAINFO</code>: remediation options.</li> <li><code>workers</code>: Uvicorn workers per pod (default 1).</li> <li><code>replicaCount</code>: number of pods (default 1).</li> </ul> <p>Filters follow rsync semantics (<code>?</code>, <code>*</code>, <code>**</code>, <code>+</code>, <code>-</code>).</p>"},{"location":"deployment/kubernetes/aws-s3/#dsx-connect-endpoint","title":"dsx-connect endpoint","text":"<p>Defaults to <code>http://dsx-connect-api</code> (or <code>https://dsx-connect-api</code> when TLS enabled). Override via <code>env.DSXCONNECTOR_DSX_CONNECT_URL</code> if dsx-connect is exposed elsewhere.</p>"},{"location":"deployment/kubernetes/aws-s3/#authentication-tls","title":"Authentication &amp; TLS","text":""},{"location":"deployment/kubernetes/aws-s3/#deployment","title":"Deployment","text":""},{"location":"deployment/kubernetes/aws-s3/#method-1-oci-chart-with-cli-overrides-fastest","title":"Method 1 \u2013 OCI chart with CLI overrides (fastest)","text":"<pre><code>helm install aws-invoices-dev oci://registry-1.docker.io/dsxconnect/aws-s3-connector-chart \\\n  --version &lt;chart-version&gt; \\\n  --set env.DSXCONNECTOR_ASSET=my-bucket \\\n  --set-string env.DSXCONNECTOR_FILTER=\"\" \\\n  --set-string image.tag=&lt;connector-version&gt;\n</code></pre>"},{"location":"deployment/kubernetes/aws-s3/#method-2-work-from-a-pulled-chart-edit-values-locally","title":"Method 2 \u2013 Work from a pulled chart (edit values locally)","text":"<pre><code>helm pull oci://registry-1.docker.io/dsxconnect/aws-s3-connector-chart --version &lt;chart-version&gt;\ntar -xzf aws-s3-connector-chart-&lt;chart-version&gt;.tgz\ncd aws-s3-connector-chart\n</code></pre> <p>Example values file:</p> <pre><code>env:\n  DSXCONNECTOR_ASSET: \"invoices-bucket/prefix\"\n  DSXCONNECTOR_FILTER: \"**/*.pdf\"\nimage:\n  tag: \"&lt;connector-version&gt;\"\n</code></pre> <p>Install from the extracted chart root (<code>.</code>):</p> <pre><code>helm install aws-invoices-dev . -f values-dev.yaml\n</code></pre>"},{"location":"deployment/kubernetes/aws-s3/#method-3-gitops-production-style","title":"Method 3 \u2013 GitOps / production style","text":"<pre><code>helm upgrade --install aws-prod oci://registry-1.docker.io/dsxconnect/aws-s3-connector-chart \\\n  --version &lt;chart-version&gt; \\\n  -f values-prod.yaml\n</code></pre>"},{"location":"deployment/kubernetes/aws-s3/#verification","title":"Verification","text":"<pre><code>helm list\nkubectl get pods\nkubectl logs deploy/aws-s3-connector -f\n</code></pre>"},{"location":"deployment/kubernetes/aws-s3/#scaling-tuning","title":"Scaling &amp; tuning","text":"<ul> <li>Raise <code>workers</code> for more concurrent <code>read_file</code> responses within a pod.</li> <li>Increase <code>replicaCount</code> for HA or to fan out item actions; each pod registers separately.</li> <li>Keep AWS throttling in mind when increasing concurrency; adjust filters to limit scope.</li> </ul> <p>See <code>connectors/aws_s3/deploy/helm/values.yaml</code> for the exhaustive parameter reference.</p>"},{"location":"deployment/kubernetes/azure-blob/","title":"Azure Blob Storage Connector \u2014 Helm Deployment","text":"<p>Use this guide to deploy <code>azure-blob-storage-connector-chart</code> (found under <code>connectors/azure_blob_storage/deploy/helm</code>). The same instructions apply whether you pull the chart from the repository or from the OCI registry.</p>"},{"location":"deployment/kubernetes/azure-blob/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.19+ cluster and <code>kubectl</code> context.</li> <li>Helm 3.2+.</li> <li>Access to the connector chart in OCI: <code>oci://registry-1.docker.io/dsxconnect/azure-blob-connector-chart</code>.</li> <li>Optional: <code>openssl</code> for validating TLS assets.</li> </ul>"},{"location":"deployment/kubernetes/azure-blob/#preflight-tasks","title":"Preflight Tasks","text":"<ol> <li>Create the Azure Storage connection-string Secret before installing the chart:</li> <li>Edit and apply <code>connectors/azure_blob_storage/deploy/helm/azure-secret.yaml</code>, or</li> <li>Create it inline:      <pre><code>kubectl create secret generic azure-storage-connection-string \\\n  --from-literal=AZURE_STORAGE_CONNECTION_STRING='&lt;conn-string&gt;'\n</code></pre></li> <li>Confirm the namespace has network access to dsx-connect (same cluster or reachable service).</li> </ol>"},{"location":"deployment/kubernetes/azure-blob/#configuration","title":"Configuration","text":""},{"location":"deployment/kubernetes/azure-blob/#required-settings","title":"Required settings","text":"<ul> <li><code>env.DSXCONNECTOR_ASSET</code>: target container (optionally <code>container/prefix</code>).</li> <li><code>env.DSXCONNECTOR_FILTER</code>: optional rsync-style include/exclude patterns (see Filter reference).</li> <li><code>env.DSXCONNECTOR_DISPLAY_NAME</code>: friendly label for the dsx-connect UI card.</li> <li><code>env.DSXCONNECTOR_ITEM_ACTION</code> and <code>env.DSXCONNECTOR_ITEM_ACTION_MOVE_METAINFO</code>: control remediation behavior (<code>nothing</code>, <code>delete</code>, <code>tag</code>, <code>move</code>, <code>move_tag</code>).</li> <li><code>env.DSXCONNECTOR_SCAN_CONCURRENCY</code>: number of parallel Azure list operations during full scans (default 10).</li> <li><code>env.DSXCONNECTOR_LIST_PAGE_SIZE</code>: <code>list_blobs</code> page size (default 1000).</li> <li><code>workers</code>: Uvicorn workers per pod (default 1); increase for more concurrent <code>read_file</code> traffic.</li> <li><code>replicaCount</code>: pod count (default 1).</li> </ul> <p>Filters follow rsync semantics (<code>?</code>, <code>*</code>, <code>**</code>, <code>+</code>, <code>-</code>). See the chart values file for complex examples.</p>"},{"location":"deployment/kubernetes/azure-blob/#dsx-connect-endpoint","title":"dsx-connect endpoint","text":"<p>The chart defaults to <code>http://dsx-connect-api</code> (or <code>https://dsx-connect-api</code> when TLS is enabled). Override with <code>env.DSXCONNECTOR_DSX_CONNECT_URL</code> if your dsx-connect instance is reachable via another hostname or port.</p>"},{"location":"deployment/kubernetes/azure-blob/#authentication-tls","title":"Authentication &amp; TLS","text":""},{"location":"deployment/kubernetes/azure-blob/#deployment","title":"Deployment","text":""},{"location":"deployment/kubernetes/azure-blob/#method-1-oci-chart-with-cli-overrides-fastest","title":"Method 1 \u2013 OCI chart with CLI overrides (fastest)","text":"<pre><code>helm install abs-dev oci://registry-1.docker.io/dsxconnect/azure-blob-connector-chart \\\n  --version &lt;chart-version&gt; \\\n  --set env.DSXCONNECTOR_ASSET=my-container \\\n  --set-string env.DSXCONNECTOR_FILTER=\"**/*.docx\" \\\n  --set-string image.tag=&lt;connector-version&gt;\n</code></pre> <p>If you omit <code>image.tag</code>, Helm uses the chart\u2019s <code>appVersion</code>. Pinning it is recommended for reproducibility.</p>"},{"location":"deployment/kubernetes/azure-blob/#method-2-work-from-a-pulled-chart-edit-values-locally","title":"Method 2 \u2013 Work from a pulled chart (edit values locally)","text":"<pre><code>helm pull oci://registry-1.docker.io/dsxconnect/azure-blob-connector-chart --version &lt;chart-version&gt;\ntar -xzf azure-blob-storage-connector-chart-&lt;chart-version&gt;.tgz\ncd azure-blob-storage-connector-chart\n</code></pre> <p>Create <code>values-dev.yaml</code> (example):</p> <pre><code>env:\n  DSXCONNECTOR_ASSET: \"my-container\"\n  DSXCONNECTOR_FILTER: \"prefix/**\"\nimage:\n  tag: \"&lt;connector-version&gt;\"\n</code></pre> <p>Install from the extracted chart root (<code>.</code>):</p> <pre><code>helm install abs-dev . -f values-dev.yaml\n</code></pre>"},{"location":"deployment/kubernetes/azure-blob/#method-3-gitops-production-style","title":"Method 3 \u2013 GitOps / production style","text":"<p>Store environment-specific values files in Git and let your CD system upgrade from OCI:</p> <pre><code>helm upgrade --install abs-prod oci://registry-1.docker.io/dsxconnect/azure-blob-connector-chart \\\n  --version &lt;chart-version&gt; \\\n  -f values-prod.yaml\n</code></pre>"},{"location":"deployment/kubernetes/azure-blob/#verification","title":"Verification","text":"<pre><code>helm list\nkubectl get pods\nkubectl logs deploy/azure-blob-storage-connector -f\n</code></pre>"},{"location":"deployment/kubernetes/azure-blob/#scaling-tuning","title":"Scaling &amp; tuning","text":"<ul> <li>Increase <code>workers</code> and/or <code>replicaCount</code> for more concurrent <code>read_file</code> responses or HA.</li> <li>Adjust <code>DSXCONNECTOR_SCAN_CONCURRENCY</code> / <code>DSXCONNECTOR_LIST_PAGE_SIZE</code> if Azure throttles (reduce) or if you need faster enumeration (increase carefully).</li> <li>Each pod registers independently with dsx-connect; replicas do not parallelize a single full scan but do improve availability.</li> </ul> <p>See <code>connectors/azure_blob_storage/deploy/helm/values.yaml</code> for the complete parameter catalog.</p>"},{"location":"deployment/kubernetes/dsx-connect/","title":"Deploying DSX\u2011Connect (Helm)","text":"<p>This Helm chart deploys the DSX\u2011Connect stack (API + workers + Redis +  rsyslog), with an optional in\u2011cluster DSXA scanner for local testing.</p> <p>This guide explains the core configuration concepts and details three deployment methods, from a quick local test to a production-grade GitOps workflow.</p>"},{"location":"deployment/kubernetes/dsx-connect/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.19+ (a local cluster like Colima or Minikube is recommended for development).</li> <li>Helm 3.2+</li> <li><code>kubectl</code> configured to point to your cluster.</li> <li><code>helm</code> for deploying helm charts</li> <li><code>openssl</code> for generating a self-signed certificate if you plan to enable TLS for development.</li> <li>A dsx-connect chart release.  </li> </ul>"},{"location":"deployment/kubernetes/dsx-connect/#core-configuration-concepts","title":"Core Configuration Concepts","text":"<p>NOTE: all of the following assumes that the user has a dsx-connect chart release.</p> <p>The helm chart provided with DSX-Connect releases can be used to deploy the entire DSX\u2011Connect stack. The default <code>value.yaml</code> and <code>values-dev.yaml</code> serve as good guides for the most common deployment cases.</p>"},{"location":"deployment/kubernetes/dsx-connect/#global-configuration","title":"Global Configuration","text":"<p>The following is the global configuration section of the default <code>values.yaml</code> file.  This is a fullstack deployment that DOES NOT deploy the DSXA scanner: <pre><code>global:\n  # Minimal, common env most users need to touch. Everything else is set in templates with sane defaults.\n  env:\n    # REQUIRED when (dsxa-scanner.enabled=false): set to your external DSXA endpoint\n    DSXCONNECT_SCANNER__SCAN_BINARY_URL: \"http://external-dsxa:5000/scan/binary/v2\"\n    # Results/stats DB URL (redis://... =&gt; Redis, anything else =&gt; in-memory)\n    DSXCONNECT_RESULTS_DB: redis://redis:6379/3\n\n  # Optional shared image defaults (components fall back to these when set)\n  image:\n    repository: dsxconnect/dsx-connect\n    tag: \"\"\n    pullPolicy: IfNotPresent\n    # Note: When installing from an OCI chart with --version=X.Y.Z, the default image tag will be that chart's appVersion.\n</code></pre></p> <p>The most common settings to be configured are in the <code>global</code> section.  Deployments will need to supply:</p> <ul> <li>global.image.tag: the version of dsx-connect to deploy</li> <li>global.env.DSXCONNECT_SCANNER__SCAN_BINARY_URL: if using an external (not deployed via this helm) DSXA scanner</li> </ul> <p>The <code>values-dev.yaml</code> file is a good starting point for local testing.  The key difference is that it sets <code>dsxa-scanner.enabled=true</code> and <code>global.env.DSXCONNECT_SCANNER__SCAN_BINARY_URL=\"\"</code> in which case a single DSXA scanner is deployed into the same cluster.</p> <pre><code>global:\n  # Minimal, common env most users need to touch. Everything else is set in templates with sane defaults.\n  env:\n    DSXCONNECT_SCANNER__SCAN_BINARY_URL: \"\"\n    ...\n# typical deployments would have dsxa-scanners in their own cluster and namespace, but for if all that's needed\n# is a single dsxa-scanner pod that only supports scan/binary/v2 (file size &lt;= 2GB), then enable here.\ndsxa-scanner:\n  enabled: true\n</code></pre>"},{"location":"deployment/kubernetes/dsx-connect/#deployment-methods","title":"Deployment Methods","text":"<p>This chart is flexible. The following methods show how to deploy it, from a simple test to a production-grade workflow.</p>"},{"location":"deployment/kubernetes/dsx-connect/#assumptions","title":"Assumptions","text":"<p>In the following guide assumed that the release name used is \"dsx\" and the namespace is the <code>default</code> namespace.  for example: <pre><code>helm upgrade --install dsx &lt;helm root directory&gt; -f &lt;helm root directory&gt;/values.yaml &lt;command line arguments&gt;\n</code></pre> If you use a different release name, change secrets and values files accordingly. If you deploy to a different namespace, add <code>-n &lt;namespace&gt;</code> to your <code>kubectl</code> and <code>helm upgrade --install</code> commands, and create Secrets in that namespace.</p>"},{"location":"deployment/kubernetes/dsx-connect/#prerequisites-deploying-secrets","title":"Prerequisites - Deploying Secrets","text":"<p>Before starting the dsx-connect deployment, create the supporting Secrets if you plan to enable TLS, connector authentication, or DIANNA.</p> <p>1. Create the TLS Certificate Secret (if enabling TLS): Edit <code>examples/secrets/tls-secret.yaml</code> (sample provided with the chart) or create your own. The chart expects a Secret named <code>&lt;release&gt;-dsx-connect-api-tls</code> (e.g., <code>dsx-dsx-connect-api-tls</code> when the release is <code>dsx</code>). <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;release&gt;-dsx-connect-api-tls\n  namespace: &lt;your-namespace&gt;\ntype: kubernetes.io/tls\ndata:\n  tls.crt: &lt;base64-encoded-cert&gt;\n  tls.key: &lt;base64-encoded-key&gt;\n</code></pre> Apply the Secret before deploying the dsx-connect stack: <pre><code>kubectl apply -f examples/secrets/tls-secret.yaml\n</code></pre></p> <p>2. Create the Enrollment Token Secret (if enabling Authentication): Edit <code>examples/secrets/auth-enrollment-secret.yaml</code> (sample provided). The secret name should be <code>&lt;release&gt;-dsx-connect-api-auth-enrollment</code> unless you override it in values (example release <code>dsx</code> \u2192 <code>dsx-dsx-connect-api-auth-enrollment</code>). The enrollment can be any alphanumeric string, ideally a long random string, such as a UUID.  <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;release&gt;-dsx-connect-api-auth-enrollment\n  namespace: &lt;your-namespace&gt;\ntype: Opaque\nstringData:\n  # REQUIRED: the enrollment token\n  token: &lt;enrollment-token&gt;\n</code></pre></p> <pre><code>kubectl apply -f examples/secrets/auth-enrollment-secret.yaml\n</code></pre> <p>3. Create the DIANNA API Secret: Edit <code>examples/secrets/di-api-secret.yaml</code> (sample provided) with your DI API token and management URL, then apply it. The sample Secret is named <code>di-api</code>; set <code>global.dianna.secretName</code> (and optionally <code>dsx-connect-dianna-worker.dianna.secretName</code>) if you use a different name. <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: di-api\n  namespace: &lt;your-namespace&gt;\ntype: Opaque\nstringData:\n  apiToken: \"&lt;di-token&gt;\"\n  managementUrl: \"https://di.example.com\"\n</code></pre> <pre><code>kubectl apply -f examples/secrets/di-api-secret.yaml\n</code></pre></p> <p>Configure the chart to read this Secret at install time, for example:</p> <pre><code>helm upgrade --install dsx . \\\n  --set-string global.dianna.secretName=di-api\n</code></pre>"},{"location":"deployment/kubernetes/dsx-connect/#method-1-quick-start-command-line-overrides","title":"Method 1: Quick Start (Command-Line Overrides)","text":"<p>This method is best for quick, temporary deployments, like for local testing. It uses the <code>--set</code> flag to provide configuration directly on the command line.</p> <p>4. Deploy the Stack: *   Simplest deployment: deploys DSXA scanner and dsx-connect on the same cluster:     Development mode deployment with a local DSXA scanner.  Use the <code>values-dev.yaml</code> (or copy it) to set deploy dsx-connect with a dsxa-scanner.     You can change the values in <code>values-dev.yaml</code> to match your environment,     but, overriding values in the command line allows for flexible deployment.  In this case the only     setting that needs to be set is global.image.tag.     <pre><code>helm upgrade --install dsx . -n &lt;namespace&gt; -f values-dev.yaml --set-string global.image.tag=0.2.82\n</code></pre></p> <ul> <li> <p>Using an external DSX/A Scanner, HTTP deployment:     In this case, using the values.yaml (the default), DSXA scanner is not deployed, so the scan binary URL must be set.     You can either edit the values.yaml, or copy it and edit, or simply pass in settings on the helm arguments:     <pre><code>helm upgrade --install dsx -n &lt;namespace&gt; -f values.yaml --set-string global.image.tag=0.2.82\n  --set-string global.env.DSXCONNECT_SCANNER__SCAN_BINARY_URL=http://my-dsxa-url:5000/scan/binary/v2\n</code></pre></p> </li> <li> <p>For a TLS-enabled deployment:     ```bash     helm upgrade --install dsx . -n  \\       --set-string       --set dsx-connect-api.tls.enabled=true \\       --set dsx-connect-api.tls.secretName=my-dsx-connect-api-tls \\       --set-string global.env.DSXCONNECT_SCANNER__SCAN_BINARY_URL=https://my-dsxa.example.com/scan/binary/v2 <li> <p>For an Authentication-enabled (enrollment) deployment: <pre><code>helm upgrade --install dsx . -n &lt;namespace&gt; \\\n  --set-string global.image.tag=0.2.82 \\\n  --set dsx-connect-api.auth.enabled=true\n</code></pre></p> </li> <li> <p>For a DIANNA-enabled deployment: <pre><code>helm upgrade --install dsx . -n &lt;namespace&gt; \\\n  --set-string global.image.tag=0.2.82 \\\n  --set dsx-connect-dianna-worker.enabled=true \\\n  --set-string global.dianna.secretName=di-api\n</code></pre></p> </li> <li> <p>Combined TLS + Authentication + DIANNA (CLI): <pre><code># Pre-create the required secrets\nkubectl apply -f examples/secrets/auth-enrollment-secret.yaml\nkubectl apply -f examples/secrets/di-api-secret.yaml\nkubectl create secret tls my-dsx-connect-api-tls --cert=tls.crt --key=tls.key\n\n# Install with TLS + Auth + DIANNA enabled\nhelm upgrade --install dsx . \\\n  --set-string global.image.tag=0.2.82 \\\n  --set-string global.env.DSXCONNECT_SCANNER__SCAN_BINARY_URL=https://external-dsxa.example.com/scan/binary/v2 \\\n  --set dsx-connect-api.tls.enabled=true \\\n  --set dsx-connect-api.tls.secretName=my-dsx-connect-api-tls \\\n  --set dsx-connect-api.auth.enabled=true \\\n  --set dsx-connect-dianna-worker.enabled=true \\\n  --set-string global.dianna.secretName=di-api\n</code></pre>     ```</p> </li>"},{"location":"deployment/kubernetes/dsx-connect/#production-install-recommended-flags","title":"Production Install (recommended flags)","text":"<p>Use the production defaults in <code>values.yaml</code> and set the external DSXA URL explicitly. Also pin the image tag for reproducibility.</p> <pre><code>helm upgrade --install dsx dsx_connect/deploy/helm \\\n  -f dsx_connect/deploy/helm/values.yaml \\\n  --set-string global.env.DSXCONNECT_SCANNER__SCAN_BINARY_URL=https://my-dsxa.example.com/scan/binary/v2 \\\n  --set-string global.image.tag=0.2.66\n</code></pre>"},{"location":"deployment/kubernetes/dsx-connect/#method-2-standard-deployment-custom-values-file","title":"Method 2: Standard Deployment (Custom Values File)","text":"<p>This is the most common and recommended method for managing deployments. It involves creating a dedicated values file for each instance of the connector.</p> <p>1. Create the Required Secrets: - TLS (if enabling HTTPS for the API):   <pre><code>kubectl create secret tls my-dsx-connect-api-tls --cert=tls.crt --key=tls.key\n</code></pre> - Enrollment token (if enabling Authentication):   <pre><code>kubectl apply -f examples/secrets/auth-enrollment-secret.yaml\n</code></pre> - DIANNA API token (if enabling DI workers):   <pre><code>kubectl apply -f examples/secrets/di-api-secret.yaml\n</code></pre></p> <p>2. Create a Custom Values File: Create a new file, for example <code>my-dsx-connect-values.yaml</code>, to hold your configuration.</p> <p>```yaml    # my-dsx-connect-values.yaml</p> <p>global:      env:        # REQUIRED when dsxa-scanner.enabled=false (default): point to your external DSXA        DSXCONNECT_SCANNER__SCAN_BINARY_URL: \"http://external-dsxa:5000/scan/binary/v2\"      dianna:        secretName: \"di-api\"                              # Secret created from examples/secrets/di-api-secret.yaml        managementUrlKey: managementUrl                   # Omit if you kept the default key name        apiTokenKey: apiToken                             # Omit if you kept the default key name        verifyTls: true        caBundle: \"\"                                      # optional path if you mount a custom CA        chunkSize: 4194304                                # bytes        timeout: 60                                       # seconds        autoOnMalicious: false      scanner:        # serviceName: \"dsx-connect-dsxa-scanner\"  # defaults to \"-dsxa-scanner\"        # port: 5000        # scheme: http <p>dsx-connect-api:      tls:        enabled: true        secretName: \"my-dsx-connect-api-tls\"      auth:        enabled: true        enrollment:          key: ENROLLMENT_TOKEN       # value: \"\"  # leave empty when providing Secret via examples/secrets/auth-enrollment-secret.yaml      env:        LOG_LEVEL: info</p> <p>dsx-connect-scan-request-worker:      enabled: true      env:        LOG_LEVEL: info      celery:        concurrency: 2</p> <p>dsx-connect-dianna-worker:      enabled: true      env:        LOG_LEVEL: info      celery:        # override only if you changed naming; default queue prefix is \"dev\"        # queue: \"custom-prefix.dsx_connect.analyze.dianna\"        concurrency: 2</p> <p># Example: do not hardcode secrets here in production. Prefer pulling from a   # Kubernetes Secret at install time as shown above.</p> <p># ... configure other workers, redis, syslog as needed    ```</p> <p>3. Install the Chart: Install the chart, referencing your custom values file with the <code>-f</code> flag.    <pre><code>helm install dsx-connect . -f my-dsx-connect-values.yaml\n</code></pre></p>"},{"location":"deployment/kubernetes/dsx-connect/#example-combined-tls-authentication-dianna-values-production","title":"Example: Combined TLS + Authentication + DIANNA values (production)","text":"<pre><code># values-prod.yaml\nglobal:\n  env:\n    DSXCONNECT_SCANNER__SCAN_BINARY_URL: \"https://external-dsxa.example.com/scan/binary/v2\"\n  dianna:\n    secretName: \"di-api\"\n    managementUrlKey: managementUrl\n    apiTokenKey: apiToken\n    verifyTls: true\n    chunkSize: 4194304\n    timeout: 60\n    autoOnMalicious: false\n\ndsx-connect-api:\n  tls:\n    enabled: true\n    secretName: \"my-dsx-connect-api-tls\"\n  auth:\n    enabled: true\n    enrollment:\n      key: ENROLLMENT_TOKEN\n      # value: \"\"  # leave empty; provide Secret via examples/secrets/auth-enrollment-secret.yaml\n  env:\n    LOG_LEVEL: info\n\ndsx-connect-scan-request-worker:\n  enabled: true\n  env:\n    LOG_LEVEL: info\n  celery:\n    concurrency: 2\n\ndsx-connect-dianna-worker:\n  enabled: true\n  env:\n    LOG_LEVEL: info\n  celery:\n    concurrency: 2\n</code></pre> <p>Install:</p> <pre><code>kubectl apply -f examples/secrets/auth-enrollment-secret.yaml\nkubectl apply -f examples/secrets/di-api-secret.yaml\nkubectl create secret tls my-dsx-connect-api-tls --cert=tls.crt --key=tls.key\n\nhelm upgrade --install dsx . -f values-prod.yaml \\\n  --set-string global.image.tag=&lt;version&gt;\n</code></pre>"},{"location":"deployment/kubernetes/dsx-connect/#method-3-production-grade-deployment-gitops-cicd","title":"Method 3: Production-Grade Deployment (GitOps &amp; CI/CD)","text":"<p>This is the definitive, scalable, and secure approach for managing production applications. It uses modern Continuous Delivery (CD) mechanisms.</p> <p>The Philosophy: Instead of running <code>helm</code> commands manually, you declare the desired state of your application in a Git repository. A GitOps tool (like Argo CD or Flux) runs in your cluster, monitors the repository, and automatically syncs the cluster state to match what is defined in Git.</p> <p>The Workflow: This involves storing environment-specific values files (e.g., <code>values-prod.yaml</code>) in a separate GitOps repository. The GitOps tool then uses these files to automate Helm deployments, providing a fully auditable and declarative system for managing your application lifecycle.</p>"},{"location":"deployment/kubernetes/dsx-connect/#scan-result-logging","title":"Scan Result Logging","text":"<p>The <code>dsx-connect-results-worker</code> component is responsible for logging scan results to syslog.  By default, it sends syslog over TCP to port 514.</p> <p>This helm chart includes a rsyslog service that can be used to collect scan results within the cluster.  The rsyslog service is enabled by default, but can be disabled by setting <code>rsyslog.enabled=false</code> in the <code>values.yaml</code> file.</p> <p>If you need to change the default configuration or chose to send scan results to other collectors, see the file: See the APPENDIX-LOG-COLLECTORS.md file for more information.</p>"},{"location":"deployment/kubernetes/dsx-connect/#packaging-publishing-helm","title":"Packaging &amp; Publishing (Helm)","text":"<p>You have a few good options to distribute this umbrella chart so others can install it with a single Helm command.</p> <ul> <li>Option A \u2014 OCI registry (recommended if you already push Docker images)<ul> <li>Package: <code>inv helm-package</code> (outputs <code>dist/charts/dsx-connect-&lt;ver&gt;.tgz</code>)</li> <li>Login: <code>helm registry login registry-1.docker.io -u &lt;user&gt;</code></li> <li>Push: <code>inv helm-push-oci --repo=oci://registry-1.docker.io/dsxconnect</code></li> <li>Install: <code>helm install dsx-connect oci://registry-1.docker.io/dsxconnect/dsx-connect-chart --version &lt;ver&gt; -f values.yaml</code></li> <li>Pros: lives alongside container images, easy auth, immutable versions.</li> </ul> </li> </ul>"},{"location":"deployment/kubernetes/dsx-connect/#oci-install-prewired-image-tag","title":"OCI Install (prewired image tag)","text":"<p>When installing from OCI with <code>--version X.Y.Z</code>, the chart at that version is pulled and its <code>appVersion</code> is used as the default image tag. In other words, versions are prewired \u2014 the Helm chart version selects the matching image tag by default. You can still override via <code>--set-string global.image.tag=...</code> if needed.</p> <ul> <li> <p>Option B \u2014 Static Helm repo (e.g., GitHub Pages)</p> <ul> <li>Package: <code>inv helm-package</code></li> <li>Index: <code>inv helm-repo-index --base-url=https://&lt;org&gt;.github.io/&lt;repo&gt;/charts</code></li> <li>Publish: upload contents of <code>dist/charts/</code> (including <code>index.yaml</code>) to your site (e.g., <code>gh-pages/charts/</code>)</li> <li>Use: <code>helm repo add dsx https://&lt;org&gt;.github.io/&lt;repo&gt;/charts &amp;&amp; helm install dsx-connect dsx/dsx-connect --version &lt;ver&gt;</code></li> <li>Pros: public/simple distribution, no auth required.</li> </ul> </li> <li> <p>Option C \u2014 Zip/attach chart in a release bundle</p> <ul> <li>Package: <code>inv helm-package</code>; attach tgz to a GitHub release or bundle artifact.</li> <li>Use: <code>helm install dsx-connect ./dsx-connect-&lt;ver&gt;.tgz</code></li> <li>Pros: lightweight for ad\u2011hoc distribution, but no repo metadata.</li> </ul> </li> </ul> <p>The Invoke tasks (<code>inv helm-package</code>, <code>inv helm-push-oci</code>, <code>inv helm-repo-index</code>) are provided to standardize the packaging flow. Choose the publishing method that best fits your environment and CI/CD.</p>"},{"location":"deployment/kubernetes/dsx-connect/#advanced-configuration-overriding-default-environment-variables","title":"Advanced Configuration: Overriding Default Environment Variables","text":"<p>Many environment variables have sensible default values set directly within the component templates. These defaults align with the <code>docker-compose-dsx-connect-all-services.yaml</code> configuration. You only need to override them if your deployment requires a different value.</p> <p>To override a default environment variable, specify it under the <code>env</code> section of the respective component in your custom <code>values.yaml</code> file.</p> <p>Commonly Overridden Variables (and their defaults):</p> <ul> <li>API keys removed (use JWT/HMAC flows instead)</li> <li><code>DSXCONNECT_SCANNER__SCAN_BINARY_URL</code>: REQUIRED when <code>dsxa-scanner.enabled=false</code> (the default). If you enable <code>dsxa-scanner</code>, templates compute <code>http(s)://&lt;release&gt;-dsxa-scanner:&lt;port&gt;/scan/binary/v2</code> using <code>global.scanner</code>.</li> <li><code>DSXCONNECT_WORKERS__BROKER</code>: <code>redis://redis:6379/5</code></li> <li><code>DSXCONNECT_WORKERS__BACKEND</code>: <code>redis://redis:6379/6</code></li> <li><code>DSXCONNECT_REDIS_URL</code>: <code>redis://redis:6379/3</code></li> <li><code>DSXCONNECT_RESULTS_DB</code>: <code>redis://redis:6379/3</code></li> <li><code>DSXCONNECT_RESULTS_DB__RETAIN</code>: <code>100</code></li> <li>Results/Stats DB: controlled by <code>DSXCONNECT_RESULTS_DB</code> (redis://\u2026 for Redis, else in-memory)</li> <li><code>PYTHONUNBUFFERED</code>: <code>1</code></li> <li><code>LOG_LEVEL</code>: <code>debug</code> (for API), <code>warning</code> (for workers)</li> </ul> <p>Specific Worker Overrides:</p> <ul> <li><code>dsx-connect-results-worker</code>:<ul> <li><code>DSXCONNECT_SCAN_RESULT_TASK_WORKER__SYSLOG_SERVER_URL</code>: <code>rsyslog</code></li> <li><code>DSXCONNECT_SCAN_RESULT_TASK_WORKER__SYSLOG_SERVER_PORT</code>: <code>514</code></li> </ul> </li> </ul>"},{"location":"deployment/kubernetes/dsx-connect/#client-trust-and-ca-bundles","title":"Client Trust and CA Bundles","text":"<p>When clients (like the Azure Blob Storage Connector) communicate with the <code>dsx-connect-api</code> server over HTTPS, they must be able to verify the server's identity. If the <code>dsx-connect-api</code> server is using a certificate from an internal or self-signed Certificate Authority (CA), you must provide that CA's certificate to each client in a CA Bundle.</p> <p>Encryption vs. Authentication: It is important to understand that even with <code>verify=false</code>, the connection is still encrypted. However, without verification, the identity of the server is not authenticated. This leaves you vulnerable to man-in-the-middle attacks. Using a CA bundle to verify the connection is critical for security.</p> <p>Procedure for Clients (e.g., Azure Blob Storage Connector):</p> <ol> <li> <p>Obtain the CA Certificate: Get the public certificate file (e.g., <code>ca.crt</code>) of the CA that signed your <code>dsx-connect-api</code> server's certificate.</p> </li> <li> <p>Create a Secret from the CA Certificate: <pre><code>kubectl create secret generic dsx-connect-ca --from-file=ca.crt=/path/to/your/ca.crt\n</code></pre></p> </li> <li> <p>Configure the Client's Helm Chart:     Refer to the client's (e.g., <code>connectors/azure_blob_storage/deploy/helm/DEVELOPER_README.md</code>) documentation for how to configure its <code>DSXCONNECTOR_CA_BUNDLE</code> and <code>DSXCONNECTOR_VERIFY_TLS</code> settings to trust this CA.</p> </li> </ol>"},{"location":"deployment/kubernetes/dsx-connect/#verifying-the-deployment","title":"Verifying the Deployment","text":"<p>After deploying with any method, you can check the status of your release.</p> <ol> <li> <p>Check the Helm Release: <pre><code>helm list\n</code></pre></p> </li> <li> <p>Check the Pods: <pre><code>kubectl get pods\n</code></pre></p> </li> </ol>"},{"location":"deployment/kubernetes/dsx-connect/#full-configuration-parameters","title":"Full Configuration Parameters","text":"<p>For a full list of configurable parameters for all components, see the <code>values.yaml</code> file.</p> <p>Commonly tuned values (by component):</p> <ul> <li>Global<ul> <li><code>global.image.tag</code>: pin a specific image tag for all components.</li> <li><code>global.env.DSXCONNECT_SCANNER__SCAN_BINARY_URL</code>: REQUIRED when <code>dsxa-scanner.enabled=false</code> (external DSXA).</li> <li><code>dsx-connect-dianna-worker.dianna.*</code>: DIANNA settings for the DI worker; map directly to <code>DSXCONNECT_DIANNA__*</code> env.</li> </ul> </li> <li>API<ul> <li><code>dsx-connect-api.tls.enabled</code>: enable HTTPS for the API. Certs are loaded from Secret <code>&lt;release&gt;-dsx-connect-api-tls</code> and mounted at <code>/app/certs</code>; HTTPS listens on 443.</li> <li><code>dsx-connect-api.auth.enabled</code> + <code>dsx-connect-api.auth.enrollment.{key,value}</code>: enable HMAC auth and set enrollment token.</li> </ul> </li> <li>Workers<ul> <li><code>dsx-connect-*-worker.celery.concurrency</code>: per\u2011worker parallelism inside a pod.</li> <li><code>dsx-connect-*-worker.replicaCount</code>: number of pods (horizontal scale/HA).</li> </ul> </li> <li>DIANNA<ul> <li><code>dsx-connect-dianna-worker</code>: set <code>enabled</code>, <code>celery.concurrency</code>, and the <code>dianna.*</code> values for this worker.</li> </ul> </li> </ul>"},{"location":"deployment/kubernetes/dsx-connect/#minimal-configuration","title":"Minimal configuration","text":"<p>For most deployments you only need to set a few environment values once, at the top-level <code>values.yaml</code> under <code>global.env</code>:</p> <pre><code>global:\n  env:\n    # Optional: only set if pointing to an external DSXA; otherwise dsx-connect auto-targets\n    # the in-cluster DSXA service when `dsxa-scanner.enabled=true`.\n    # DSXCONNECT_SCANNER__SCAN_BINARY_URL: \"http://&lt;external-dsxa-host&gt;:5000/scan/binary/v2\"\n</code></pre> <p>All other settings have sensible defaults baked into the component templates (Redis URLs, DB paths, etc.). You can still override per\u2011service <code>env</code> keys if needed, but typically the defaults are fine.</p>"},{"location":"deployment/kubernetes/dsx-connect/#scanner-discovery-external-vs-incluster","title":"Scanner Discovery (External vs In\u2011Cluster)","text":"<ul> <li>Default (external DSXA): <code>dsxa-scanner.enabled=false</code> (values default). You must set <code>global.env.DSXCONNECT_SCANNER__SCAN_BINARY_URL</code> to your DSXA endpoint.</li> <li>In\u2011cluster (local testing): enable <code>dsxa-scanner.enabled=true</code>. API and workers default to <code>http(s)://&lt;release&gt;-dsxa-scanner:&lt;port&gt;/scan/binary/v2</code>, guided by <code>global.scanner</code> hints (service name/port/scheme). You can still override the env if needed.</li> </ul> <p>Override examples:</p> <ul> <li>All components:</li> <li><code>--set-string global.env.DSXCONNECT_SCANNER__SCAN_BINARY_URL=http://external-dsxa:5000/scan/binary/v2</code></li> <li>Only API:</li> <li><code>--set-string dsx-connect-api.env.DSXCONNECT_SCANNER__SCAN_BINARY_URL=http://external-dsxa:5000/scan/binary/v2</code></li> </ul>"},{"location":"deployment/kubernetes/dsx-connect/#optional-example-dsxa-scanner","title":"Optional: Example DSXA Scanner","text":"<p>For quick local testing, this chart can deploy a single DSXA scanner pod (not production). Enable it and DSX\u2011connect will default its scanner URL to that in\u2011cluster service name.</p> <ul> <li> <p>Enable in <code>values.yaml</code>:   <pre><code>dsxa-scanner:\n  enabled: true\n</code></pre></p> </li> <li> <p>Or via CLI:   <pre><code>helm upgrade --install dsx . -f values.yaml --set dsxa-scanner.enabled=true\n</code></pre></p> </li> </ul> <p>When enabled, API and workers default to: <code>DSXCONNECT_SCANNER__SCAN_BINARY_URL = http://&lt;release&gt;-dsxa-scanner:5000/scan/binary/v2</code> You can still override this env if you\u2019re pointing at an external DSXA.</p> <p>Remember: the inline DSXA scanner needs real credentials. Override <code>dsxa-scanner.env.APPLIANCE_URL</code>, <code>dsxa-scanner.env.TOKEN</code>, and <code>dsxa-scanner.env.SCANNER_ID</code> (ideally via Secrets) before enabling it anywhere beyond local testing.</p>"},{"location":"deployment/kubernetes/dsx-connect/#image-version-overrides","title":"Image Version Overrides","text":"<p>How image tags are chosen:</p> <ul> <li>From local chart path (this repo): templates default to the chart <code>appVersion</code> unless you override <code>global.image.tag</code> (or per-component <code>image.tag</code>).</li> <li>From OCI registry (helm install oci://\u2026 --version X.Y.Z): Helm pulls the chart at that version; templates use that chart\u2019s <code>appVersion</code> as the default image tag. You can still override with <code>--set-string global.image.tag=...</code> if needed.</li> </ul> <p>Override examples (no need to edit <code>values.yaml</code>):</p> <ul> <li> <p>All components (via global image tag):</p> <ul> <li><code>helm upgrade --install dsx . -f values.yaml --set-string global.image.tag=0.2.66</code></li> <li>Later: <code>helm upgrade dsx . --reuse-values --set-string global.image.tag=0.2.67</code></li> </ul> </li> <li> <p>Single component override:</p> <ul> <li>API only: <code>helm upgrade dsx . --reuse-values --set-string dsx-connect-api.image.tag=0.2.66</code></li> <li>Results worker: <code>helm upgrade dsx . --reuse-values --set-string dsx-connect-results-worker.image.tag=0.2.66</code></li> </ul> </li> <li> <p>Quick hot-fix without Helm (not recommended long-term):</p> <ul> <li><code>kubectl set image deploy/dsx-connect-api dsx-connect-api=dsxconnect/dsx-connect:0.2.66</code></li> <li><code>kubectl rollout status deploy/dsx-connect-api</code></li> </ul> </li> </ul> <p>Tip: In CI, drive the tag with a variable, e.g.</p> <pre><code>TAG=\"$(git describe --tags --always)\"\nhelm upgrade dsx . --reuse-values --set-string global.image.tag=\"$TAG\"\n</code></pre>"},{"location":"deployment/kubernetes/dsx-connect/#authentication-support","title":"Authentication Support","text":""},{"location":"deployment/kubernetes/dsx-connect/#enrollment-token-via-secret-optional-but-recommended","title":"Enrollment Token via Secret (optional, but recommended)","text":"<p>Use an enrollment token to bootstrap connector registration. After registration, HMAC authenticates both directions.</p> <p>Option A (recommended): apply the provided Secret manifest and reference it implicitly</p> <p>1) Copy and edit <code>dsx-connect-&lt;version&gt;/helm/examples/secrets/auth-enrollment-secret.yaml</code> (set your namespace and token), then apply:</p> <pre><code>kubectl apply -f examples/secrets/auth-enrollment-secret.yaml\n</code></pre> <p>2) Enable auth in values without embedding the token:</p> <pre><code>dsx-connect-api:\n  auth:\n    enabled: true\n    enrollment:\n      key: ENROLLMENT_TOKEN\n      # value: \"\"   # leave empty to use the external Secret created above\n</code></pre> <p>Option B (demo only): embed the token so the chart creates the Secret for you</p> <pre><code>dsx-connect-api:\n  auth:\n    enabled: true\n    enrollment:\n      key: ENROLLMENT_TOKEN\n      value: \"&lt;strong-random&gt;\"\n</code></pre> <p>Connector charts: - Set <code>auth.enabled=true</code> to verify HMAC on private routes. - Provide the same token to connectors via <code>auth_dsxconnect.enrollmentSecretName</code>/<code>enrollmentKey</code> so they set <code>DSXCONNECT_ENROLLMENT_TOKEN</code>. - Expose only <code>/webhook_event</code> via Ingress; use NetworkPolicies to allow traffic from dsx\u2011connect and your ingress controller.</p> <p>Notes: - Swagger remains available for docs; \u201cTry it out\u201d will not work for HMAC\u2011protected connector endpoints. - Frontend (user) auth is separate (recommend an Ingress with OIDC/oauth2\u2011proxy in production).</p>"},{"location":"deployment/kubernetes/dsx-connect/#dianna-api-token-via-secret-required-if-dianna-workers-enabled","title":"DIANNA API Token via Secret (required if DIANNA workers enabled)","text":"<p>For production, source the DIANNA API token from a Kubernetes Secret rather than embedding it in values or passing it on the CLI.</p> <p>1) Create the DI secret (see the sample <code>examples/secrets/di-api-secret.yaml</code> earlier in this guide) so it contains both <code>apiToken</code> and <code>managementUrl</code>.</p> <p>2) Reference that secret from your values file (recommended so it applies to the API and every worker):</p> <pre><code># values-dianna.yaml\nglobal:\n  dianna:\n    secretName: \"di-api\"\n    # managementUrlKey/apiTokenKey default to managementUrl/apiToken; override only if you rename keys.\n    verifyTls: true\n    chunkSize: 4194304\n    timeout: 60\n    autoOnMalicious: false\n\ndsx-connect-dianna-worker:\n  enabled: true\n  celery:\n    concurrency: 2\n</code></pre> <p>Install:</p> <pre><code>helm upgrade --install dsx dsx_connect/deploy/helm \\\n  -f dsx_connect/deploy/helm/values.yaml \\\n  -f values-dianna.yaml \\\n  --set-string global.env.DSXCONNECT_SCANNER__SCAN_BINARY_URL=https://my-dsxa.example.com/scan/binary/v2 \\\n  --set-string global.image.tag=&lt;version&gt;\n</code></pre> <p>3) Queue name and scaling:</p> <pre><code>- Queue defaults to `dev.dsx_connect.analyze.dianna` (prefix derived automatically).\n- Override with `dsx-connect-dianna-worker.celery.queue` if you run multiple isolated environments against the same Redis broker.\n</code></pre> <ul> <li>Scale parallelism via <code>dsx-connect-dianna-worker.celery.concurrency</code>.</li> </ul>"},{"location":"deployment/kubernetes/dsx-connect/#concurrency-and-replicas","title":"Concurrency and Replicas","text":"<p>Workers scale with two knobs. Use them together for best results:</p> <ul> <li>Replica count (<code>replicaCount</code>): number of pods. Each pod has its own CPU/memory limits/requests and its own Celery process. Good for horizontal scaling and resilience.</li> <li>Concurrency (<code>celery.concurrency</code>): number of task workers inside one pod. Increases parallelism within a pod; shares that pod\u2019s resources.</li> </ul> <p>Guidance:</p> <ul> <li>The Scan request workers are generally the place to start with concurrency.  These workers take enqueued scan request tasks, reads a file from a connector, and sends the file to DSXA for scanning.   Needless to say, a single pod / single celery worker can only handle a single scan request at a time.</li> <li>Start by raising <code>celery.concurrency</code> modestly (2\u20134), then add <code>replicaCount</code> to spread load across nodes.</li> <li>If CPU-bound within a pod, increase pod resources or add replicas. If I/O-bound (network/Redis/HTTP), modest concurrency increases often help.</li> <li>Example: 3 pods \u00d7 concurrency 3 \u2248 9 workers on the queue.</li> <li>Scale downstream workers (verdict/result/notification) when increasing request throughput to avoid bottlenecks.</li> </ul>"},{"location":"deployment/kubernetes/dsx-connect/#practical-tuning-tips","title":"Practical Tuning Tips","text":"<ul> <li>Continue favoring modest Celery concurrency (2\u20134) before adding pods; add replicas when you see CPU saturation or want resiliency.</li> <li>For connectors, bump <code>workers</code> to 2\u20134 if read_file handlers are CPU-bound or you want more in-pod parallel reads; add connector replicas if a single pod\u2019s CPU or network is saturated, or for HA.</li> <li>If you notice uneven distribution across connector replicas due to HTTP keep-alive, higher Celery concurrency tends to open more connections and spread load better; you can also tune httpx connection limits if needed later.</li> </ul>"},{"location":"deployment/kubernetes/dsx-connect/#note-on-connector-replicas","title":"Note on Connector Replicas","text":"<p>Connectors also have a replicaCount, but it's important to understand what it's doing:</p> <ul> <li>Setting a connector chart\u2019s <code>replicaCount &gt; 1</code> deploys multiple identical connector pods that each register independently with dsx-connect, each with a unique connector UUID. The UI will show multiple connectors for the same asset/filter.</li> <li>A Full Scan request (from the UI or API) targets a single registered connector instance. Increasing <code>replicaCount</code> does not parallelize a single full-scan enqueue path.</li> <li>Where replicas do help:<ul> <li>High availability (one pod can restart while another continues to serve), and</li> <li>Serving concurrent <code>read_file</code> requests from the dsx-connect scan-request workers (Kubernetes Service balances connections across pods; higher Celery concurrency opens more connections and spreads load). To parallelize work across a single asset intentionally, prefer:</li> <li>Increasing connector <code>workers</code> (Uvicorn processes) for in-pod concurrency, and/or</li> <li>Running multiple connector releases with different <code>DSXCONNECTOR_FILTER</code> partitions (sharding), so Full Scan is performed in parallel across slices by distinct connector instances.</li> </ul> </li> </ul>"},{"location":"deployment/kubernetes/dsx-connect/#ingress-load-balancer-examples","title":"Ingress &amp; Load Balancer Examples","text":"<p>The core chart deliberately stops at ClusterIP services so it works across any platform. If you need ingress routes or load balancer services, use the sample manifests under <code>dsx_connect/deploy/helm/examples/ingress/</code>.</p> <ul> <li>Pick the file that matches your environment (e.g., <code>ingress-colima.yaml</code>, <code>ingress-eks-alb.yaml</code>, <code>openshift-route.yaml</code>)</li> <li>Edit hosts/TLS secrets as needed</li> <li>Apply it after installing the chart:</li> </ul> <pre><code>kubectl apply -f dsx_connect/deploy/helm/examples/ingress/ingress-colima.yaml\n</code></pre> <p>These are meant as starting points\u2014feel free to adapt or author your own ingress resources if your environment requires different settings.</p>"},{"location":"deployment/kubernetes/filesystem/","title":"Filesystem Connector \u2014 Helm Deployment","text":"<p>Deploy the <code>filesystem-connector-chart</code> (under <code>connectors/filesystem/deploy/helm</code>) to scan on-prem or mounted network shares from Kubernetes.</p>"},{"location":"deployment/kubernetes/filesystem/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.19+ cluster with <code>kubectl</code>.</li> <li>Helm 3.2+.</li> <li>Access to <code>oci://registry-1.docker.io/dsxconnect/filesystem-connector-chart</code>.</li> <li>A volume (PVC, hostPath, or CSI driver) that exposes the filesystem to be scanned.</li> </ul>"},{"location":"deployment/kubernetes/filesystem/#preflight-tasks","title":"Preflight Tasks","text":"<ol> <li>Provision the volume and (if using PVC) bind it in the connector namespace.</li> <li>Decide where the volume should mount inside the pod (default <code>/app/scan_folder</code>).</li> <li>Confirm the namespace can reach dsx-connect\u2019s service/ingress.</li> </ol>"},{"location":"deployment/kubernetes/filesystem/#configuration","title":"Configuration","text":""},{"location":"deployment/kubernetes/filesystem/#required-settings","title":"Required settings","text":"<ul> <li><code>scanVolume.*</code>: enable the mount and point to the PVC/hostPath plus <code>mountPath</code>.</li> <li><code>env.DSXCONNECTOR_ASSET</code>: automatically set to <code>scanVolume.mountPath</code>, override if needed.</li> <li><code>env.DSXCONNECTOR_FILTER</code>: optional rsync-style include/exclude list (see Filter reference).</li> <li>Monitoring flags: <code>env.DSXCONNECTOR_MONITOR</code>, <code>env.DSXCONNECTOR_MONITOR_FORCE_POLLING</code>, <code>env.DSXCONNECTOR_MONITOR_POLL_INTERVAL_MS</code>.</li> <li>Remediation: <code>env.DSXCONNECTOR_ITEM_ACTION</code>, <code>env.DSXCONNECTOR_ITEM_ACTION_MOVE_METAINFO</code>.</li> <li><code>workers</code> and <code>replicaCount</code> for concurrency/HA.</li> </ul>"},{"location":"deployment/kubernetes/filesystem/#storage-examples","title":"Storage examples","text":"<p>PVC:</p> <pre><code>scanVolume:\n  enabled: true\n  existingClaim: my-files-pvc\n  mountPath: /app/scan_folder\n</code></pre> <p>HostPath (single-node dev):</p> <pre><code>scanVolume:\n  enabled: true\n  hostPath: /Users/&lt;you&gt;/scan-data\n  mountPath: /app/scan_folder\n</code></pre> <p>Verify after deployment:</p> <pre><code>kubectl exec -it deploy/filesystem-connector -- ls /app/scan_folder\n</code></pre>"},{"location":"deployment/kubernetes/filesystem/#dsx-connect-endpoint","title":"dsx-connect endpoint","text":"<p>Defaults to <code>http://dsx-connect-api</code> (or HTTPS when TLS is on). Override via <code>env.DSXCONNECTOR_DSX_CONNECT_URL</code> for external endpoints.</p>"},{"location":"deployment/kubernetes/filesystem/#authentication-tls","title":"Authentication &amp; TLS","text":""},{"location":"deployment/kubernetes/filesystem/#ingress-networkpolicy-optional","title":"Ingress &amp; NetworkPolicy (optional)","text":"<ul> <li>Enable <code>ingressWebhook</code> if you must expose <code>/filesystem-connector/webhook/event</code>.</li> <li>Use <code>networkPolicy.allowFrom</code> to restrict ingress to dsx-connect and your ingress controller (example in the chart values).</li> </ul>"},{"location":"deployment/kubernetes/filesystem/#deployment","title":"Deployment","text":""},{"location":"deployment/kubernetes/filesystem/#method-1-oci-chart-with-cli-overrides-fastest","title":"Method 1 \u2013 OCI chart with CLI overrides (fastest)","text":"<pre><code>helm install fs-dev oci://registry-1.docker.io/dsxconnect/filesystem-connector-chart \\\n  --version &lt;chart-version&gt; \\\n  --set scanVolume.enabled=true \\\n  --set scanVolume.existingClaim=my-pvc \\\n  --set-string env.DSXCONNECTOR_FILTER=\"\" \\\n  --set-string image.tag=&lt;connector-version&gt;\n</code></pre>"},{"location":"deployment/kubernetes/filesystem/#method-2-work-from-a-pulled-chart-edit-values-locally","title":"Method 2 \u2013 Work from a pulled chart (edit values locally)","text":"<pre><code>helm pull oci://registry-1.docker.io/dsxconnect/filesystem-connector-chart --version &lt;chart-version&gt;\ntar -xzf filesystem-connector-chart-&lt;chart-version&gt;.tgz\ncd filesystem-connector-chart\n</code></pre> <p>Example values:</p> <pre><code>scanVolume:\n  enabled: true\n  existingClaim: my-pvc\n  mountPath: /app/scan_folder\nenv:\n  DSXCONNECTOR_FILTER: \"**/*.zip\"\n  DSXCONNECTOR_MONITOR: \"true\"\nimage:\n  tag: \"&lt;connector-version&gt;\"\n</code></pre> <p>Install from the extracted chart root (<code>.</code>):</p> <pre><code>helm install fs-dev . -f values-dev.yaml\n</code></pre>"},{"location":"deployment/kubernetes/filesystem/#method-3-gitops-production-style","title":"Method 3 \u2013 GitOps / production style","text":"<pre><code>helm upgrade --install fs-prod oci://registry-1.docker.io/dsxconnect/filesystem-connector-chart \\\n  --version &lt;chart-version&gt; \\\n  -f values-prod.yaml\n</code></pre>"},{"location":"deployment/kubernetes/filesystem/#verification","title":"Verification","text":"<pre><code>helm list\nkubectl get pods\nkubectl logs deploy/filesystem-connector -f\n</code></pre>"},{"location":"deployment/kubernetes/filesystem/#scaling-guidance","title":"Scaling guidance","text":"<ul> <li>Increase <code>workers</code> for additional in-pod <code>read_file</code> concurrency.</li> <li>Raise <code>replicaCount</code> for HA. Each pod registers independently; replicas do not split an individual full scan but improve throughput/resiliency.</li> </ul> <p>See <code>connectors/filesystem/deploy/helm/values.yaml</code> for the full parameter reference.</p>"},{"location":"deployment/kubernetes/getting-started-quickstart-azure/","title":"Quick Tutorial 2: Helm with local values (dsx-connect + DSXA + Azure Blob Connector)","text":"<p>This tutorial mirrors Tutorial #1 but targets the Azure Blob connector. Instead of inline <code>--set</code> overrides, we\u2019ll pull the charts locally, edit <code>values.yaml</code>, and apply example Secret manifests. This approach scales better for GitOps or teams that prefer checked-in configs.</p>"},{"location":"deployment/kubernetes/getting-started-quickstart-azure/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.19+ cluster (Colima with <code>--kubernetes</code> works well for local runs).</li> <li>Helm 3.2+, kubectl, access to Docker Hub (<code>helm registry login registry-1.docker.io</code>).</li> <li>Azure storage connection string for a test container with sample files.</li> <li>DSXA appliance URL, token, and scanner ID.</li> </ul>"},{"location":"deployment/kubernetes/getting-started-quickstart-azure/#1-download-extract-the-helm-charts-set-helper-variables","title":"1. Download &amp; extract the Helm charts; set helper variables","text":"<pre><code># Example versions; substitute the chart versions you need.\nmkdir -p charts &amp;&amp; cd charts\nhelm pull oci://registry-1.docker.io/dsxconnect/dsx-connect-chart --version 0.3.46 --untar\nhelm pull oci://registry-1.docker.io/dsxconnect/azure-blob-storage-connector-chart --version 0.5.27 --untar\ncd ..\n</code></pre> <pre><code>export NAMESPACE=dsx-tutorial-2\nexport RELEASE=dsx-tutorial-2\nexport ENROLLMENT_TOKEN=$(uuidgen)\nexport DSXA_APPLIANCE_URL=your-dsxa-appliance.example.com\nexport DSXA_SCANNER_ID=1\nexport DSXA_TOKEN=changeme\n</code></pre> <pre><code>kubectl create namespace $NAMESPACE\n</code></pre>"},{"location":"deployment/kubernetes/getting-started-quickstart-azure/#2-create-secrets-from-example-manifests","title":"2. Create Secrets from example manifests","text":"<p>Open <code>dsx_connect/deploy/helm/examples/secrets/auth-enrollment-secret.yaml</code> and <code>connectors/azure_blob_storage/deploy/helm/azure-secret.yaml</code>. Edit the placeholders (or copy the files elsewhere, edit, and apply). Set:</p> <ul> <li>In <code>auth-enrollment-secret.yaml</code>: <code>metadata.name = ${RELEASE}-dsx-connect-api-auth-enrollment</code>, <code>metadata.namespace = ${NAMESPACE}</code>, and <code>stringData.token = ${ENROLLMENT_TOKEN}</code>.</li> <li>In <code>azure-secret.yaml</code>: <code>metadata.namespace = ${NAMESPACE}</code> and set <code>stringData.AZURE_STORAGE_CONNECTION_STRING</code>.</li> </ul> <p>Apply both:</p> <pre><code>kubectl apply -f dsx_connect/deploy/helm/examples/secrets/auth-enrollment-secret.yaml\nkubectl apply -f connectors/azure_blob_storage/deploy/helm/azure-secret.yaml\n</code></pre>"},{"location":"deployment/kubernetes/getting-started-quickstart-azure/#3-pull-charts-locally","title":"3. Pull charts locally","text":""},{"location":"deployment/kubernetes/getting-started-quickstart-azure/#4-edit-values-files","title":"4. Edit values files","text":"<p>Create <code>values-dsx.yaml</code>:</p> <pre><code>global:\n  env:\n    DSXCONNECT_SCANNER__SCAN_BINARY_URL: \"\"\ndsxa-scanner:\n  enabled: true\n  env:\n    APPLIANCE_URL: \"${DSXA_APPLIANCE_URL}\"\n    TOKEN: \"${DSXA_TOKEN}\"\n    SCANNER_ID: \"${DSXA_SCANNER_ID}\"\ndsx-connect-api:\n  auth:\n    enabled: true\n    enrollment:\n      key: ENROLLMENT_TOKEN\n</code></pre> <p>Create <code>values-azure.yaml</code>:</p> <pre><code>env:\n  DSXCONNECTOR_ASSET: \"mytestcontainer\"\nauth_dsxconnect:\n  enabled: true\n  enrollmentSecretName: azure-connector-env\n  enrollmentKey: DSXCONNECT_ENROLLMENT_TOKEN\nsecrets:\n  name: azure-connector-env\n</code></pre> <p>Replace the <code>${DSXA_*}</code> placeholders via an editor or by running <code>envsubst</code> (similar to the Secret step).</p>"},{"location":"deployment/kubernetes/getting-started-quickstart-azure/#5-install-dsx-connect","title":"5. Install dsx-connect","text":"<pre><code>helm upgrade --install $RELEASE ./charts/dsx-connect-chart \\\n  --namespace $NAMESPACE \\\n  -f values-dsx.yaml\n</code></pre> <p>Wait for pods:</p> <pre><code>kubectl get pods -n $NAMESPACE\n</code></pre>"},{"location":"deployment/kubernetes/getting-started-quickstart-azure/#6-install-the-azure-blob-connector","title":"6. Install the Azure Blob connector","text":"<pre><code>helm upgrade --install azure-connector ./charts/azure-blob-storage-connector-chart \\\n  --namespace $NAMESPACE \\\n  -f values-azure.yaml\n</code></pre>"},{"location":"deployment/kubernetes/getting-started-quickstart-azure/#7-verify-and-test","title":"7. Verify and test","text":"<pre><code>kubectl port-forward svc/${RELEASE}-dsx-connect-api 8586:8586 -n $NAMESPACE\n</code></pre> <p>Open <code>http://localhost:8586</code>, confirm the Azure connector card shows READY, and trigger a full scan. Upload a sample blob to the container to confirm dsx-connect picks it up.</p>"},{"location":"deployment/kubernetes/getting-started-quickstart-azure/#cleanup","title":"Cleanup","text":"<pre><code>helm uninstall azure-connector -n $NAMESPACE\nhelm uninstall $RELEASE -n $NAMESPACE\nkubectl delete namespace $NAMESPACE\nrm -rf charts values-dsx.yaml values-azure.yaml dsx-auth-secret.yaml azure-connector-secret.yaml\n</code></pre>"},{"location":"deployment/kubernetes/getting-started-quickstart/","title":"DSX-Connect and AWS S3 Connector on K8S Tutorial","text":"<p>This quickstart runs on a lightweight k3s cluster via Colima to keep things simple, but the same steps work on any Kubernetes cluster. We\u2019ll deploy dsx-connect (with auth enabled so only enrollment-tokened connectors can join), the in-cluster DSXA scanner, and the AWS S3 connector. Everything comes straight from the Helm OCI charts using only CLI overrides, so you can copy/paste the commands (substituting your AWS credentials).</p>"},{"location":"deployment/kubernetes/getting-started-quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.19+ cluster with working LoadBalancer/NodePort support. On macOS, Colima with <code>colima start --kubernetes</code> is a compact local k3s option.</li> <li>Helm 3.2+, kubectl, and access to Docker Hub\u2019s OCI registry (<code>helm registry login registry-1.docker.io</code>).</li> <li>AWS access key/secret with read/write access to a test bucket and at least one sample file already in the bucket.</li> <li>DSXA appliance URL, scanner ID, and API token that you are allowed to use for testing.</li> </ul>"},{"location":"deployment/kubernetes/getting-started-quickstart/#1-set-variables-and-namespace","title":"1. Set variables and namespace","text":"<pre><code>export NAMESPACE=dsx-tutorial-1\nexport RELEASE=dsx-tutorial-1\nexport AWS_BUCKET=my-demo-bucket      # replace with a real bucket\nexport ENROLLMENT_TOKEN=$(uuidgen)    # or any strong random string\nexport DSXA_APPLIANCE_URL=your-dsxa-appliance.example.com\nexport DSXA_SCANNER_ID=1\nexport DSXA_TOKEN=changeme\n\nkubectl create namespace $NAMESPACE\n</code></pre>"},{"location":"deployment/kubernetes/getting-started-quickstart/#2-create-secrets","title":"2. Create secrets","text":""},{"location":"deployment/kubernetes/getting-started-quickstart/#enrollment-token","title":"Enrollment token","text":"<pre><code>kubectl create secret generic ${RELEASE}-dsx-connect-api-auth-enrollment \\\n  -n $NAMESPACE \\\n  --from-literal=ENROLLMENT_TOKEN=\"$ENROLLMENT_TOKEN\"\n</code></pre> <p>The dsx-connect chart always looks for a Secret named <code>&lt;release&gt;-dsx-connect-api-auth-enrollment</code> with a key called <code>ENROLLMENT_TOKEN</code>, so keep that convention when you create it.</p>"},{"location":"deployment/kubernetes/getting-started-quickstart/#aws-env-connector-config","title":"AWS env + connector config","text":"<p>Export your AWS creds (or pull them from <code>~/.aws/credentials</code> manually) so the heredoc can reference them:</p> <p><pre><code>export AWS_ACCESS_KEY_ID=&lt;your-access-key&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;your-secret-key&gt;\n</code></pre> Note: If you already have a profile in <code>~/.aws/credentials</code>, you can pull the values directly: <pre><code>export AWS_ACCESS_KEY_ID=$(aws configure get default.aws_access_key_id)\nexport AWS_SECRET_ACCESS_KEY=$(aws configure get default.aws_secret_access_key)\n</code></pre></p> <p>Create a temporary file (do not commit this):</p> <pre><code>cat &lt;&lt;EOF &gt; .env.aws-creds\nAWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY\nEOF\n</code></pre> <pre><code>kubectl create secret generic aws-credentials \\\n  --from-env-file=.env.aws-creds \\\n  -n $NAMESPACE\n</code></pre> <p>Optional: If you prefer editing YAML directly or storing secrets in source control, you can create the Secret like this instead:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-credentials  # default name expected by the Helm chart\n  namespace: ${NAMESPACE}\ntype: Opaque\nstringData:\n  AWS_ACCESS_KEY_ID: \"&lt;your-access-key-id&gt;\"\n  AWS_SECRET_ACCESS_KEY: \"&lt;your-secret-access-key&gt;\"\n</code></pre> <p>Save it as <code>aws-secret.yaml</code>, edit the values, and apply with <code>kubectl apply -f aws-secret.yaml</code>.</p>"},{"location":"deployment/kubernetes/getting-started-quickstart/#3-install-dsx-connect-api-dsxa","title":"3. Install dsx-connect (API + DSXA)","text":"<p>Enables authentication and the bundled DSXA scanner:</p> <pre><code>helm upgrade --install $RELEASE \\\n  oci://registry-1.docker.io/dsxconnect/dsx-connect-chart \\\n  --namespace $NAMESPACE \\\n  --set dsxa-scanner.enabled=true \\\n  --set dsx-connect-api.auth.enabled=true \\\n  --set-string dsxa-scanner.env.APPLIANCE_URL=$DSXA_APPLIANCE_URL \\\n  --set-string dsxa-scanner.env.TOKEN=$DSXA_TOKEN \\\n  --set-string dsxa-scanner.env.SCANNER_ID=$DSXA_SCANNER_ID \\\n  --set-string global.image.tag=0.3.46\n</code></pre> <p>Example versions: the <code>0.3.46</code> tag should match the dsx-connect chart/appVersion you intend to run.</p> <p>For production, store DSXA info in a Kubernetes Secret and use <code>values.yaml</code> or <code>helm upgrade --set-file</code> so tokens are not exposed in shell history. Here we keep everything inline for clarity.</p> <p>Check pods:</p> <pre><code>kubectl get pods -n $NAMESPACE\n</code></pre>"},{"location":"deployment/kubernetes/getting-started-quickstart/#4-install-aws-s3-connector","title":"4. Install AWS S3 connector","text":"<pre><code>helm upgrade --install aws-s3 \\\n  oci://registry-1.docker.io/dsxconnect/aws-s3-connector-chart \\\n  --namespace $NAMESPACE \\\n  --set-string env.DSXCONNECTOR_ASSET=$AWS_BUCKET \\\n  --set auth_dsxconnect.enabled=true \\\n  --set auth_dsxconnect.enrollmentSecretName=${RELEASE}-dsx-connect-api-auth-enrollment \\\n  --set auth_dsxconnect.enrollmentKey=ENROLLMENT_TOKEN \\\n  --set-string image.tag=0.5.27\n</code></pre> <p>Replace <code>0.5.27</code> with the AWS connector version you plan to run.</p> <p>Watch logs until the connector reports READY:</p> <pre><code>kubectl logs deploy/aws-s3-aws-s3-connector-chart -n $NAMESPACE -f | grep READY\n</code></pre>"},{"location":"deployment/kubernetes/getting-started-quickstart/#5-access-the-ui-and-test","title":"5. Access the UI and test","text":"<p>Port-forward the dsx-connect API/UI:</p> <pre><code>kubectl port-forward svc/dsx-connect-api 8080:80 -n $NAMESPACE\n</code></pre> <p>Port-forwarding is a quick way to expose a service for local testing only. In real deployments you\u2019d configure an Ingress controller, LoadBalancer service, or some other edge proxy based on your cluster environment. We will provide examples throughout the guides, but the exact setup is cluster-dependent.</p> <p>Visit <code>http://localhost:8080</code>, confirm the AWS connector shows READY, and launch a Full Scan from the UI. Files already in <code>$AWS_BUCKET</code> should queue. </p> <p>Note: Webhook/on-access tests require S3 event wiring, which is beyond the scope of this quickstart.  See the Connector deployment for AWS S3 for more details.</p>"},{"location":"deployment/kubernetes/getting-started-quickstart/#cleanup","title":"Cleanup","text":"<pre><code>helm uninstall aws-s3 -n $NAMESPACE\nhelm uninstall $RELEASE -n $NAMESPACE\nkubectl delete namespace $NAMESPACE\nrm .env.aws-creds\n</code></pre>"},{"location":"deployment/kubernetes/getting-started/","title":"Kubernetes Deployment Tips","text":"<p>Use this page as the single checklist before diving into the connector-specific guides. It covers the cluster requirements, where to fetch Helm charts, and the high-level deployment workflow shared by every dsx-connect component.</p>"},{"location":"deployment/kubernetes/getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.19+ (tested on k3s/AKS/EKS/GKE)</li> <li>Helm 3.2+ and <code>kubectl</code></li> <li>Cluster admin rights to create namespaces, Secrets, and ServiceAccounts</li> <li>Access to the dsx-connect Helm charts hosted in Docker Hub\u2019s OCI registry: https://hub.docker.com/r/dsxconnect</li> <li>Connector-specific credentials (for example: AWS IAM keys, Azure AD app secrets, GCP service-account JSON; see Reference pages for each provider)</li> </ul>"},{"location":"deployment/kubernetes/getting-started/#helm-chart-locations","title":"Helm chart locations","text":"<p>The <code>inv release-all</code> pipeline publishes every chart to Docker Hub under the <code>dsxconnect</code> namespace. Browse the full catalog (images and charts) at https://hub.docker.com/r/dsxconnect. Pull/install specific charts directly with Helm\u2019s OCI support.</p> Example component OCI reference Example install dsx-connect core (API + workers) <code>oci://registry-1.docker.io/dsxconnect/dsx-connect-chart</code> <code>helm install dsx oci://registry-1.docker.io/dsxconnect/dsx-connect-chart --version 0.3.44 -f your-values.yaml</code> Filesystem connector <code>oci://registry-1.docker.io/dsxconnect/filesystem-connector-chart</code> <code>helm install fs oci://registry-1.docker.io/dsxconnect/filesystem-connector-chart --version 0.5.25</code> Google Cloud Storage connector <code>oci://registry-1.docker.io/dsxconnect/google-cloud-storage-connector-chart</code> <code>helm install gcs oci://registry-1.docker.io/dsxconnect/google-cloud-storage-connector-chart --version 0.5.25 --set env.DSXCONNECTOR_ASSET=my-bucket</code> SharePoint connector <code>oci://registry-1.docker.io/dsxconnect/sharepoint-connector-chart</code> <code>helm install sp oci://registry-1.docker.io/dsxconnect/sharepoint-connector-chart --version 0.5.25</code> <p>Tip: Use <code>helm pull &lt;oci-url&gt; --version X --untar</code> if you want to download the chart, inspect or customize a chart locally before installing.</p>"},{"location":"deployment/kubernetes/getting-started/#deployment-flow","title":"Deployment flow","text":"<ol> <li>Prepare secrets: Create Kubernetes Secrets for enrollment tokens, connector credentials (AWS keys, Azure app secrets, GCP JSON), and any TLS bundles. Each connector guide links to the exact <code>kubectl create secret</code> commands.</li> <li>Deploy dsx-connect core: Follow dsx-connect (Helm) to install the API, workers, Redis, and syslog stack. Verify <code>/readyz</code> and watch the UI before layering connectors.</li> <li>Deploy connectors: Pick the connector guide under this section (Filesystem, AWS S3, Azure Blob, Google Cloud Storage, SharePoint, OneDrive, etc.). Each page documents the required values, secrets, and network exposure.</li> <li>Ingress &amp; auth: Configure your cluster ingress controller (NGINX, ALB, etc.) and, where required, expose only the connector webhook path. Front the dsx-connect UI/API with your organization\u2019s SSO or oauth2-proxy.</li> <li>Monitoring &amp; rotation: Enable Prometheus/Syslog targets if you have centralized logging, and plan secret rotations (enrollment token CSVs, connector credentials, DSX-HMAC reprovisioning) as described in Deployment \u2192 Authentication.</li> </ol> <p>This keeps sensitive data in Secrets and simplifies upgrades (<code>helm upgrade -f values-prod.yaml</code>).</p>"},{"location":"deployment/kubernetes/getting-started/#reusing-env-files-from-compose-helm","title":"Reusing <code>.env</code> files from Compose / Helm","text":"<p>If you already maintain <code>.env</code> files for Docker Compose, convert them directly into Kubernetes Secrets:</p> <ol> <li>Keep your connector settings in a <code>KEY=value</code> file such as <code>.env.aws-s3</code> (see Docker Quickstart step 4 for an example layout).</li> <li>Create a Secret: <pre><code>kubectl create secret generic aws-s3-connector-env \\\n  --from-env-file=.env.aws-s3 \\\n  --namespace your-namespace\n</code></pre></li> <li>Reference it via the chart, e.g.:    <pre><code>envSecretRefs:\n  - aws-s3-connector-env\nauth_dsxconnect:\n  enabled: true\n  enrollmentSecretName: aws-s3-connector-env\n  enrollmentKey: DSXCONNECT_ENROLLMENT_TOKEN\n</code></pre></li> </ol> <p>This way Docker Compose, Helm, and GitOps overlays can all pull from the same source of truth.</p>"},{"location":"deployment/kubernetes/getting-started/#next-steps","title":"Next steps","text":"<ul> <li>Deploy dsx-connect core via dsx-connect (Helm).</li> <li>Choose the connector page that matches your repository (Filesystem, AWS S3, Azure Blob Storage, Google Cloud Storage, SharePoint, OneDrive, M365 Mail, etc.).</li> <li>Review Deployment \u2192 Authentication for the enrollment + DSX-HMAC model used by every connector.</li> </ul> <p>Once the core stack is online and at least one connector is registered, log into the dsx-connect UI to monitor health, run scans, and verify webhook activity.</p>"},{"location":"deployment/kubernetes/google-cloud-storage/","title":"Google Cloud Storage Connector \u2014 Helm Deployment","text":"<p>Use this guide to deploy the <code>google-cloud-storage-connector-chart</code> for full scans, monitoring scans, and remediation actions.</p>"},{"location":"deployment/kubernetes/google-cloud-storage/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.19+ and <code>kubectl</code>.</li> <li>Helm 3.2+.</li> <li>Access to <code>oci://registry-1.docker.io/dsxconnect/google-cloud-storage-connector-chart</code>.</li> <li>A Google Cloud service account JSON key with the permissions listed in Reference \u2192 Google Cloud Credentials.</li> </ul>"},{"location":"deployment/kubernetes/google-cloud-storage/#preflight-tasks","title":"Preflight Tasks","text":"<p>Create the service-account Secret before installing:</p> <pre><code># gcp-sa-secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: gcp-sa\ntype: Opaque\nstringData:\n  service-account.json: |\n    { ...your JSON key... }\n</code></pre> <pre><code>kubectl apply -f gcp-sa-secret.yaml\n</code></pre> <p>The chart references <code>gcp-sa</code> by default; set <code>serviceAccount.secretName</code> if you use a different name.</p>"},{"location":"deployment/kubernetes/google-cloud-storage/#configuration","title":"Configuration","text":""},{"location":"deployment/kubernetes/google-cloud-storage/#required-settings","title":"Required settings","text":"Key Description <code>env.DSXCONNECTOR_ASSET</code> Bucket or <code>bucket/prefix</code> root to scan. <code>env.DSXCONNECTOR_FILTER</code> Optional rsync-style include/exclude list relative to the asset root (see Filter reference). <code>env.DSXCONNECTOR_ITEM_ACTION</code> / <code>env.DSXCONNECTOR_ITEM_ACTION_MOVE_METAINFO</code> Remediation rules (<code>nothing</code>, <code>delete</code>, <code>move</code>, <code>move_tag</code>, <code>tag</code>). <code>env.DSXCONNECTOR_MONITOR</code> <code>\"true\"</code> to enable on-access scanning via Pub/Sub. <code>workers</code>, <code>replicaCount</code> Concurrency and HA knobs."},{"location":"deployment/kubernetes/google-cloud-storage/#monitoring-inputs","title":"Monitoring inputs","text":"<p>When <code>env.DSXCONNECTOR_MONITOR=true</code>, populate the Pub/Sub settings:</p> Key Description <code>env.GCS_PUBSUB_PROJECT_ID</code> Project that owns the Pub/Sub subscription. <code>env.GCS_PUBSUB_SUBSCRIPTION</code> Subscription name or full path (<code>projects/&lt;proj&gt;/subscriptions/&lt;sub&gt;</code>). <code>env.GCS_PUBSUB_ENDPOINT</code> Optional override (useful for local emulators). Leave blank for production. <p>Pub/Sub is the recommended trigger path. You can also drive the connector via <code>/webhook/event</code> from Cloud Functions/Run; in that case leave <code>env.DSXCONNECTOR_MONITOR=false</code> and expose the webhook ingress.</p>"},{"location":"deployment/kubernetes/google-cloud-storage/#dsx-connect-endpoint","title":"dsx-connect endpoint","text":"<p>Defaults to the in-cluster service (<code>http://dsx-connect-api</code>). Override via <code>env.DSXCONNECTOR_DSX_CONNECT_URL</code> for external deployments.</p>"},{"location":"deployment/kubernetes/google-cloud-storage/#authentication-tls","title":"Authentication &amp; TLS","text":""},{"location":"deployment/kubernetes/google-cloud-storage/#webhook-ingress-optional","title":"Webhook ingress (optional)","text":"<p>Enable <code>ingressWebhook</code> to expose <code>/google-cloud-storage-connector/webhook/event</code> if you rely on Cloud Functions/Run instead of Pub/Sub. Lock ingress down via annotations or NetworkPolicy so only trusted sources can reach it.</p>"},{"location":"deployment/kubernetes/google-cloud-storage/#deployment","title":"Deployment","text":""},{"location":"deployment/kubernetes/google-cloud-storage/#method-1-oci-chart-with-cli-overrides-fastest","title":"Method 1 \u2013 OCI chart with CLI overrides (fastest)","text":"<pre><code>helm install gcs-dev oci://registry-1.docker.io/dsxconnect/google-cloud-storage-connector-chart \\\n  --version &lt;chart-version&gt; \\\n  --set env.DSXCONNECTOR_ASSET=my-bucket/prefix \\\n  --set-string env.DSXCONNECTOR_FILTER=\"\" \\\n  --set-string image.tag=&lt;connector-version&gt;\n</code></pre>"},{"location":"deployment/kubernetes/google-cloud-storage/#method-2-work-from-a-pulled-chart-edit-values-locally","title":"Method 2 \u2013 Work from a pulled chart (edit values locally)","text":"<pre><code>helm pull oci://registry-1.docker.io/dsxconnect/google-cloud-storage-connector-chart --version &lt;chart-version&gt;\ntar -xzf google-cloud-storage-connector-chart-&lt;chart-version&gt;.tgz\ncd google-cloud-storage-connector-chart\n</code></pre> <p>Example <code>values-dev.yaml</code>:</p> <pre><code>env:\n  DSXCONNECTOR_ASSET: \"my-bucket\"\n  DSXCONNECTOR_FILTER: \"**/*.pdf\"\n  DSXCONNECTOR_MONITOR: \"true\"\n  GCS_PUBSUB_PROJECT_ID: \"my-project\"\n  GCS_PUBSUB_SUBSCRIPTION: \"gcs-events\"\nimage:\n  tag: \"&lt;connector-version&gt;\"\n</code></pre> <p>Install from the extracted chart root (<code>.</code>):</p> <pre><code>helm install gcs-dev . -f values-dev.yaml\n</code></pre>"},{"location":"deployment/kubernetes/google-cloud-storage/#method-3-gitops-production-style","title":"Method 3 \u2013 GitOps / production style","text":"<pre><code>helm upgrade --install gcs-prod oci://registry-1.docker.io/dsxconnect/google-cloud-storage-connector-chart \\\n  --version &lt;chart-version&gt; \\\n  -f values-prod.yaml\n</code></pre>"},{"location":"deployment/kubernetes/google-cloud-storage/#verification","title":"Verification","text":"<pre><code>helm list\nkubectl get pods\nkubectl logs deploy/google-cloud-storage-connector -f\n</code></pre>"},{"location":"deployment/kubernetes/google-cloud-storage/#scaling","title":"Scaling","text":"<ul> <li>Increase <code>workers</code> for additional in-pod concurrency.</li> <li>Raise <code>replicaCount</code> for HA or more <code>read_file</code> throughput; each pod registers separately with dsx-connect.</li> <li>For Pub/Sub, ensure your subscription acknowledgement deadlines accommodate scan duration.</li> </ul> <p>Refer to <code>connectors/google_cloud_storage/deploy/helm/values.yaml</code> for the full option set.</p>"},{"location":"deployment/kubernetes/m365-mail/","title":"M365 Mail Connector \u2014 Helm Deployment","text":"<p>The Helm chart under <code>connectors/m365_mail/deploy/helm/</code> packages the connector for Kubernetes. This guide focuses on the webhook ingress and environment settings needed when Microsoft Graph must reach the connector from outside the cluster.</p>"},{"location":"deployment/kubernetes/m365-mail/#values-overview","title":"Values Overview","text":"<p>Key sections in <code>values.yaml</code>:</p> Section Purpose <code>env</code> Graph credentials, mailbox list, connector options, dsx-connect URL. <code>auth_dsxconnect.enabled</code> Enables DSX-HMAC verification on inbound connector routes. <code>auth_dsxconnect.enrollmentSecretName</code> / <code>.enrollmentKey</code> References the enrollment-token Secret shared with dsx-connect. <code>webhook.publicUrl</code> Optional base URL the connector advertises to Microsoft Graph (new). <code>ingressWebhook</code> Ingress definition for exposing <code>/m365-mail-connector/webhook/event</code>."},{"location":"deployment/kubernetes/m365-mail/#retrieve-microsoft-entra-credentials","title":"Retrieve Microsoft Entra credentials","text":"<p>Guidance current as of 2025-11-04. The Azure portal occasionally renames blades or tabs; if you see different labels, follow the closest equivalent.</p>"},{"location":"deployment/kubernetes/m365-mail/#portal-workflow","title":"Portal workflow","text":"<ol> <li>Sign in at https://portal.azure.com with an Application Administrator or Global Administrator account. Make sure the header shows the directory that owns the mailboxes you plan to scan. The Tenant ID shown under Directory + subscription becomes <code>M365_TENANT_ID</code>.</li> <li>Go to Microsoft Entra ID \u2192 App registrations \u2192 New registration. Give the app a descriptive name (for example <code>dsx-m365-mail-connector</code>), keep the default Single tenant option, and click Register. On the Overview blade copy the Application (client) ID for <code>M365_CLIENT_ID</code>.</li> <li>Open Certificates &amp; secrets, add a new client secret, set the expiry that fits your rotation policy, and copy the Value immediately for <code>M365_CLIENT_SECRET</code> (the portal will never show it again).</li> <li>Under API permissions choose Add a permission \u2192 Microsoft Graph \u2192 Application, add <code>Mail.Read</code>, <code>Mail.ReadWrite</code>, and <code>Files.Read.All</code>, then click Grant admin consent so the roles are enabled for the app.</li> </ol> <p>Store the tenant ID, client ID, and secret in your secret manager of choice (Kubernetes Secret, Azure Key Vault, etc.) before you render the Helm values.</p>"},{"location":"deployment/kubernetes/m365-mail/#azure-cli-alternative","title":"Azure CLI alternative","text":"<ol> <li>Log the CLI into the correct tenant. Use <code>az login --tenant &lt;tenant-id&gt;</code> if you already know the tenant, or run <code>az login</code> first, list your available accounts with <code>az account list -o table</code>, and select the right one via <code>az account set --subscription \"&lt;subscription-id-or-name&gt;\"</code>. If you previously authenticated against another tenant, run <code>az logout</code> (or <code>az account clear</code> to remove all cached accounts) before logging in again.</li> <li>Create the application and credentials. The snippet below mirrors the portal workflow and prints the values you need:</li> </ol> <pre><code>APP_NAME=\"dsx-m365-mail-connector\"\nGRAPH_APP_ID=\"00000003-0000-0000-c000-000000000000\"\naz ad app create --display-name \"$APP_NAME\"\nCLIENT_ID=$(az ad app list --display-name \"$APP_NAME\" --query \"[0].appId\" -o tsv)\nSECRET=$(az ad app credential reset --id \"$CLIENT_ID\" --display-name dsx-connector --years 2 --query password -o tsv)\naz ad app permission add --id \"$CLIENT_ID\" --api \"$GRAPH_APP_ID\" \\\n  --api-permissions Mail.Read=Role Mail.ReadWrite=Role Files.Read.All=Role\naz ad app permission admin-consent --id \"$CLIENT_ID\"\nTENANT_ID=$(az account show --query tenantId -o tsv)\n\nprintf 'Tenant ID: %s\\nClient ID: %s\\nClient Secret: %s\\n' \"$TENANT_ID\" \"$CLIENT_ID\" \"$SECRET\"\n</code></pre> <p>See Reference \u2192 Azure Credentials for expanded CLI automation (including SharePoint and OneDrive) and admin-consent troubleshooting tips.</p>"},{"location":"deployment/kubernetes/m365-mail/#basic-values-example","title":"Basic Values Example","text":"<pre><code>env:\n  DSXCONNECTOR_DSX_CONNECT_URL: \"http://dsx-connect-api:8586\"\n  DSXCONNECTOR_CONNECTOR_URL: \"http://m365-mail-connector:80\"\n  M365_TENANT_ID: \"&lt;tenant-guid&gt;\"\n  M365_CLIENT_ID: \"&lt;app-id&gt;\"\n  M365_CLIENT_SECRET: \"&lt;client-secret&gt;\"\n  M365_MAILBOX_UPNS: \"user1@contoso.com,user2@contoso.com\"\n  M365_CLIENT_STATE: \"3f1e9de2-ee73-4b02-8d17-16adf0c6a28c\"\n  DSXCONNECTOR_TRIGGER_DELTA_ON_NOTIFICATION: \"true\"\n\nwebhook:\n  publicUrl: \"https://mail-connector.example.com\"\n\ningressWebhook:\n  enabled: true\n  className: nginx\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt\n  hosts:\n    - mail-connector.example.com\n  tls:\n    - secretName: mail-connector-tls\n      hosts:\n        - mail-connector.example.com\n</code></pre> <ul> <li><code>DSXCONNECTOR_CONNECTOR_URL</code> stays internal so dsx-connect talks to the ClusterIP service.</li> <li><code>webhook.publicUrl</code> tells the connector which HTTPS base to register with Graph.</li> <li>The ingress exposes <code>/m365-mail-connector/webhook/event</code> publicly; terminate TLS here and reuse the same hostname in <code>webhook.publicUrl</code>.</li> </ul>"},{"location":"deployment/kubernetes/m365-mail/#chart-behaviour","title":"Chart Behaviour","text":"<ul> <li>If <code>webhook.publicUrl</code> is set, the chart injects <code>DSXCONNECTOR_WEBHOOK_URL</code> for you; otherwise the connector falls back to <code>DSXCONNECTOR_CONNECTOR_URL</code>.</li> <li>The provided ingress templates only expose the webhook path. Add a second ingress or service if you want other routes reachable outside the cluster.</li> <li>Remember to create (or reuse) the enrollment-token Secret referenced under <code>auth_dsxconnect.enrollmentSecretName</code>; dsx-connect and the connector must share the same token when <code>auth_dsxconnect.enabled=true</code>.</li> <li>For near real-time scanning, you can add <code>DSXCONNECTOR_TRIGGER_DELTA_ON_NOTIFICATION=true</code> (or lower <code>DSXCONNECTOR_DELTA_RUN_INTERVAL_SECONDS</code>). The connector will still run periodic delta passes as a safety net.</li> </ul>"},{"location":"deployment/kubernetes/m365-mail/#next-steps","title":"Next Steps","text":"<ol> <li>Apply the values file alongside any Secrets:    <pre><code>helm upgrade --install m365-mail connectors/m365_mail/deploy/helm \\\n  -f values.yaml\n</code></pre></li> <li>Confirm the pod starts and the ingress hostname resolves over HTTPS.</li> <li>Check logs for <code>Subscriptions reconciled</code> and <code>Delta runner</code> entries to ensure Graph notifications are flowing.</li> </ol> <p>For more on Graph credentials and permission setup, see the Reference \u2192 Azure Credentials page.</p>"},{"location":"deployment/kubernetes/onedrive/","title":"OneDrive Connector \u2014 Helm Deployment","text":"<p>Deploy the <code>onedrive-connector-chart</code> (under <code>connectors/onedrive/deploy/helm</code>) to scan OneDrive content via delegated app credentials.</p>"},{"location":"deployment/kubernetes/onedrive/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.19+, Helm 3.2+, and <code>kubectl</code>.</li> <li>Access to <code>oci://registry-1.docker.io/dsxconnect/onedrive-connector-chart</code>.</li> </ul>"},{"location":"deployment/kubernetes/onedrive/#preflight-tasks","title":"Preflight Tasks","text":"<p>Create the OneDrive/Graph credentials Secret (template: <code>connectors/onedrive/deploy/helm/od-secret.yaml</code>):</p> <pre><code>kubectl create secret generic onedrive-credentials \\\n  --from-literal=DSXCONNECTOR_ONEDRIVE_TENANT_ID=&lt;tenant&gt; \\\n  --from-literal=DSXCONNECTOR_ONEDRIVE_CLIENT_ID=&lt;client-id&gt; \\\n  --from-literal=DSXCONNECTOR_ONEDRIVE_CLIENT_SECRET=&lt;client-secret&gt;\n</code></pre> <p>Ensure dsx-connect is reachable (same cluster or routable ingress). If you plan to receive Microsoft Graph webhooks, make sure <code>/onedrive-connector/webhook/event</code> can be exposed through an ingress and reachable by Graph.</p>"},{"location":"deployment/kubernetes/onedrive/#configuration","title":"Configuration","text":""},{"location":"deployment/kubernetes/onedrive/#required-settings","title":"Required settings","text":"<ul> <li><code>env.DSXCONNECTOR_ONEDRIVE_TENANT_ID</code>, <code>env.DSXCONNECTOR_ONEDRIVE_CLIENT_ID</code>, <code>env.DSXCONNECTOR_ONEDRIVE_CLIENT_SECRET</code>: provided via the Secret above or direct env overrides.</li> <li><code>env.DSXCONNECTOR_ONEDRIVE_USER_ID</code>: the user/drive to scan.</li> <li><code>env.DSXCONNECTOR_ASSET</code>: drive-relative path (e.g., <code>/Documents/dsx-connect</code>).</li> <li><code>env.DSXCONNECTOR_FILTER</code>: optional rsync-style include/exclude list (see Filter reference).</li> <li><code>env.DSXCONNECTOR_ONEDRIVE_WEBHOOK_ENABLED</code>, <code>env.DSXCONNECTOR_ONEDRIVE_WEBHOOK_URL</code>, <code>env.DSXCONNECTOR_ONEDRIVE_WEBHOOK_CLIENT_STATE</code>: set when using Graph webhooks.</li> <li><code>workers</code> / <code>replicaCount</code>: concurrency and HA knobs.</li> </ul>"},{"location":"deployment/kubernetes/onedrive/#dsx-connect-endpoint","title":"dsx-connect endpoint","text":"<p>Defaults to the in-cluster service; override with <code>env.DSXCONNECTOR_DSX_CONNECT_URL</code> if dsx-connect is exposed elsewhere.</p>"},{"location":"deployment/kubernetes/onedrive/#authentication-tls","title":"Authentication &amp; TLS","text":""},{"location":"deployment/kubernetes/onedrive/#webhook-ingress","title":"Webhook ingress","text":"<p>Enable <code>ingressWebhook</code> to expose <code>/onedrive-connector/webhook/event</code> when using Graph webhooks. Restrict ingress to Microsoft Graph IPs or your ingress controller as needed.</p>"},{"location":"deployment/kubernetes/onedrive/#deployment","title":"Deployment","text":""},{"location":"deployment/kubernetes/onedrive/#method-1-oci-chart-with-cli-overrides-fastest","title":"Method 1 \u2013 OCI chart with CLI overrides (fastest)","text":"<pre><code>helm install onedrive-dev oci://registry-1.docker.io/dsxconnect/onedrive-connector-chart \\\n  --version &lt;chart-version&gt; \\\n  --set-string env.DSXCONNECTOR_ONEDRIVE_USER_ID=\"user@contoso.com\" \\\n  --set-string env.DSXCONNECTOR_ASSET=\"/Documents/dsx-connect\" \\\n  --set-string env.DSXCONNECTOR_FILTER=\"\" \\\n  --set-string image.tag=&lt;connector-version&gt;\n</code></pre>"},{"location":"deployment/kubernetes/onedrive/#method-2-work-from-a-pulled-chart-edit-values-locally","title":"Method 2 \u2013 Work from a pulled chart (edit values locally)","text":"<pre><code>helm pull oci://registry-1.docker.io/dsxconnect/onedrive-connector-chart --version &lt;chart-version&gt;\ntar -xzf onedrive-connector-chart-&lt;chart-version&gt;.tgz\ncd onedrive-connector-chart\n</code></pre> <p>Example values file:</p> <pre><code>image:\n  tag: \"&lt;connector-version&gt;\"\nenv:\n  DSXCONNECTOR_ONEDRIVE_USER_ID: \"user@contoso.com\"\n  DSXCONNECTOR_ASSET: \"/Documents/dsx-connect\"\n  DSXCONNECTOR_FILTER: \"\"\n  DSXCONNECTOR_ONEDRIVE_WEBHOOK_ENABLED: \"true\"\n  DSXCONNECTOR_ONEDRIVE_WEBHOOK_URL: \"https://&lt;public-host&gt;/onedrive-connector/webhook/event\"\n  DSXCONNECTOR_ONEDRIVE_WEBHOOK_CLIENT_STATE: \"&lt;shared-secret&gt;\"\n</code></pre> <p>Install from the extracted chart root (<code>.</code>):</p> <pre><code>helm install onedrive-dev . -f values-dev.yaml\n</code></pre>"},{"location":"deployment/kubernetes/onedrive/#method-3-gitops-production-style","title":"Method 3 \u2013 GitOps / production style","text":"<pre><code>helm upgrade --install onedrive-prod oci://registry-1.docker.io/dsxconnect/onedrive-connector-chart \\\n  --version &lt;chart-version&gt; \\\n  -f values-prod.yaml\n</code></pre>"},{"location":"deployment/kubernetes/onedrive/#verification","title":"Verification","text":"<pre><code>helm list\nkubectl get pods\nkubectl logs deploy/onedrive-connector -f\n</code></pre>"},{"location":"deployment/kubernetes/onedrive/#assets-filters","title":"Assets &amp; filters","text":"<ul> <li><code>DSXCONNECTOR_ONEDRIVE_ASSET</code> points to the drive-relative folder. Navigate to the desired folder in OneDrive, copy the path, and paste it (for example <code>/Documents/dsx-connect/scantest</code>).</li> <li>Filters are relative to that asset path and follow rsync syntax (<code>?</code>, <code>*</code>, <code>**</code>, <code>+</code>, <code>-</code>).</li> </ul> <p>See <code>connectors/onedrive/deploy/helm/values.yaml</code> for the exhaustive option set.</p>"},{"location":"deployment/kubernetes/salesforce/","title":"Salesforce Connector \u2014 Helm Deployment","text":"<p>Use this guide to deploy <code>salesforce-connector-chart</code> (under <code>connectors/salesforce/deploy/helm/</code>) to Kubernetes.</p>"},{"location":"deployment/kubernetes/salesforce/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.19+, <code>kubectl</code>, and Helm 3.2+.</li> <li>Salesforce Connected App &amp; integration user (username/password OAuth flow).</li> <li>Access to the Helm chart (local checkout or OCI: <code>oci://registry-1.docker.io/dsxconnect/salesforce-connector-chart</code>).</li> <li>dsx-connect deployed and reachable from the connector namespace.</li> </ul>"},{"location":"deployment/kubernetes/salesforce/#preflight-tasks","title":"Preflight Tasks","text":"<p>Create a Secret containing the Salesforce credentials (replace placeholders before applying):</p> <pre><code># salesforce-credentials.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: salesforce-connector-credentials\ntype: Opaque\nstringData:\n  DSXCONNECTOR_SF_CLIENT_ID: \"&lt;consumer-key&gt;\"\n  DSXCONNECTOR_SF_CLIENT_SECRET: \"&lt;consumer-secret&gt;\"\n  DSXCONNECTOR_SF_USERNAME: \"dsx@customer.com\"\n  DSXCONNECTOR_SF_PASSWORD: \"&lt;password&gt;\"\n  DSXCONNECTOR_SF_SECURITY_TOKEN: \"&lt;optional-token&gt;\"\n</code></pre> <pre><code>kubectl apply -f salesforce-credentials.yaml\n</code></pre> <p>The chart can project this secret via <code>envSecretRefs</code>.</p>"},{"location":"deployment/kubernetes/salesforce/#configuration","title":"Configuration","text":"<p>The connector charts now share a common <code>values.yaml</code> structure so operators can reuse the same knobs across AWS/Azure/SharePoint/etc.  The major sections are:</p> <ul> <li><code>image</code>, <code>imagePullSecrets</code>, <code>nameOverride/fullnameOverride</code></li> <li><code>service</code>, <code>tls</code>, and optional <code>ingressWebhook</code>/<code>networkPolicy</code></li> <li><code>env</code> (human-friendly defaults) plus <code>envSecretRefs</code> for projecting Kubernetes Secrets</li> <li><code>auth_dsxconnect</code> (enrollment token + DSX-HMAC) and worker/replica scaling knobs</li> </ul> <p>Refer to <code>connectors/salesforce/deploy/helm/values.yaml</code> for inline comments on each block.</p> <p>Key <code>.Values</code>:</p> Value Description <code>env.DSXCONNECTOR_CONNECTOR_URL</code> Connector base URL (defaults to in-cluster service). <code>env.DSXCONNECTOR_DSX_CONNECT_URL</code> dsx-connect API URL. <code>env.DSXCONNECTOR_ASSET</code> Optional SOQL clause appended via <code>AND</code> (e.g., <code>ContentDocumentId = '069xx...'</code>). <code>env.DSXCONNECTOR_FILTER</code> Optional comma-separated extensions (<code>pdf,docx</code>). <code>env.DSXCONNECTOR_SF_LOGIN_URL</code> / <code>env.DSXCONNECTOR_SF_API_VERSION</code> Login host + REST API version. <code>env.DSXCONNECTOR_SF_WHERE</code>, <code>env.DSXCONNECTOR_SF_FIELDS</code>, <code>env.DSXCONNECTOR_SF_ORDER_BY</code>, <code>env.DSXCONNECTOR_SF_MAX_RECORDS</code> Tune the ContentVersion query/batch size. <code>envSecretRefs</code> List of Kubernetes Secret names projected via <code>envFrom</code> (use this for client ID/secret/username/password). <code>auth_dsxconnect.enabled</code> Enables DSX-HMAC verification on the connector\u2019s private endpoints. <code>auth_dsxconnect.enrollmentSecretName</code> / <code>.enrollmentKey</code> Secret &amp; key that provide <code>DSXCONNECT_ENROLLMENT_TOKEN</code> (should match dsx-connect)."},{"location":"deployment/kubernetes/salesforce/#example-values-file","title":"Example values file","text":"<pre><code>env:\n  DSXCONNECTOR_DISPLAY_NAME: \"Salesforce Connector\"\n  DSXCONNECTOR_SF_LOGIN_URL: \"https://login.salesforce.com\"\n  DSXCONNECTOR_SF_API_VERSION: \"v60.0\"\n  DSXCONNECTOR_SF_WHERE: \"IsLatest = true\"\n  DSXCONNECTOR_SF_MAX_RECORDS: \"500\"\n\nenvSecretRefs:\n  - salesforce-connector-credentials\n\nauth_dsxconnect:\n  enabled: true\n  enrollmentSecretName: dsx-connect-enrollment\n  enrollmentKey: ENROLLMENT_TOKEN\n</code></pre>"},{"location":"deployment/kubernetes/salesforce/#deployment-methods","title":"Deployment Methods","text":""},{"location":"deployment/kubernetes/salesforce/#oci-chart-with-cli-overrides","title":"OCI chart with CLI overrides","text":"<pre><code>helm install salesforce \\\n  oci://registry-1.docker.io/dsxconnect/salesforce-connector-chart \\\n  --version &lt;chart-version&gt; \\\n  --set-string env.DSXCONNECTOR_SF_LOGIN_URL=https://login.salesforce.com \\\n  --set envSecretRefs[0]=salesforce-connector-credentials \\\n  --set auth_dsxconnect.enabled=true \\\n  --set auth_dsxconnect.enrollmentSecretName=dsx-connect-enrollment\n</code></pre>"},{"location":"deployment/kubernetes/salesforce/#local-chart-edit-values","title":"Local chart (edit values)","text":"<pre><code>helm pull oci://registry-1.docker.io/dsxconnect/salesforce-connector-chart --version &lt;chart-version&gt;\ntar -xzf salesforce-connector-chart-&lt;chart-version&gt;.tgz\nhelm install salesforce ./salesforce-connector-chart -f values-salesforce.yaml\n</code></pre>"},{"location":"deployment/kubernetes/salesforce/#gitops-production","title":"GitOps / Production","text":"<p>Check your values file into Git (with secrets stored in Kubernetes or an external secret manager) and let Argo CD/Flux sync from the OCI chart:</p> <pre><code>helm upgrade --install salesforce-prod \\\n  oci://registry-1.docker.io/dsxconnect/salesforce-connector-chart \\\n  --version &lt;chart-version&gt; \\\n  -f values-prod.yaml\n</code></pre>"},{"location":"deployment/kubernetes/salesforce/#verification","title":"Verification","text":"<pre><code>helm list\nkubectl get pods\nkubectl logs deploy/salesforce-connector -f\n</code></pre> <ul> <li>The pod should reach <code>READY</code>.</li> <li>In the dsx-connect UI, the Salesforce connector card should show <code>READY</code>.</li> <li>Run a test full scan and confirm ContentVersions queue properly.</li> </ul>"},{"location":"deployment/kubernetes/salesforce/#secret-rotation-tls","title":"Secret Rotation &amp; TLS","text":"<ul> <li>Rotate Salesforce secrets by updating the Kubernetes Secret and restarting the connector deployment (<code>kubectl rollout restart deploy/salesforce-connector</code>).</li> <li>To serve the connector over HTTPS, set <code>env.DSXCONNECTOR_USE_TLS=true</code> and provide TLS cert/key via extra secrets or volumes.</li> <li>For dsx-connect auth, keep enrollment tokens short-lived and rotate DSX-HMAC credentials by re-registering the connector.</li> </ul>"},{"location":"deployment/kubernetes/sharepoint/","title":"SharePoint Connector \u2014 Helm Deployment","text":"<p>Deploy the <code>sharepoint-connector-chart</code> (under <code>connectors/sharepoint/deploy/helm</code>) to scan SharePoint Online document libraries.</p>"},{"location":"deployment/kubernetes/sharepoint/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.19+ cluster and <code>kubectl</code>.</li> <li>Helm 3.2+.</li> <li>Access to <code>oci://registry-1.docker.io/dsxconnect/sharepoint-connector-chart</code>.</li> </ul>"},{"location":"deployment/kubernetes/sharepoint/#preflight-tasks","title":"Preflight Tasks","text":"<p>Create a Secret containing the Microsoft Entra (Azure AD) app credentials:</p> <pre><code>kubectl create secret generic sharepoint-credentials \\\n  --from-literal=DSXCONNECTOR_SP_TENANT_ID=&lt;tenant-id&gt; \\\n  --from-literal=DSXCONNECTOR_SP_CLIENT_ID=&lt;client-id&gt; \\\n  --from-literal=DSXCONNECTOR_SP_CLIENT_SECRET=&lt;client-secret&gt;\n</code></pre> <p>(<code>connectors/sharepoint/deploy/helm/sp-secret.yaml</code> provides a template if you prefer editing a manifest.)</p>"},{"location":"deployment/kubernetes/sharepoint/#configuration","title":"Configuration","text":""},{"location":"deployment/kubernetes/sharepoint/#required-settings","title":"Required settings","text":"<ul> <li><code>env.DSXCONNECTOR_ASSET</code>: full SharePoint library URL (e.g., <code>https://contoso.sharepoint.com/sites/Site/Shared%20Documents/dsx-connect</code>).</li> <li><code>env.DSXCONNECTOR_FILTER</code>: rsync-style include/exclude paths relative to the asset root (see Filter reference).</li> <li><code>env.DSXCONNECTOR_DISPLAY_NAME</code>: optional UI label.</li> <li><code>env.DSXCONNECTOR_ITEM_ACTION</code> / <code>env.DSXCONNECTOR_ITEM_ACTION_MOVE_METAINFO</code>: remediation behavior.</li> <li><code>workers</code> / <code>replicaCount</code>: concurrency and HA knobs.</li> </ul>"},{"location":"deployment/kubernetes/sharepoint/#dsx-connect-endpoint","title":"dsx-connect endpoint","text":"<p>Defaults to the in-cluster dsx-connect service. Override via <code>env.DSXCONNECTOR_DSX_CONNECT_URL</code> if dsx-connect is exposed through another hostname.</p>"},{"location":"deployment/kubernetes/sharepoint/#authentication-tls","title":"Authentication &amp; TLS","text":""},{"location":"deployment/kubernetes/sharepoint/#deployment","title":"Deployment","text":""},{"location":"deployment/kubernetes/sharepoint/#method-1-oci-chart-with-cli-overrides-fastest","title":"Method 1 \u2013 OCI chart with CLI overrides (fastest)","text":"<pre><code>helm install sp-docs-dev oci://registry-1.docker.io/dsxconnect/sharepoint-connector-chart \\\n  --version &lt;chart-version&gt; \\\n  --set env.DSXCONNECTOR_ASSET=\"https://&lt;host&gt;/sites/&lt;SiteName&gt;/Shared%20Documents\" \\\n  --set-string env.DSXCONNECTOR_FILTER=\"\" \\\n  --set-string image.tag=&lt;connector-version&gt;\n</code></pre>"},{"location":"deployment/kubernetes/sharepoint/#method-2-work-from-a-pulled-chart-edit-values-locally","title":"Method 2 \u2013 Work from a pulled chart (edit values locally)","text":"<pre><code>helm pull oci://registry-1.docker.io/dsxconnect/sharepoint-connector-chart --version &lt;chart-version&gt;\ntar -xzf sharepoint-connector-chart-&lt;chart-version&gt;.tgz\ncd sharepoint-connector-chart\n</code></pre> <p>Create <code>values-dev.yaml</code>:</p> <pre><code>env:\n  DSXCONNECTOR_ASSET: \"https://contoso.sharepoint.com/sites/Site/Shared%20Documents\"\n  DSXCONNECTOR_FILTER: \"reports/**\"\nimage:\n  tag: \"&lt;connector-version&gt;\"\n</code></pre> <p>Install from the extracted chart root (<code>.</code>):</p> <pre><code>helm install sp-docs-dev . -f values-dev.yaml\n</code></pre>"},{"location":"deployment/kubernetes/sharepoint/#method-3-gitops-production-style","title":"Method 3 \u2013 GitOps / production style","text":"<pre><code>helm upgrade --install sp-prod oci://registry-1.docker.io/dsxconnect/sharepoint-connector-chart \\\n  --version &lt;chart-version&gt; \\\n  -f values-prod.yaml\n</code></pre>"},{"location":"deployment/kubernetes/sharepoint/#verification","title":"Verification","text":"<pre><code>helm list\nkubectl get pods\nkubectl logs deploy/sharepoint-connector -f\n</code></pre>"},{"location":"deployment/kubernetes/sharepoint/#scaling-guidance","title":"Scaling guidance","text":"<ul> <li>Increase <code>workers</code> for additional in-pod concurrency.</li> <li>Increase <code>replicaCount</code> for HA / throughput. Each replica registers independently with dsx-connect; replicas do not shard a single full scan.</li> </ul> <p>See <code>connectors/sharepoint/deploy/helm/values.yaml</code> for the full configuration surface.</p>"},{"location":"operations/kv-state/","title":"Connector State KV (dsx\u2011connect)","text":"<p>Connectors are intended to be stateless, as it promotes simple, \"do-your-job\" connectors, that focus on a few single tasks, and not complex interactions, fault tolerance, redundancy, etc.... For example, if a connector crashes, simply restart and the dsx-connect core will pick up where things left off.</p> <p>However, there are cases where connectors need to utilize state, such retaining delta tokens or cursors that tracks Microsoft Graph changes (see Sharepoint/One Drive connectors). In this case, where a saved state is needed for resuming work after connector restarts or redeployments (e.g., persisting the latest Graph delta cursor so the next run can pick up from the correct change token), connectors should use dsx\u2011connect APIs to store small, key/values. </p>"},{"location":"operations/kv-state/#endpoints","title":"Endpoints","text":"<ul> <li>PUT <code>/dsx-connect/api/v1/connectors/state/{connector_uuid}/{ns}/{key}</code></li> <li>Auth: DSX\u2011HMAC (connector \u2192 dsx\u2011connect)</li> <li>Body: raw string (e.g., delta token)</li> <li>Stores under Redis key: <code>dsxconnect:connector_state:{uuid}:{ns}:{key}</code></li> <li> <p>Response: 204 No Content on success</p> </li> <li> <p>GET <code>/dsx-connect/api/v1/connectors/state/{connector_uuid}/{ns}/{key}</code></p> </li> <li>Auth: DSX\u2011HMAC (connector \u2192 dsx\u2011connect)</li> <li>Response: <code>{ \"value\": \"...\" }</code> (string or empty if missing)</li> </ul>"},{"location":"operations/kv-state/#notes","title":"Notes","text":"<ul> <li>Designed for small values (a few KB). Not a general database.</li> <li>HMAC protects read/write; policy is per\u2011connector (by UUID).</li> <li>Keys are namespaced by <code>{ns}</code> to avoid collisions across features.</li> </ul>"},{"location":"operations/kv-state/#mermaid-state-kv-flow","title":"Mermaid: State KV Flow","text":"<pre><code>sequenceDiagram\n    participant Conn as Connector\n    participant DSX as dsx\u2011connect\n    participant R as Redis (KV)\n\n    Note over Conn, DSX: Store a delta token\n    Conn-&gt;&gt;+DSX: PUT /connectors/state/{uuid}/m365/delta:user@contoso\n    DSX-&gt;&gt;R: SET dsxconnect:connector_state:{uuid}:m365:delta:user@contoso\n    DSX--&gt;&gt;-Conn: 204 No Content\n\n    Note over Conn, DSX: Read the delta token\n    Conn-&gt;&gt;+DSX: GET /connectors/state/{uuid}/m365/delta:user@contoso\n    DSX-&gt;&gt;R: GET dsxconnect:connector_state:{uuid}:m365:delta:user@contoso\n    R--&gt;&gt;DSX: \"deltaLink-token\"\n    DSX--&gt;&gt;-Conn: { \"value\": \"deltaLink-token\" }</code></pre>"},{"location":"operations/kv-state/#minimal-curl-examples","title":"Minimal Curl Examples","text":"<ul> <li> <p>Store: <pre><code>curl -s -X PUT \\\n  -H \"Authorization: DSX-HMAC key_id=&lt;kid&gt;, ts=&lt;ts&gt;, nonce=&lt;nonce&gt;, sig=&lt;sig&gt;\" \\\n  --data 'deltaLink-token' \\\n  http://dsx-connect-api/dsx-connect/api/v1/connectors/state/&lt;uuid&gt;/m365/delta:user@contoso\n</code></pre></p> </li> <li> <p>Fetch: <pre><code>curl -s \\\n  -H \"Authorization: DSX-HMAC key_id=&lt;kid&gt;, ts=&lt;ts&gt;, nonce=&lt;nonce&gt;, sig=&lt;sig&gt;\" \\\n  http://dsx-connect-api/dsx-connect/api/v1/connectors/state/&lt;uuid&gt;/m365/delta:user@contoso\n</code></pre></p> </li> </ul>"},{"location":"reference/assets-and-filters/","title":"Assets &amp; Filters (Sharding and Scoping)","text":"<p>This page explains how to choose and combine <code>DSXCONNECTOR_ASSET</code> (the exact scan root) and <code>DSXCONNECTOR_FILTER</code> (rsync\u2011like scoping) across connectors. It also covers sharding large repositories for parallel scans.</p>"},{"location":"reference/assets-and-filters/#assets-dsxconnector_asset","title":"Assets (DSXCONNECTOR_ASSET)","text":"<p><code>DSXCONNECTOR_ASSET</code> defines the exact root of a scan \u2014 no wildcards.</p> <ul> <li>Filesystem: a mounted path (e.g., <code>/app/scan_folder</code>)</li> <li>AWS S3: <code>bucket</code> or <code>bucket/prefix</code></li> <li>Azure Blob: <code>container</code> or <code>container/prefix</code></li> <li>GCS: <code>bucket</code> or <code>bucket/prefix</code></li> <li>SharePoint: a site/doc library/folder URL (scope)</li> </ul> <p>Use filters to further include/exclude items under the asset root (see below). Filters are evaluated relative to the asset and run inside the connector; the provider still lists everything under the asset root, and the connector drops items client-side. Most repositories (S3, Blob, GCS, filesystem, SharePoint) only support narrowing via \u201cprefix/scope\u201d (<code>asset</code>); there is no native include/exclude filtering on list APIs, so the connector must walk every object under the asset during a full_scan. For example, Azure Blob only exposes <code>container/optional-prefix</code> list APIs \u2014 it cannot answer \u201clist only PDFs or skip tmp/\u201d. That filtering happens locally in the connector after Azure returns every blob in the asset scope.</p>"},{"location":"reference/assets-and-filters/#filters-dsxconnector_filter","title":"Filters (DSXCONNECTOR_FILTER)","text":"<p>Rsync\u2011like include/exclude rules evaluated under the asset root. See Filters (Details) for the full reference.</p>"},{"location":"reference/assets-and-filters/#asset-vs-filter","title":"Asset vs Filter","text":"<ul> <li>Asset: exact scan root; provider can often narrow list operations to <code>name_starts_with</code> that root/prefix.</li> <li>Filter: expressive rsync\u2011style rules under the asset; wildcard selection and excludes.</li> </ul>"},{"location":"reference/assets-and-filters/#equivalences","title":"Equivalences","text":"<ul> <li><code>asset=my-container</code>, <code>filter=prefix1/**</code>  \u2248  <code>asset=my-container/prefix1</code>, <code>filter=\"\"</code></li> <li><code>asset=my-container</code>, <code>filter=sub1</code>       \u2248  <code>asset=my-container/sub1</code>, <code>filter=\"\"</code></li> <li><code>asset=my-container</code>, <code>filter=sub1/*</code>     \u2248  <code>asset=my-container/sub1</code>, <code>filter=\"*\"</code></li> </ul>"},{"location":"reference/assets-and-filters/#guidance","title":"Guidance","text":"<ul> <li>Prefer Asset for the stable, exact root of a scan (fast provider narrowing; simpler mental model). When the asset is narrow, the provider only returns the relevant subset and the connector spends less time listing. This matters because listing (full_scan) is typically the most expensive, inherently serial connector operation.</li> <li>Use Filter for wildcard selection and excludes under that root, but remember the connector still has to fetch all objects beneath the asset and then evaluate filters locally.</li> <li>Complex filters (e.g., excludes like <code>-tmp</code>) can force broader provider listings with lots of client-side filtering. Whenever possible, push coarse boundaries into the asset (e.g., <code>asset=my-bucket/prefix1</code>) and keep filters for light-touch adjustments.</li> </ul>"},{"location":"reference/assets-and-filters/#sharding-deployment-strategies","title":"Sharding &amp; Deployment Strategies","text":"<p>Use multiple assets or include\u2011only filters to split a large repository into smaller partitions that can be scanned in parallel by multiple connector instances.</p> <ul> <li>Asset\u2011based sharding (preferred for coarse partitions):</li> <li>S3: <code>my-bucket/A</code>, <code>my-bucket/B</code>, \u2026 (alphabetic)</li> <li>S3: <code>my-bucket/2025-01</code>, <code>my-bucket/2025-02</code>, \u2026 (time)</li> <li>Filesystem: <code>/app/scan_folder/shard1</code>, <code>/app/scan_folder/shard2</code></li> <li>SharePoint: distinct doc libraries/folders</li> <li>Filter\u2011based sharding (include\u2011only filters):</li> <li>Asset at container/bucket root, with partitions via include\u2011only filters (e.g., <code>prefix1/sub1/**</code>, <code>prefix1/sub2/**</code>)</li> </ul> <p>Compose POV: run multiple connector containers, each with a distinct asset partition or include\u2011only filter. In private K8S, deploy multiple releases with different values.</p>"},{"location":"reference/assets-and-filters/#see-also","title":"See Also","text":"<ul> <li>Filters (Details)</li> </ul>"},{"location":"reference/assets/","title":"Assets (DSXCONNECTOR_ASSET) and Sharding","text":"<p><code>DSXCONNECTOR_ASSET</code> defines the logical root of a connector\u2019s scan scope. It allows you to partition a large repository into smaller \u201cshards\u201d so that multiple connector instances can scan in parallel (one asset partition per instance).</p>"},{"location":"reference/assets/#what-is-an-asset","title":"What is an Asset?","text":"<ul> <li>Filesystem: a mounted path (e.g., <code>/app/scan_folder</code>)</li> <li>AWS S3: <code>bucket</code> or <code>bucket/prefix</code></li> <li>Azure Blob: <code>container</code> or <code>container/prefix</code></li> <li>GCS: <code>bucket</code> or <code>bucket/prefix</code></li> <li>SharePoint: a site/doc library/folder URL (scope)</li> </ul> <p>Within the asset, use <code>DSXCONNECTOR_FILTER</code> to further include/exclude items. Filters are evaluated relative to the asset root.</p>"},{"location":"reference/assets/#when-to-use-assets-sharding","title":"When to Use Assets (Sharding)","text":"<ul> <li>Large repositories (many millions/billions of items)</li> <li>Need to parallelize scans across multiple connector instances</li> <li>Need to bound enumerations for fault isolation (one partition failing doesn\u2019t block others)</li> </ul>"},{"location":"reference/assets/#sharding-examples","title":"Sharding Examples","text":"<ul> <li>S3: run multiple connectors, each with a distinct <code>DSXCONNECTOR_ASSET</code> such as:</li> <li><code>my-bucket/A/*</code>, <code>my-bucket/B/*</code>, \u2026 (alphabetic partitions)</li> <li><code>my-bucket/2025-01/*</code>, <code>my-bucket/2025-02/*</code>, \u2026 (time partitions)</li> <li>Filesystem: split by subfolders: <code>/app/scan_folder/shard1</code>, <code>/app/scan_folder/shard2</code></li> <li>SharePoint: split by doc library/folder</li> </ul> <p>Compose POV: scale out by starting multiple connector containers (each pointing at a distinct asset partition). In K8S (private), you\u2019d deploy multiple releases or replicas with distinct values.</p>"},{"location":"reference/assets/#filters-vs-assets-pros-cons","title":"Filters vs Assets \u2014 Pros &amp; Cons","text":"<ul> <li>Assets (partitioning at source):</li> <li>Pros: enables parallel enumeration; reduces per\u2011connector list volume; isolates failures per shard</li> <li>Cons: requires coordination of partitioning (naming/scope decisions)</li> <li>Filters (evaluation at connector):</li> <li>Pros: simple per\u2011connector scoping without changing infrastructure; expressive (rsync\u2011like)</li> <li>Cons: filters are applied after listing within the asset; for very large repos, exhaustive filters can still incur heavy list operations</li> </ul>"},{"location":"reference/assets/#practical-guidance","title":"Practical Guidance","text":"<ul> <li>Prefer Assets for coarse partitioning (sharding) and scalability</li> <li>Use Filters for fine\u2011grained scoping inside an asset</li> <li>Combine both: shard by asset; refine within each partition via filters</li> </ul>"},{"location":"reference/assets/#see-also","title":"See Also","text":"<ul> <li>Filters reference: Filters</li> </ul>"},{"location":"reference/azure-credentials/","title":"Azure Credentials for DSX Connectors","text":"<p>The following guidance is for adding a connector as an app in Azure, retrieving credentials, and assigning permissions. The workflow is identical for the M365 Mail (Outlook), SharePoint, and OneDrive connectors; only the Microsoft Graph permissions differ.</p> <p>Guidance current as of 2025-10-27. Azure portal labels occasionally change; adjust as needed if the UI differs.</p>"},{"location":"reference/azure-credentials/#portal-workflow","title":"Portal Workflow","text":"<ol> <li> <p>Sign in to Azure    Browse to https://portal.azure.com and sign in with an account that can register applications (Application Administrator or Global Administrator).</p> </li> <li> <p>Locate Tenant ID    In the header, ensure you are in the correct directory. Copy the \u201cTenant ID\u201d shown under Directory + subscription. This becomes <code>M365_TENANT_ID</code> (or the equivalent setting for other connectors).</p> </li> <li> <p>Register the Application </p> </li> <li>Go to Microsoft Entra ID \u2192 App registrations \u2192 New registration.  </li> <li>Name the app (e.g., <code>dsx-m365-mail-connector</code>).  </li> <li>Leave the default single-tenant option.  </li> <li>Redirect URIs are optional for client credentials.  </li> <li> <p>Click Register.    On the Overview blade, copy the Application (client) ID (<code>M365_CLIENT_ID</code>).</p> </li> <li> <p>Create a Client Secret </p> </li> <li>Navigate to Certificates &amp; secrets.  </li> <li> <p>Add a new client secret, note the expiration, and copy the Value immediately (<code>M365_CLIENT_SECRET</code>). You cannot retrieve it later.</p> </li> <li> <p>Assign Microsoft Graph Application Permissions </p> </li> <li>Open API permissions \u2192 Add a permission \u2192 Microsoft Graph \u2192 Application.  </li> <li>Select the permissions your connector needs (see Required Microsoft Graph permissions).  </li> <li> <p>After adding them, click Grant admin consent and approve.</p> </li> <li> <p>Optional Adjustments </p> </li> <li>Configure redirect URIs only if you plan to use interactive flows.  </li> <li>Use certificates instead of secrets if your security policy requires it.</li> </ol> <p>Record the tenant ID, client ID, and secret securely (Azure Key Vault, Kubernetes Secret, etc.) before proceeding with connector deployment.</p>"},{"location":"reference/azure-credentials/#required-microsoft-graph-permissions","title":"Required Microsoft Graph permissions","text":""},{"location":"reference/azure-credentials/#m365-mail-connector-outlook-exchange-online","title":"M365 Mail connector (Outlook / Exchange Online)","text":"<p>Grant these application permissions when running the Outlook/M365 Mail connector:</p> Permission Purpose <code>Mail.Read</code> Inspect message metadata, check for attachments, and run delta queries. <code>Mail.ReadWrite</code> Remove attachments, tag subjects, and move/quarantine malicious messages. <code>Files.Read.All</code> Required by Graph when downloading attachments exposed as drive items. <p>If you disable remediation actions, you may omit <code>Mail.ReadWrite</code>, but keep it if you expect the connector to modify messages.</p>"},{"location":"reference/azure-credentials/#sharepoint-connector","title":"SharePoint connector","text":"<p>Grant these application permissions when running the SharePoint connector:</p> Permission Purpose <code>Sites.Read.All</code> Discover SharePoint sites, enumerate drives, and create change-notification subscriptions. <code>Files.Read.All</code> Enumerate files and stream content to dsx-connect for scanning. <code>Files.ReadWrite.All</code> Needed when MOVE or DELETE item actions are enabled. <p>If you use the connector strictly for read-only scanning, you can omit <code>Files.ReadWrite.All</code>, but leave it enabled if remediation actions are planned.</p>"},{"location":"reference/azure-credentials/#onedrive-connector","title":"OneDrive connector","text":"<p>Grant these application permissions when running the OneDrive connector:</p> Permission Purpose <code>Files.Read.All</code> Enumerate files and stream content from users' OneDrive accounts. <code>Files.ReadWrite.All</code> Required if you plan to delete or move items after verdicts. <p>If your deployment is read-only, you can omit <code>Files.ReadWrite.All</code>, but keep it when item actions are enabled.</p>"},{"location":"reference/azure-credentials/#automation-with-azure-cli","title":"Automation with Azure CLI","text":"<p>The Azure CLI can create the application and add permissions programmatically. Choose the permission set that matches your connector:</p> <ul> <li>M365 Mail: <code>Mail.Read</code>, <code>Mail.ReadWrite</code>, <code>Files.Read.All</code></li> <li>SharePoint: <code>Sites.Read.All</code>, <code>Files.Read.All</code>, <code>Files.ReadWrite.All</code></li> <li>OneDrive: <code>Files.Read.All</code>, <code>Files.ReadWrite.All</code></li> </ul> <pre><code>TENANT_ID=$(az account show --query tenantId -o tsv)\n# Pick a descriptive name per connector, e.g., dsx-m365-mail-connector or dsx-sharepoint-connector\n# Pick an app name per connector, e.g., dsx-m365-mail-connector, dsx-sharepoint-connector, dsx-onedrive-connector\nAPP_NAME=\"dsx-m365-mail-connector\"\nAPP=$(az ad app create --display-name \"$APP_NAME\" --query \"{appId:appId,objectId:id}\" -o json)\nCLIENT_ID=$(echo \"$APP\" | jq -r .appId)\nSECRET=$(az ad app credential reset --id \"$CLIENT_ID\" --display-name dsx-connector --years 2 \\\n         --query \"{secret:password}\" -o json | jq -r .secret)\n</code></pre>"},{"location":"reference/azure-credentials/#assign-permissions-for-m365-mail","title":"Assign permissions for M365 Mail","text":"<pre><code>GRAPH_APP_ID=\"00000003-0000-0000-c000-000000000000\"\nMAIL_READ_ID=$(az ad sp show --id \"$GRAPH_APP_ID\" \\\n  --query \"appRoles[?value=='Mail.Read' &amp;&amp; contains(allowedMemberTypes, 'Application')].id\" -o tsv)\nMAIL_READWRITE_ID=$(az ad sp show --id \"$GRAPH_APP_ID\" \\\n  --query \"appRoles[?value=='Mail.ReadWrite' &amp;&amp; contains(allowedMemberTypes, 'Application')].id\" -o tsv)\nFILES_READ_ALL_ID=$(az ad sp show --id \"$GRAPH_APP_ID\" \\\n  --query \"appRoles[?value=='Files.Read.All' &amp;&amp; contains(allowedMemberTypes, 'Application')].id\" -o tsv)\n\nROLE_IDS=(\"$MAIL_READ_ID\" \"$MAIL_READWRITE_ID\" \"$FILES_READ_ALL_ID\")\nPERMISSIONS=()\nfor ROLE_ID in \"${ROLE_IDS[@]}\"; do\n  [ -n \"$ROLE_ID\" ] &amp;&amp; PERMISSIONS+=(\"${ROLE_ID}=Role\")\ndone\n\nif [ ${#PERMISSIONS[@]} -gt 0 ]; then\n  az ad app permission add --id \"$CLIENT_ID\" --api \"$GRAPH_APP_ID\" \\\n    --api-permissions \"${PERMISSIONS[@]}\"\nfi\n\naz ad sp create --id \"$CLIENT_ID\"\naz ad app permission admin-consent --id \"$CLIENT_ID\"\n\necho \"Tenant ID: $TENANT_ID\"\necho \"Client ID: $CLIENT_ID\"\necho \"Client Secret: $SECRET\"\n</code></pre>"},{"location":"reference/azure-credentials/#assign-permissions-for-sharepoint","title":"Assign permissions for SharePoint","text":"<p>If you create a separate application for the SharePoint connector, repeat the <code>APP_NAME</code>/creation step with a new value (for example <code>dsx-sharepoint-connector</code>). Otherwise, reuse the same <code>CLIENT_ID</code>.</p> <pre><code>GRAPH_APP_ID=\"00000003-0000-0000-c000-000000000000\"\nSITES_READ_ALL_ID=$(az ad sp show --id \"$GRAPH_APP_ID\" \\\n  --query \"appRoles[?value=='Sites.Read.All' &amp;&amp; contains(allowedMemberTypes, 'Application')].id\" -o tsv)\nFILES_READ_ALL_ID=$(az ad sp show --id \"$GRAPH_APP_ID\" \\\n  --query \"appRoles[?value=='Files.Read.All' &amp;&amp; contains(allowedMemberTypes, 'Application')].id\" -o tsv)\nFILES_READWRITE_ALL_ID=$(az ad sp show --id \"$GRAPH_APP_ID\" \\\n  --query \"appRoles[?value=='Files.ReadWrite.All' &amp;&amp; contains(allowedMemberTypes, 'Application')].id\" -o tsv)\n\nROLE_IDS=(\"$SITES_READ_ALL_ID\" \"$FILES_READ_ALL_ID\" \"$FILES_READWRITE_ALL_ID\")\nPERMISSIONS=()\nfor ROLE_ID in \"${ROLE_IDS[@]}\"; do\n  [ -n \"$ROLE_ID\" ] &amp;&amp; PERMISSIONS+=(\"${ROLE_ID}=Role\")\ndone\n\nif [ ${#PERMISSIONS[@]} -gt 0 ]; then\n  az ad app permission add --id \"$CLIENT_ID\" --api \"$GRAPH_APP_ID\" \\\n    --api-permissions \"${PERMISSIONS[@]}\"\nfi\n\naz ad sp create --id \"$CLIENT_ID\"\naz ad app permission admin-consent --id \"$CLIENT_ID\"\n\necho \"Tenant ID: $TENANT_ID\"\necho \"Client ID: $CLIENT_ID\"\n</code></pre> <ul> <li><code>az ad app permission grant</code> is designed for delegated scopes and requires <code>--scope</code>; for application permissions you can skip it.  </li> <li>If <code>az ad app permission admin-consent</code> fails with <code>Consent validation failed</code>, use either the Azure portal or the Graph API approach described below.  </li> <li>Secrets created with <code>az ad app credential reset</code> expire automatically; adjust <code>--years</code> as needed and rotate before expiry.</li> </ul>"},{"location":"reference/azure-credentials/#troubleshooting-admin-consent","title":"Troubleshooting Admin Consent","text":"<p>Some tenants block the legacy Azure AD Graph endpoint that the CLI uses for admin consent. Two reliable alternatives:</p> <ol> <li>Azure Portal \u2013 Go to App registrations \u2192 API permissions \u2192 Grant admin consent and approve the dialog.  </li> <li>Microsoft Graph API \u2013 Assign each app role directly:</li> </ol> <p>Reuse the <code>ROLE_IDS</code> array from the snippet above (ensure it includes every role you granted).</p> <pre><code>APP_SP=$(az ad sp show --id \"$CLIENT_ID\" --query id -o tsv)\nGRAPH_SP=$(az ad sp show --id \"$GRAPH_APP_ID\" --query id -o tsv)\n\nfor ROLE_ID in \"${ROLE_IDS[@]}\"; do\n  [ -n \"$ROLE_ID\" ] || continue\n  az rest --method POST \\\n    --uri \"https://graph.microsoft.com/v1.0/servicePrincipals/$APP_SP/appRoleAssignments\" \\\n    --body \"{\\\"principalId\\\":\\\"$APP_SP\\\",\\\"resourceId\\\":\\\"$GRAPH_SP\\\",\\\"appRoleId\\\":\\\"$ROLE_ID\\\"}\"\ndone\n</code></pre> <p>Replace or extend the <code>ROLE_ID</code> list to match the permissions you added earlier.</p> <p>Once the assignments succeed, verify with:</p> <pre><code>az ad app permission list --id \"$CLIENT_ID\" --show-resource-name --query \"[].{resource:resourceDisplayName,scope:resourceAccess[].id}\"\n</code></pre> <p>The permissions should show <code>isEnabled: true</code>. You can now supply the tenant ID, client ID, and client secret to the DSX connector (M365 Mail or SharePoint) that authenticates against Microsoft Graph.</p>"},{"location":"reference/display-icons/","title":"Customizing Connector Display Icons","text":"<p>Several connectors support a <code>DSXCONNECTOR_DISPLAY_ICON</code> setting that lets you surface a custom icon on the dsx-connect UI card. The value must be a data URI containing an SVG that passes the frontend sanitizer (<code>&lt;svg&gt;</code> only \u2014 no scripts or external references).</p>"},{"location":"reference/display-icons/#1-prepare-your-svg","title":"1. Prepare your SVG","text":"<ul> <li>Keep it simple (single <code>&lt;svg&gt;</code> element, no embedded scripts).</li> <li>For consistency with other cards, use a 48\u00d748 view box.</li> </ul> <p>Example SVG:</p> <pre><code>&lt;svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 48 48\"&gt;\n  &lt;linearGradient id=\"onedriveGradient\" x1=\"4\" x2=\"44\" y1=\"24\" y2=\"24\"&gt;\n    &lt;stop offset=\"0\" stop-color=\"#1570d9\" /&gt;\n    &lt;stop offset=\"1\" stop-color=\"#1b4bb8\" /&gt;\n  &lt;/linearGradient&gt;\n  &lt;path fill=\"url(#onedriveGradient)\" d=\"M17.2 20.3a12.4 12.4 0 0 1 22.5 4.3A9.8 9.8 0 0 1 44 43H16.7A12.7 12.7 0 0 1 17.2 20.3Z\" /&gt;\n  &lt;path fill=\"#2d8cff\" d=\"M13.6 21.5a8.9 8.9 0 0 1 14.3-5.3A12.3 12.3 0 0 0 17.4 41H6.3a8.3 8.3 0 0 1 7.3-19.5Z\" /&gt;\n&lt;/svg&gt;\n</code></pre>"},{"location":"reference/display-icons/#2-percent-encode-the-svg","title":"2. Percent-encode the SVG","text":"<p><code>DSXCONNECTOR_DISPLAY_ICON</code> expects a percent-encoded SVG with the prefix <code>data:image/svg+xml;utf8,</code>.</p> <p>Ask ChatGPT to generate the data URI for you or use an online URL encoder and prepend <code>data:image/svg+xml;utf8,</code> to the result.</p> <p>Example output:</p> <pre><code>data:image/svg+xml;utf8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%2048%2048%22%3E...\n</code></pre>"},{"location":"reference/display-icons/#3-set-the-environment-variable","title":"3. Set the environment variable","text":"<p>In the connector\u2019s <code>.dev.env</code> or deployment values:</p> <pre><code>DSXCONNECTOR_DISPLAY_ICON=\"data:image/svg+xml;utf8,%3Csvg%20...%3E\"\n</code></pre> <p>Restart the connector and the UI card will render the custom icon, provided the SVG passes dsx-connect\u2019s sanitizer.</p>"},{"location":"reference/filters/","title":"Rsync\u2011Like Filter Rules","text":"<p>The <code>DSXCONNECTOR_FILTER</code> follows rsync include/exclude semantics for scoping scans under a connector\u2019s asset root.</p> <ul> <li><code>?</code> matches any single character except a slash (/)</li> <li><code>*</code> matches zero or more non\u2011slash characters</li> <li><code>**</code> matches zero or more characters, including slashes</li> <li><code>-</code> / <code>--exclude</code> exclude the following match</li> <li><code>+</code> / <code>--include</code> include the following match (everything else is implicitly included unless a later exclude removes it)</li> <li>Tokens can be comma\u2011separated or space\u2011separated; quote tokens with spaces</li> <li>Prefix <code>+</code>/<code>-</code> directly onto the pattern (no space) or use the long forms <code>--include pattern</code>, <code>--exclude pattern</code>.</li> <li>When mixing includes/excludes, add explicit <code>+</code> rules to keep intent clear (e.g., include only <code>/Finance/**</code> before dropping <code>tmp/</code>).</li> </ul> <p>Examples (paths are relative to <code>DSXCONNECTOR_ASSET</code>):</p> Filter Description \"\" All files recursively (no filter) \"*\" Only top\u2011level files (no recursion) \"prefix/**\" Everything under <code>prefix/</code> (common for \u201cprefix\u201d scoping) \"sub1\" Files within subtree <code>sub1</code> (recurse into subtrees) \"sub1/*\" Files directly under <code>sub1</code> (no recursion) \"sub1/sub2\" Files within subtree <code>sub1/sub2</code> (recurse) \".zip,.docx\" All files with .zip and .docx extensions \"-tmp --exclude cache\" Exclude <code>tmp</code> and <code>cache</code> directories \"sub1 -tmp --exclude sub2\" Include <code>sub1</code> subtree but exclude <code>tmp</code> and <code>sub2</code>. Same as <code>+sub1/** -tmp --exclude sub2</code> or <code>--include sub1 -tmp -sub2</code> \"'scan here' -'not here' --exclude 'not here either'\" Quoted tokens for names with spaces \"+Finance/, -\" Include only <code>Finance/</code> subtree and drop everything else. \"+/*.pdf, -tmp/\" Keep PDFs anywhere but drop anything under a <code>tmp/</code> folder. \"--include Finance/ --include Legal/ -**\" Two explicit include rules followed by a drop-everything-else exclude."},{"location":"reference/google-cloud-credentials/","title":"Google Cloud Credentials","text":"<p>Use these steps to configure the Google Cloud Storage connector with the necessary permissions for both object access and Pub/Sub monitoring.</p>"},{"location":"reference/google-cloud-credentials/#1-enable-apis","title":"1. Enable APIs","text":"<pre><code>gcloud services enable storage.googleapis.com pubsub.googleapis.com\n</code></pre>"},{"location":"reference/google-cloud-credentials/#2-service-account-and-roles","title":"2. Service account and roles","text":"<p>Connectors use a service account to access the bucket, and a Pub/Sub subscription to receive notifications.  To do this they will need a Service Account Key.  Two options are available:   - Option A: Create a new service account and key   - Option B: Use an existing service account and key</p> <p>The following steps will walk you through Option A, creating a new service account and key.  If you want to use an existing service account, then you simply need to add necessary roles to the service account, if they don't already exist. Create a service account and grant the necessary roles for on-access (monitoring) and on-demand (full scan).</p> <ul> <li>Full Scan on a bucket requires the <code>storage.objectView</code> role if just scanning files, and <code>storage.objectAdmin</code> if you need to move/delete files for remediation.</li> <li>Monitoring the bucket requires the <code>storage.objectViewer</code> role and the <code>pubsub.subscriber</code> role on the subscription.</li> </ul> <pre><code>PROJECT_ID=&lt;your-project-id&gt;\nSA_NAME=gcs-dsx-connector\nSUBSCRIPTION=gcs-events-dsx-connector\n\n# Create service account\ngcloud iam service-accounts create $SA_NAME \\\n  --display-name=\"GCS DSX-Connector\"\n\n# Bucket read access - needed for Pub/Sub monitoring and scanning\ngcloud projects add-iam-policy-binding \"$PROJECT_ID\" \\\n  --member=\"serviceAccount:${SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --role=roles/storage.objectViewer\n\n# Bucket write access if you need move/delete on a bucket\ngcloud projects add-iam-policy-binding \"$PROJECT_ID\" \\\n    --member=\"serviceAccount:${SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n    --role=roles/storage.objectAdmin\n</code></pre>"},{"location":"reference/google-cloud-credentials/#3-bucket-notifications-for-monitoring","title":"3. Bucket notifications (for monitoring)","text":"<p>Each bucket you want to monitor must publish to the Pub/Sub topic. Run the notification command once per bucket. The connector filters events by <code>DSXCONNECTOR_ASSET</code>/<code>DSXCONNECTOR_FILTER</code>. <pre><code>gsutil notification create -t gcs-object-events -f json gs://YOUR_BUCKET\n</code></pre></p> <p><pre><code># Create the subscription and grant subscriber role\ngcloud pubsub subscriptions create $SUBSCRIPTION \\\n  --topic gcs-object-events\n\ngcloud pubsub subscriptions add-iam-policy-binding \\\n  projects/$PROJECT_ID/subscriptions/$SUBSCRIPTION \\\n  --member=\"serviceAccount:${SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --role=roles/pubsub.subscriber\n</code></pre> <code>OBJECT_FINALIZE</code> and metadata update events now flow into the subscription. Repeat the <code>gsutil notification create</code> command for every bucket that should trigger events. The connector uses <code>DSXCONNECTOR_ASSET</code> and <code>DSXCONNECTOR_FILTER</code> to decide which objects to process.</p>"},{"location":"reference/google-cloud-credentials/#4-service-account-key-option-a-if-you-want-a-new-key-for-the-connector","title":"4. Service account key (Option A - if you want a new key for the connector)","text":"<p>Create a service account key (JSON) for the connector. <pre><code>gcloud iam service-accounts keys create gcs-sa.json \\\n  --iam-account ${SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\n</code></pre> See deployment guides on how to use/mount the JSON with the <code>GOOGLE_APPLICATION_CREDENTIALS</code> setting.</p>"},{"location":"reference/google-cloud-credentials/#5-connector-configuration","title":"5. Connector configuration","text":"<p><pre><code>GOOGLE_APPLICATION_CREDENTIALS=/app/creds/gcp-sa.json\nDSXCONNECTOR_ASSET=YOUR_BUCKET\nDSXCONNECTOR_MONITOR=true\nGCS_PUBSUB_PROJECT_ID=&lt;your-project-id&gt;\nGCS_PUBSUB_SUBSCRIPTION=$SUBSCRIPTION\n</code></pre> Install dependencies (if running locally): <pre><code>pip install google-cloud-storage google-cloud-pubsub\n</code></pre></p> <p>Pub/Sub monitoring now always listens for <code>OBJECT_FINALIZE</code> and <code>OBJECT_METADATA_UPDATE</code> events. Any legacy <code>*_MONITOR_EVENT_TYPES</code> variables are ignored.</p>"},{"location":"reference/google-cloud-credentials/#7-alternative-webhook","title":"7. Alternative webhook","text":"<p>If you want to use the connector without Pub/Sub, you can use the connector webhook. Deploy Cloud Functions/Run that receive events on buckets and use it to send object metainfo to <code>/google-cloud-storage-connector/webhook/event</code>. Both use the same handler.</p>"},{"location":"reference/google-cloud-credentials/#8-role-summary","title":"8. Role summary","text":"<ul> <li><code>roles/storage.objectViewer</code> (plus <code>storage.objectAdmin</code> if you use move/delete).</li> <li><code>roles/pubsub.subscriber</code> on the subscription.</li> </ul>"}]}